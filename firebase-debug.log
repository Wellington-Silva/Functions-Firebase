[debug] [2025-04-30T18:43:04.893Z] ----------------------------------------------------------------------
[debug] [2025-04-30T18:43:04.895Z] Command:       /usr/local/bin/firebase /home/wellington/.cache/firebase/tools/lib/node_modules/firebase-tools/lib/bin/firebase deploy --only functions
[debug] [2025-04-30T18:43:04.895Z] CLI Version:   14.0.0
[debug] [2025-04-30T18:43:04.895Z] Platform:      linux
[debug] [2025-04-30T18:43:04.896Z] Node Version:  v20.18.2
[debug] [2025-04-30T18:43:04.896Z] Time:          Wed Apr 30 2025 15:43:04 GMT-0300 (GMT-03:00)
[debug] [2025-04-30T18:43:04.896Z] ----------------------------------------------------------------------
[debug] 
[debug] [2025-04-30T18:43:04.899Z] >>> [apiv2][query] GET https://firebase-public.firebaseio.com/cli.json [none]
[debug] [2025-04-30T18:43:05.181Z] > command requires scopes: ["email","openid","https://www.googleapis.com/auth/cloudplatformprojects.readonly","https://www.googleapis.com/auth/firebase","https://www.googleapis.com/auth/cloud-platform"]
[debug] [2025-04-30T18:43:05.181Z] > authorizing via signed-in user (wellingtonsilva112000@gmail.com)
[debug] [2025-04-30T18:43:05.181Z] [iam] checking project bigqueryexampleproject-e4ef9 for permissions ["cloudfunctions.functions.create","cloudfunctions.functions.delete","cloudfunctions.functions.get","cloudfunctions.functions.list","cloudfunctions.functions.update","cloudfunctions.operations.get","firebase.projects.get"]
[debug] [2025-04-30T18:43:05.182Z] Checked if tokens are valid: false, expires at: 1746039013099
[debug] [2025-04-30T18:43:05.182Z] Checked if tokens are valid: false, expires at: 1746039013099
[debug] [2025-04-30T18:43:05.183Z] > refreshing access token with scopes: []
[debug] [2025-04-30T18:43:05.183Z] >>> [apiv2][query] POST https://www.googleapis.com/oauth2/v3/token [none]
[debug] [2025-04-30T18:43:05.184Z] >>> [apiv2][body] POST https://www.googleapis.com/oauth2/v3/token [omitted]
[debug] [2025-04-30T18:43:05.873Z] <<< [apiv2][status] POST https://www.googleapis.com/oauth2/v3/token 200
[debug] [2025-04-30T18:43:05.874Z] <<< [apiv2][body] POST https://www.googleapis.com/oauth2/v3/token [omitted]
[debug] [2025-04-30T18:43:05.919Z] >>> [apiv2][query] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions [none]
[debug] [2025-04-30T18:43:05.919Z] >>> [apiv2][(partial)header] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:43:05.919Z] >>> [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.create","cloudfunctions.functions.delete","cloudfunctions.functions.get","cloudfunctions.functions.list","cloudfunctions.functions.update","cloudfunctions.operations.get","firebase.projects.get"]}
[debug] [2025-04-30T18:43:06.019Z] <<< [apiv2][status] GET https://firebase-public.firebaseio.com/cli.json 200
[debug] [2025-04-30T18:43:06.019Z] <<< [apiv2][body] GET https://firebase-public.firebaseio.com/cli.json {"cloudBuildErrorAfter":1594252800000,"cloudBuildWarnAfter":1590019200000,"defaultNode10After":1594252800000,"minVersion":"3.0.5","node8DeploysDisabledAfter":1613390400000,"node8RuntimeDisabledAfter":1615809600000,"node8WarnAfter":1600128000000}
[debug] [2025-04-30T18:43:07.106Z] <<< [apiv2][status] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions 200
[debug] [2025-04-30T18:43:07.106Z] <<< [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.create","cloudfunctions.functions.delete","cloudfunctions.functions.get","cloudfunctions.functions.list","cloudfunctions.functions.update","cloudfunctions.operations.get","firebase.projects.get"]}
[debug] [2025-04-30T18:43:07.107Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:07.108Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:07.108Z] >>> [apiv2][query] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions [none]
[debug] [2025-04-30T18:43:07.108Z] >>> [apiv2][body] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions {"permissions":["iam.serviceAccounts.actAs"]}
[debug] [2025-04-30T18:43:08.297Z] <<< [apiv2][status] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions 404
[debug] [2025-04-30T18:43:08.297Z] <<< [apiv2][body] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions {"error":{"code":404,"message":"Unknown service account","status":"NOT_FOUND"}}
[debug] [2025-04-30T18:43:08.298Z] [functions] service account IAM check errored, deploy may fail: Request to https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions had HTTP Error: 404, Unknown service account {"name":"FirebaseError","children":[],"context":{"body":{"error":{"code":404,"message":"Unknown service account","status":"NOT_FOUND"}},"response":{"statusCode":404}},"exit":1,"message":"Request to https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions had HTTP Error: 404, Unknown service account","status":404}
[info] 
[info] === Deploying to 'bigqueryexampleproject-e4ef9'...
[info] 
[info] i  deploying functions 
[debug] [2025-04-30T18:43:08.305Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:08.305Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:08.305Z] >>> [apiv2][query] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 [none]
[debug] [2025-04-30T18:43:08.602Z] <<< [apiv2][status] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 200
[debug] [2025-04-30T18:43:08.603Z] <<< [apiv2][body] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 {"projectNumber":"893582587169","projectId":"bigqueryexampleproject-e4ef9","lifecycleState":"ACTIVE","name":"BigQueryExampleProject","labels":{"firebase":"enabled","firebase-core":"disabled"},"createTime":"2025-04-24T16:34:06.379745Z"}
[info] i  functions: preparing codebase default for deployment 
[info] i  functions: ensuring required API cloudfunctions.googleapis.com is enabled... 
[debug] [2025-04-30T18:43:08.605Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:08.605Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:08.606Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:08.606Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API cloudbuild.googleapis.com is enabled... 
[debug] [2025-04-30T18:43:08.607Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:08.607Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  artifactregistry: ensuring required API artifactregistry.googleapis.com is enabled... 
[debug] [2025-04-30T18:43:08.608Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:08.608Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:08.608Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com [none]
[debug] [2025-04-30T18:43:08.608Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:43:08.611Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com [none]
[debug] [2025-04-30T18:43:08.611Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:43:08.614Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com [none]
[debug] [2025-04-30T18:43:08.614Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:43:08.617Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com [none]
[debug] [2025-04-30T18:43:08.617Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:43:10.131Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com 200
[debug] [2025-04-30T18:43:10.131Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com [omitted]
[debug] [2025-04-30T18:43:10.134Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com 200
[debug] [2025-04-30T18:43:10.135Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com [omitted]
[info] ✔  artifactregistry: required API artifactregistry.googleapis.com is enabled 
[debug] [2025-04-30T18:43:10.144Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com 200
[debug] [2025-04-30T18:43:10.145Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com [omitted]
[info] ✔  functions: required API cloudfunctions.googleapis.com is enabled 
[debug] [2025-04-30T18:43:10.151Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com 200
[debug] [2025-04-30T18:43:10.151Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com [omitted]
[info] ✔  functions: required API cloudbuild.googleapis.com is enabled 
[debug] [2025-04-30T18:43:10.152Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:10.152Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:10.153Z] >>> [apiv2][query] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig [none]
[debug] [2025-04-30T18:43:10.959Z] <<< [apiv2][status] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig 200
[debug] [2025-04-30T18:43:10.960Z] <<< [apiv2][body] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig {"projectId":"bigqueryexampleproject-e4ef9","storageBucket":"bigqueryexampleproject-e4ef9.firebasestorage.app"}
[debug] [2025-04-30T18:43:10.961Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:10.961Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:10.961Z] >>> [apiv2][query] GET https://runtimeconfig.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/configs [none]
[debug] [2025-04-30T18:43:11.970Z] <<< [apiv2][status] GET https://runtimeconfig.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/configs 200
[debug] [2025-04-30T18:43:11.971Z] <<< [apiv2][body] GET https://runtimeconfig.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/configs {}
[debug] [2025-04-30T18:43:11.973Z] Validating nodejs source
[debug] [2025-04-30T18:43:12.673Z] > [functions] package.json contents: {
  "name": "functions",
  "description": "Cloud Functions for Firebase",
  "scripts": {
    "serve": "firebase emulators:start --only functions",
    "shell": "firebase functions:shell",
    "start": "npm run shell",
    "deploy": "firebase deploy --only functions",
    "logs": "firebase functions:log"
  },
  "engines": {
    "node": "22"
  },
  "main": "index.mjs",
  "dependencies": {
    "firebase-admin": "^12.6.0",
    "firebase-functions": "^6.0.1",
    "uuid": "^11.1.0"
  },
  "devDependencies": {
    "firebase-functions-test": "^3.1.0"
  },
  "private": true
}
[debug] [2025-04-30T18:43:12.674Z] Building nodejs source
[info] i  functions: Loading and analyzing source code for codebase default to determine what to deploy 
[debug] [2025-04-30T18:43:12.677Z] Could not find functions.yaml. Must use http discovery
[debug] [2025-04-30T18:43:12.688Z] Found firebase-functions binary at '/home/wellington/Documentos/Git/BigQueryExampleProject/functions/node_modules/.bin/firebase-functions'
[info] Serving at port 8720

[debug] [2025-04-30T18:43:13.479Z] Got response from /__/functions.yaml {"endpoints":{"createuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"createuser"},"deleteuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"deleteuser"},"listusers":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"listusers"},"processdata":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"processdata"},"showuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"showuser"},"updateuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"updateuser"}},"specVersion":"v1alpha1","requiredAPIs":[],"extensions":{}}
[info] i  extensions: ensuring required API firebaseextensions.googleapis.com is enabled... 
[debug] [2025-04-30T18:43:17.559Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:17.559Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:17.560Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com [none]
[debug] [2025-04-30T18:43:17.560Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:43:18.844Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com 200
[debug] [2025-04-30T18:43:18.844Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com [omitted]
[info] ✔  extensions: required API firebaseextensions.googleapis.com is enabled 
[debug] [2025-04-30T18:43:18.845Z] > command requires scopes: ["email","openid","https://www.googleapis.com/auth/cloudplatformprojects.readonly","https://www.googleapis.com/auth/firebase","https://www.googleapis.com/auth/cloud-platform"]
[debug] [2025-04-30T18:43:18.845Z] > authorizing via signed-in user (wellingtonsilva112000@gmail.com)
[debug] [2025-04-30T18:43:18.845Z] [iam] checking project bigqueryexampleproject-e4ef9 for permissions ["firebase.projects.get","firebaseextensions.instances.list"]
[debug] [2025-04-30T18:43:18.845Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:18.846Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:18.846Z] >>> [apiv2][query] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions [none]
[debug] [2025-04-30T18:43:18.846Z] >>> [apiv2][(partial)header] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:43:18.846Z] >>> [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["firebase.projects.get","firebaseextensions.instances.list"]}
[debug] [2025-04-30T18:43:19.906Z] <<< [apiv2][status] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions 200
[debug] [2025-04-30T18:43:19.906Z] <<< [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["firebase.projects.get","firebaseextensions.instances.list"]}
[debug] [2025-04-30T18:43:19.907Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:19.907Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:19.909Z] >>> [apiv2][query] GET https://firebaseextensions.googleapis.com/v1beta/projects/bigqueryexampleproject-e4ef9/instances pageSize=100&pageToken=
[debug] [2025-04-30T18:43:21.711Z] <<< [apiv2][status] GET https://firebaseextensions.googleapis.com/v1beta/projects/bigqueryexampleproject-e4ef9/instances 200
[debug] [2025-04-30T18:43:21.712Z] <<< [apiv2][body] GET https://firebaseextensions.googleapis.com/v1beta/projects/bigqueryexampleproject-e4ef9/instances {"instances":[{"name":"projects/bigqueryexampleproject-e4ef9/instances/firestore-bigquery-export","createTime":"2025-04-24T18:43:47.251262Z","updateTime":"2025-04-24T18:58:40.150355Z","state":"ACTIVE","config":{"name":"projects/bigqueryexampleproject-e4ef9/instances/firestore-bigquery-export/configurations/2661e7fb-2095-49c8-9edb-f7aeeb3fe7d6","createTime":"2025-04-24T18:43:47.251262Z","source":{"name":"projects/firebaseextensions/sources/1bcd5ca9-fd21-4b21-9984-8d6ee3a0a7c8","packageUri":"https://storage.googleapis.com/firebase-extensions-packages-prod/firebase-firestore-bigquery-export-0.2.2-713b5ff6-19ab-459d-8d21-b6eec0e8d5f9.zip","hash":"5df1592f3afe939bcf4a63279f7de800aade1153433d37d17a65109cf854dfa5","extensionRoot":"/","spec":{"specVersion":"v1beta","name":"firestore-bigquery-export","version":"0.2.2","description":"Sends realtime, incremental updates from a specified Cloud Firestore collection to BigQuery.","apis":[{"apiName":"bigquery.googleapis.com","reason":"Mirrors data from your Cloud Firestore collection in BigQuery."}],"roles":[{"role":"bigquery.dataEditor","reason":"Allows the extension to configure and export data into BigQuery."},{"role":"datastore.user","reason":"Allows the extension to write updates to the database."},{"role":"bigquery.user","reason":"Allows the extension to create and manage BigQuery materialized views."}],"resources":[{"name":"fsexportbigquery","type":"firebaseextensions.v1beta.v2function","propertiesYaml":"buildConfig:\n  runtime: nodejs22\neventTrigger:\n  eventFilters:\n  - attribute: database\n    value: ${DATABASE}\n  - attribute: document\n    operator: match-path-pattern\n    value: ${COLLECTION_PATH}/{documentId}\n  eventType: google.cloud.firestore.document.v1.written\n  triggerRegion: ${DATABASE_REGION}\nsourceDirectory: functions\n","description":"Listens for document changes in your specified Cloud Firestore collection, then exports the changes into BigQuery.","deletionPolicy":"DELETE"},{"name":"syncBigQuery","type":"firebaseextensions.v1beta.function","propertiesYaml":"runtime: nodejs20\ntaskQueueTrigger:\n  rateLimits:\n    maxConcurrentDispatches: 500\n    maxDispatchesPerSecond: ${param:MAX_DISPATCHES_PER_SECOND}\n  retryConfig:\n    maxAttempts: 5\n    minBackoffSeconds: 60\n","description":"A task-triggered function that gets called on BigQuery sync","deletionPolicy":"DELETE"},{"name":"initBigQuerySync","type":"firebaseextensions.v1beta.function","propertiesYaml":"runtime: nodejs20\ntaskQueueTrigger:\n  retryConfig:\n    maxAttempts: 15\n    minBackoffSeconds: 60\n","description":"Runs configuration for sycning with BigQuery","deletionPolicy":"DELETE"},{"name":"setupBigQuerySync","type":"firebaseextensions.v1beta.function","propertiesYaml":"runtime: nodejs20\ntaskQueueTrigger:\n  retryConfig:\n    maxAttempts: 15\n    minBackoffSeconds: 60\n","description":"Runs configuration for sycning with BigQuery","deletionPolicy":"DELETE"}],"billingRequired":true,"author":{"authorName":"Firebase","url":"https://firebase.google.com"},"contributors":[{"authorName":"Jan Wyszynski","email":"wyszynski@google.com","url":"https://github.com/IanWyszynski"}],"license":"Apache-2.0","releaseNotesUrl":"https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/CHANGELOG.md","sourceUrl":"https://github.com/firebase/extensions/tree/master/firestore-bigquery-export","params":[{"param":"DATASET_LOCATION","label":"BigQuery Dataset location","type":"SELECT","description":"Where do you want to deploy the BigQuery dataset created for this extension? For help selecting a location, refer to the [location selection guide](https://cloud.google.com/bigquery/docs/locations).","required":true,"options":[{"value":"us-central1","label":"Iowa (us-central1)"},{"value":"us-west4","label":"Las Vegas (us-west4)"},{"value":"europe-central2","label":"Warsaw (europe-central2)"},{"value":"us-west2","label":"Los Angeles (us-west2)"},{"value":"northamerica-northeast1","label":"Montreal (northamerica-northeast1)"},{"value":"us-east4","label":"Northern Virginia (us-east4)"},{"value":"us-west1","label":"Oregon (us-west1)"},{"value":"us-west3","label":"Salt Lake City (us-west3)"},{"value":"southamerica-east1","label":"Sao Paulo (southamerica-east1)"},{"value":"us-east1","label":"South Carolina (us-east1)"},{"value":"europe-west1","label":"Belgium (europe-west1)"},{"value":"europe-north1","label":"Finland (europe-north1)"},{"value":"europe-west3","label":"Frankfurt (europe-west3)"},{"value":"europe-west2","label":"London (europe-west2)"},{"value":"europe-west4","label":"Netherlands (europe-west4)"},{"value":"europe-west6","label":"Zurich (europe-west6)"},{"value":"asia-east1","label":"Taiwan (asia-east1)"},{"value":"asia-east2","label":"Hong Kong (asia-east2)"},{"value":"asia-southeast2","label":"Jakarta (asia-southeast2)"},{"value":"asia-south1","label":"Mumbai (asia-south1)"},{"value":"asia-southeast1","label":"Singapore (asia-southeast1)"},{"value":"asia-northeast2","label":"Osaka (asia-northeast2)"},{"value":"asia-northeast3","label":"Seoul (asia-northeast3)"},{"value":"asia-southeast1","label":"Singapore (asia-southeast1)"},{"value":"australia-southeast1","label":"Sydney (australia-southeast1)"},{"value":"asia-east1","label":"Taiwan (asia-east1)"},{"value":"asia-northeast1","label":"Tokyo (asia-northeast1)"},{"value":"us","label":"United States (multi-regional)"},{"value":"eu","label":"Europe (multi-regional)"}],"default":"us","immutable":true},{"param":"BIGQUERY_PROJECT_ID","label":"BigQuery Project ID","type":"STRING","description":"Override the default project for BigQuery instance. This can allow updates to be directed to to a BigQuery instance on another GCP project.","required":true,"default":"${PROJECT_ID}"},{"param":"DATABASE","label":"Firestore Instance ID","type":"STRING","description":"The Firestore database to use. Use \"(default)\" for the default database. You can view your available Firestore databases at https://console.cloud.google.com/firestore/databases.\n","required":true,"default":"(default)","example":"(default)"},{"param":"DATABASE_REGION","label":"Firestore Instance Location","type":"SELECT","description":"Where is the Firestore database located? You can check your current database location at https://console.cloud.google.com/firestore/databases.\n","required":true,"options":[{"value":"eur3","label":"Multi-region (Europe - Belgium and Netherlands)"},{"value":"nam5","label":"Multi-region (United States)"},{"value":"nam7","label":"Multi-region (Iowa, North Virginia, and Oklahoma)"},{"value":"us-central1","label":"Iowa (us-central1)"},{"value":"us-west1","label":"Oregon (us-west1)"},{"value":"us-west2","label":"Los Angeles (us-west2)"},{"value":"us-west3","label":"Salt Lake City (us-west3)"},{"value":"us-west4","label":"Las Vegas (us-west4)"},{"value":"us-east1","label":"South Carolina (us-east1)"},{"value":"us-east4","label":"Northern Virginia (us-east4)"},{"value":"us-east5","label":"Columbus (us-east5)"},{"value":"us-south1","label":"Dallas (us-south1)"},{"value":"northamerica-northeast1","label":"Montreal (northamerica-northeast1)"},{"value":"northamerica-northeast2","label":"Toronto (northamerica-northeast2)"},{"value":"northamerica-south1","label":"Queretaro (northamerica-south1)"},{"value":"southamerica-east1","label":"Sao Paulo (southamerica-east1)"},{"value":"southamerica-west1","label":"Santiago (southamerica-west1)"},{"value":"europe-west1","label":"Belgium (europe-west1)"},{"value":"europe-west2","label":"London (europe-west2)"},{"value":"europe-west3","label":"Frankfurt (europe-west3)"},{"value":"europe-west4","label":"Netherlands (europe-west4)"},{"value":"europe-west6","label":"Zurich (europe-west6)"},{"value":"europe-west8","label":"Milan (europe-west8)"},{"value":"europe-west9","label":"Paris (europe-west9)"},{"value":"europe-west10","label":"Berlin (europe-west10)"},{"value":"europe-west12","label":"Turin (europe-west12)"},{"value":"europe-southwest1","label":"Madrid (europe-southwest1)"},{"value":"europe-north1","label":"Finland (europe-north1)"},{"value":"europe-north2","label":"Stockholm (europe-north2)"},{"value":"europe-central2","label":"Warsaw (europe-central2)"},{"value":"me-central1","label":"Doha (me-central1)"},{"value":"me-central2","label":"Dammam (me-central2)"},{"value":"me-west1","label":"Tel Aviv (me-west1)"},{"value":"asia-south1","label":"Mumbai (asia-south1)"},{"value":"asia-south2","label":"Delhi (asia-south2)"},{"value":"asia-southeast1","label":"Singapore (asia-southeast1)"},{"value":"asia-southeast2","label":"Jakarta (asia-southeast2)"},{"value":"asia-east1","label":"Taiwan (asia-east1)"},{"value":"asia-east2","label":"Hong Kong (asia-east2)"},{"value":"asia-northeast1","label":"Tokyo (asia-northeast1)"},{"value":"asia-northeast2","label":"Osaka (asia-northeast2)"},{"value":"asia-northeast3","label":"Seoul (asia-northeast3)"},{"value":"australia-southeast1","label":"Sydney (australia-southeast1)"},{"value":"australia-southeast2","label":"Melbourne (australia-southeast2)"},{"value":"africa-south1","label":"Johannesburg (africa-south1)"}]},{"param":"COLLECTION_PATH","label":"Collection path","type":"STRING","description":"What is the path of the collection that you would like to export? You may use `{wildcard}` notation to match a subcollection of all documents in a collection (for example: `chatrooms/{chatid}/posts`). Parent Firestore Document IDs from `{wildcards}` can be returned in `path_params` as a JSON formatted string.","required":true,"default":"posts","example":"posts","validationRegex":"^[^/]+(/[^/]+/[^/]+)*$","validationErrorMessage":"Firestore collection paths must be an odd number of segments separated by slashes, e.g. \"path/to/collection\"."},{"param":"WILDCARD_IDS","label":"Enable Wildcard Column field with Parent Firestore Document IDs","type":"SELECT","description":"If enabled, creates a column containing a JSON object of all wildcard ids from a documents path.","options":[{"value":"false","label":"No"},{"value":"true","label":"Yes"}],"default":"false"},{"param":"DATASET_ID","label":"Dataset ID","type":"STRING","description":"What ID would you like to use for your BigQuery dataset? This extension will create the dataset, if it doesn't already exist.","required":true,"default":"firestore_export","example":"firestore_export","validationRegex":"^[a-zA-Z0-9_]+$","validationErrorMessage":"BigQuery dataset IDs must be alphanumeric (plus underscores) and must be no more than 1024 characters.\n"},{"param":"TABLE_ID","label":"Table ID","type":"STRING","description":"What identifying prefix would you like to use for your table and view inside your BigQuery dataset? This extension will create the table and view, if they don't already exist.","required":true,"default":"posts","example":"posts","validationRegex":"^[a-zA-Z0-9_]+$","validationErrorMessage":"BigQuery table IDs must be alphanumeric (plus underscores) and must be no more than 1024 characters.\n"},{"param":"TABLE_PARTITIONING","label":"BigQuery SQL table Time Partitioning option type","type":"SELECT","description":"This parameter will allow you to partition the BigQuery table and BigQuery view created by the extension based on data ingestion time. You may select the granularity of partitioning based upon one of: HOUR, DAY, MONTH, YEAR. This will generate one partition per day, hour, month or year, respectively.","options":[{"value":"HOUR","label":"hour"},{"value":"DAY","label":"day"},{"value":"MONTH","label":"month"},{"value":"YEAR","label":"year"},{"value":"NONE","label":"none"}],"default":"NONE"},{"param":"TIME_PARTITIONING_FIELD","label":"BigQuery Time Partitioning column name","type":"STRING","description":"BigQuery table column/schema field name for TimePartitioning. You can choose schema available as `timestamp` OR a new custom defined column that will be assigned to the selected Firestore Document field below. Defaults to pseudo column _PARTITIONTIME if unspecified. Cannot be changed if Table is already partitioned."},{"param":"TIME_PARTITIONING_FIRESTORE_FIELD","label":"Firestore Document field name for BigQuery SQL Time Partitioning field option","type":"STRING","description":"This parameter will allow you to partition the BigQuery table created by the extension based on selected. The Firestore Document field value must be a top-level TIMESTAMP, DATETIME, DATE field BigQuery string format or Firestore timestamp(will be converted to BigQuery TIMESTAMP). Cannot be changed if Table is already partitioned.\n example: `postDate`(Ensure that the Firestore-BigQuery export extension\ncreates the dataset and table before initiating any backfill scripts.\n This step is crucial for the partitioning to function correctly. It is\nessential for the script to insert data into an already partitioned table.)"},{"param":"TIME_PARTITIONING_FIELD_TYPE","label":"BigQuery SQL Time Partitioning table schema field(column) type","type":"SELECT","description":"Parameter for BigQuery SQL schema field type for the selected Time Partitioning Firestore Document field option. Cannot be changed if Table is already partitioned.","options":[{"value":"TIMESTAMP","label":"TIMESTAMP"},{"value":"DATETIME","label":"DATETIME"},{"value":"DATE","label":"DATE"},{"value":"omit","label":"omit"}],"default":"omit"},{"param":"CLUSTERING","label":"BigQuery SQL table clustering","type":"STRING","description":"This parameter allows you to set up clustering for the BigQuery table created by the extension. Specify up to 4 comma-separated fields (for example:  `data,document_id,timestamp` - no whitespaces). The order of the specified  columns determines the sort order of the data. \nnote: Cluster columns must be top-level, non-repeated columns of one of the  following types: BIGNUMERIC, BOOL, DATE, DATETIME, GEOGRAPHY, INT64, NUMERIC,  RANGE, STRING, TIMESTAMP. Clustering will not be added if a field with an invalid type is present in this parameter.\nAvailable schema extensions table fields for clustering include: `document_id, document_name, timestamp, event_id,  operation, data`.","example":"data,document_id,timestamp","validationRegex":"^[^,\\s]+(?:,[^,\\s]+){0,3}$","validationErrorMessage":"No whitespaces. Max 4 fields. e.g. `data,timestamp,event_id,operation`"},{"param":"MAX_DISPATCHES_PER_SECOND","label":"Maximum number of synced documents per second","type":"STRING","description":"This parameter will set the maximum number of syncronised documents per second with BQ. Please note, any other external updates to a Big Query table will be included within this quota. Ensure that you have a set a low enough number to compensate. Defaults to 100.","default":"100","validationRegex":"^([1-9]|[1-9][0-9]|[1-4][0-9]{2}|500)$","validationErrorMessage":"Please select a number between 1 and 500"},{"param":"VIEW_TYPE","label":"View Type","type":"SELECT","description":"Select the type of view to create in BigQuery. A regular view is a virtual table defined by a SQL query.  A materialized view persists the results of a query for faster access, with either incremental or  non-incremental updates. Please note that materialized views in this extension come with several  important caveats and limitations - carefully review the pre-install documentation before selecting  these options to ensure they are appropriate for your use case.","required":true,"options":[{"value":"view","label":"View"},{"value":"materialized_incremental","label":"Materialized View (Incremental)"},{"value":"materialized_non_incremental","label":"Materialized View (Non-incremental)"}],"default":"view"},{"param":"MAX_STALENESS","label":"Maximum Staleness Duration","type":"STRING","description":"For materialized views only: Specifies the maximum staleness acceptable for the materialized view.  Should be specified as an INTERVAL value following BigQuery SQL syntax.  This parameter will only take effect if View Type is set to a materialized view option.","example":"INTERVAL \"8:0:0\" HOUR TO SECOND"},{"param":"REFRESH_INTERVAL_MINUTES","label":"Refresh Interval (Minutes)","type":"STRING","description":"For materialized views only: Specifies how often the materialized view should be refreshed, in minutes.  This parameter will only take effect if View Type is set to a materialized view option.","example":"60","validationRegex":"^[1-9][0-9]*$","validationErrorMessage":"Must be a positive integer"},{"param":"BACKUP_COLLECTION","label":"Backup Collection Name","type":"STRING","description":"This (optional) parameter will allow you to specify a collection for which failed BigQuery updates will be written to."},{"param":"TRANSFORM_FUNCTION","label":"Transform function URL","type":"STRING","description":"Specify a function URL to call that will transform the payload that will be written to BigQuery. See the pre-install documentation for more details.","example":"https://us-west1-my-project-id.cloudfunctions.net/myTransformFunction"},{"param":"USE_NEW_SNAPSHOT_QUERY_SYNTAX","label":"Use new query syntax for snapshots","type":"SELECT","description":"If enabled, snapshots will be generated with the new query syntax, which should be more performant, and avoid potential resource limitations.","required":true,"options":[{"value":"yes","label":"Yes"},{"value":"no","label":"No"}],"default":"no"},{"param":"EXCLUDE_OLD_DATA","label":"Exclude old data payloads","type":"SELECT","description":"If enabled, table rows will never contain old data (document snapshot before the Firestore onDocumentUpdate event: `change.before.data()`). The reduction in data should be more performant, and avoid potential resource limitations.","options":[{"value":"yes","label":"Yes"},{"value":"no","label":"No"}],"default":"no"},{"param":"KMS_KEY_NAME","label":"Cloud KMS key name","type":"STRING","description":"Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS. If this parameter is set, the extension will specify the KMS key name when creating the BQ table. See the PREINSTALL.md for more details.","validationRegex":"projects/([^/]+)/locations/([^/]+)/keyRings/([^/]+)/cryptoKeys/([^/]+)","validationErrorMessage":"The key name must be of the format 'projects/PROJECT_NAME/locations/KEY_RING_LOCATION/keyRings/KEY_RING_ID/cryptoKeys/KEY_ID'."},{"param":"MAX_ENQUEUE_ATTEMPTS","label":"Maximum number of enqueue attempts","type":"STRING","description":"This parameter will set the maximum number of attempts to enqueue a document to cloud tasks for export to BigQuery.","default":"3","validationRegex":"^(10|[1-9])$","validationErrorMessage":"Please select an integer between 1 and 10"},{"param":"LOG_LEVEL","label":"Log level","type":"SELECT","description":"The log level for the extension. The log level controls the verbosity of the extension's logs. The available log levels are: debug, info, warn, and error. To reduce the volume of logs, use a log level of warn or error.","required":true,"options":[{"value":"debug","label":"Debug"},{"value":"info","label":"Info"},{"value":"warn","label":"Warn"},{"value":"error","label":"Error"},{"value":"silent","label":"Silent"}],"default":"info"}],"preinstallContent":"Use this extension to export the documents in a Cloud Firestore collection to BigQuery. Exports are realtime and incremental, so the data in BigQuery is a mirror of your content in Cloud Firestore.\n\nThe extension creates and updates a [dataset](https://cloud.google.com/bigquery/docs/datasets-intro) containing the following two BigQuery resources:\n\n- A [table](https://cloud.google.com/bigquery/docs/tables-intro) of raw data that stores a full change history of the documents within your collection. This table includes a number of metadata fields so that BigQuery can display the current state of your data. The principle metadata fields are `timestamp`, `document_name`, and the `operation` for the document change.\n- A [view](https://cloud.google.com/bigquery/docs/views-intro) which represents the current state of the data within your collection. It also shows a log of the latest `operation` for each document (`CREATE`, `UPDATE`, or `IMPORT`).\n\n*Warning*: A BigQuery table corresponding to your configuration will be automatically generated upon installing or updating this extension. Manual table creation may result in discrepancies with your configured settings.\n\nIf you create, update, or delete a document in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`). \n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in ${param:COLLECTION_PATH}. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\nNote: To enable partitioning for a Big Query database, the following fields are required:\n\n - Time Partitioning option type\n - Time partitioning column name\n - Time partiitioning table schema\n - Firestore document field name\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n\n\n#### Additional setup\n\nBefore installing this extension, you'll need to:\n\n- [Set up Cloud Firestore in your Firebase project.](https://firebase.google.com/docs/firestore/quickstart)\n- [Link your Firebase project to BigQuery.](https://support.google.com/firebase/answer/6318765)\n\n\n#### Import existing documents\n\nTo import existing documents you can run the external [import script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n**Important:** Run the external import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nWithout use of this import script, the extension only exports the content of documents that are created or changed after installation.\n\n#### Transform function\n\nPrior to sending the document change to BigQuery, you have an opportunity to transform the data with an HTTP function. The payload will contain the following:\n\n```\n{ \n  data: [{\n    insertId: int;\n    json: {\n      timestamp: int;\n      event_id: int;\n      document_name: string;\n      document_id: int;\n      operation: ChangeType;\n      data: string;\n    },\n  }]\n}\n```\n\nThe response should be indentical in structure.\n\n#### Materialized Views\n\nThis extension supports both regular views and materialized views in BigQuery. While regular views compute their results each time they're queried, materialized views store their query results, providing faster access at the cost of additional storage.\n\nThere are two types of materialized views available:\n\n1. **Non-incremental Materialized Views**: These views support more complex queries including filtering on aggregated fields, but require complete recomputation during refresh.\n\n2. **Incremental Materialized Views**: These views update more efficiently by processing only new or changed records, but come with query restrictions. Most notably, they don't allow filtering or partitioning on aggregated fields in their defining SQL, among other limitations.\n\n**Important Considerations:**\n- Neither type of materialized view in this extension currently supports partitioning or clustering\n- Both types allow you to configure refresh intervals and maximum staleness settings during extension installation or configuration\n- Once created, a materialized view's SQL definition cannot be modified. If you reconfigure the extension to change either the view type (incremental vs non-incremental) or the SQL query, the extension will drop the existing materialized view and recreate it\n- Carefully consider your use case before choosing materialized views:\n  - They incur additional storage costs as they cache query results\n  - Non-incremental views may have higher processing costs during refresh\n  - Incremental views have more query restrictions but are more efficient to update\n\nExample of a non-incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    allow_non_incremental_definition = true,\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\n  AS (\n    WITH latests AS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY document_name\n    )\n    SELECT *\n    FROM latests\n    WHERE operation != \"DELETE\"\n  )\n```\n\nExample of an incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\nAS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM\n        `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY\n        document_name\n    )\n```\n\nPlease review [BigQuery's documentation on materialized views](https://cloud.google.com/bigquery/docs/materialized-views-intro) to fully understand the implications for your use case.\n\n#### Using Customer Managed Encryption Keys\n\nBy default, BigQuery encrypts your content stored at rest. BigQuery handles and manages this default encryption for you without any additional actions on your part.\n\nIf you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery. Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS.\n\nFor more general information on this, see [the docs](https://cloud.google.com/bigquery/docs/customer-managed-encryption).\n\nTo use CMEK and the Key Management Service (KMS) with this extension\n1. [Enable the KMS API in your Google Cloud Project](https://console.cloud.google.com/apis/enableflow?apiid=cloudkms.googleapis.com).\n2. Create a keyring and keychain in the KMS. Note that the region of the keyring and key *must* match the region of your bigquery dataset\n3. Grant the BigQuery service account permission to encrypt and decrypt using that key. The Cloud KMS CryptoKey Encrypter/Decrypter role grants this permission. First find your project number. You can find this for example on the cloud console dashboard `https://console.cloud.google.com/home/dashboard?project={PROJECT_ID}`. The service account which needs the Encrypter/Decrypter role is then `bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com`. You can grant this role through the credentials service in the console, or through the CLI:\n```\ngcloud kms keys add-iam-policy-binding \\\n--project=KMS_PROJECT_ID \\\n--member serviceAccount:bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com \\\n--role roles/cloudkms.cryptoKeyEncrypterDecrypter \\\n--location=KMS_KEY_LOCATION \\\n--keyring=KMS_KEY_RING \\\nKMS_KEY\n```\n4. When installing this extension, enter the resource name of your key. It will look something like the following:\n```\nprojects/<YOUR PROJECT ID>/locations/<YOUR REGION>/keyRings/<YOUR KEY RING NAME>/cryptoKeys/<YOUR KEY NAME>\n```\nIf you follow these steps, your changelog table should be created using your customer-managed encryption.\n\n#### Generate schema views\n\nAfter your data is in BigQuery, you can run the [schema-views script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md) (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter to your target BigQuery project ID.\n\n2. After installation, you'll need to grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n#### Mitigating Data Loss During Extension Updates\n\nWhen updating or reconfiguring this extension, there may be a brief period where data streaming from Firestore to BigQuery is interrupted. While this limitation exists within the Extensions platform, we provide two strategies to mitigate potential data loss.\n\n##### Strategy 1: Post-Update Import\nAfter reconfiguring the extension, run the import script on your collection to ensure all data is captured. Refer to the \"Import Existing Documents\" section above for detailed steps.\n\n##### Strategy 2: Parallel Instance Method\n1. Install a second instance of the extension that streams to a new BigQuery table\n2. Reconfigure the original extension\n3. Once the original extension is properly configured and streaming events\n4. Uninstall the second instance\n5. Run a BigQuery merge job to combine the data from both tables\n\n##### Considerations\n- Strategy 1 is simpler but may result in duplicate records that need to be deduplicated\n- Strategy 2 requires more setup but provides better data continuity\n- Choose the strategy that best aligns with your data consistency requirements and operational constraints\n\n#### Billing\nTo install an extension, your project must be on the [Blaze (pay as you go) plan](https://firebase.google.com/pricing)\n\n- This extension uses other Firebase and Google Cloud Platform services, which have associated charges if you exceed the service’s no-cost tier:\n  - BigQuery (this extension writes to BigQuery with [streaming inserts](https://cloud.google.com/bigquery/pricing#streaming_pricing))\n  - Cloud Firestore\n  - Cloud Functions (Node.js 10+ runtime. [See FAQs](https://firebase.google.com/support/faq#extensions-pricing))","postinstallContent":"### See it in action\n\nYou can test out this extension right away!\n\n1.  Go to your [Cloud Firestore dashboard](https://console.firebase.google.com/project/${param:BIGQUERY_PROJECT_ID}/firestore/data) in the Firebase console.\n\n2.  If it doesn't already exist, create the collection you specified during installation: `${param:COLLECTION_PATH}`\n\n3.  Create a document in the collection called `bigquery-mirror-test` that contains any fields with any values that you'd like.\n\n4.  Go to the [BigQuery web UI](https://console.cloud.google.com/bigquery?project=${param:BIGQUERY_PROJECT_ID}&p=${param:BIGQUERY_PROJECT_ID}&d=${param:DATASET_ID}) in the Google Cloud Platform console.\n\n5.  Query your **raw changelog table**, which should contain a single log of creating the `bigquery-mirror-test` document.\n\n    ```\n    SELECT *\n    FROM `${param:BIGQUERY_PROJECT_ID}.${param:DATASET_ID}.${param:TABLE_ID}_raw_changelog`\n    ```\n\n6.  Query your **latest view**, which should return the latest change event for the only document present -- `bigquery-mirror-test`.\n\n    ```\n    SELECT *\n    FROM `${param:BIGQUERY_PROJECT_ID}.${param:DATASET_ID}.${param:TABLE_ID}_raw_latest`\n    ```\n\n7.  Delete the `bigquery-mirror-test` document from [Cloud Firestore](https://console.firebase.google.com/project/${param:BIGQUERY_PROJECT_ID}/firestore/data).\n    The `bigquery-mirror-test` document will disappear from the **latest view** and a `DELETE` event will be added to the **raw changelog table**.\n\n8.  You can check the changelogs of a single document with this query:\n\n    ```\n    SELECT *\n    FROM `${param:BIGQUERY_PROJECT_ID}.${param:DATASET_ID}.${param:TABLE_ID}_raw_changelog`\n    WHERE document_name = \"bigquery-mirror-test\"\n    ORDER BY TIMESTAMP ASC\n    ```\n\n### Using the extension\n\nWhenever a document is created, updated, imported, or deleted in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset which contains the following resources:\n\n- **raw changelog table:** [`${param:TABLE_ID}_raw_changelog`](https://console.cloud.google.com/bigquery?project=${param:BIGQUERY_PROJECT_ID}&p=${param:BIGQUERY_PROJECT_ID}&d=${param:DATASET_ID}&t=${param:TABLE_ID}_raw_changelog&page=table)\n- **latest view:** [`${param:TABLE_ID}_raw_latest`](https://console.cloud.google.com/bigquery?project=${param:BIGQUERY_PROJECT_ID}&p=${param:BIGQUERY_PROJECT_ID}&d=${param:DATASET_ID}&t=${param:TABLE_ID}_raw_latest&page=table)\n\nTo review the schema for these two resources, click the **Schema** tab for each resource in BigQuery.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`).\n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in ${param:COLLECTION_PATH}. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter as your target BigQuery project ID.\n\n2. Identify the service account on the source project associated with the extension. By default, it will be constructed as `ext-<extension-instance-id>@project-id.iam.gserviceaccount.com`. However, if the extension instance ID is too long, it may be truncated and 4 random characters appended to abide by service account length limits.\n\n3. To find the exact service account, navigate to IAM & Admin -> IAM in the Google Cloud Platform Console. Look for the service account listed with \"Name\" as \"Firebase Extensions <your extension instance ID> service account\". The value in the \"Principal\" column will be the service account that needs permissions granted in the target project.\n\n4. Grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID] [-s SERVICE_ACCOUNT]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID] [-ServiceAccount SERVICE_ACCOUNT]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-s`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-ServiceAccount`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n### _(Optional)_ Import existing documents\n\nYou can backfill your BigQuery dataset with all the documents in your collection using the import script.\n\nIf you don't run the import script, the extension only exports the content of documents that are created or changed after installation.\n\nThe import script can read all existing documents in a Cloud Firestore collection and insert them into the raw changelog table created by this extension. The script adds a special changelog for each document with the operation of `IMPORT` and the timestamp of epoch. This is to ensure that any operation on an imported document supersedes the `IMPORT`.\n\n**Important:** Run the import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nLearn more about using the import script to [backfill your existing collection](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n### _(Optional)_ Generate schema views\n\nAfter your data is in BigQuery, you can use the schema-views script (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\nLearn more about using the schema-views script to [generate schema views](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md).\n\n### Monitoring\n\nAs a best practice, you can [monitor the activity](https://firebase.google.com/docs/extensions/manage-installed-extensions#monitor) of your installed extension, including checks on its health, usage, and logs.\n","readmeContent":"# Stream Firestore to BigQuery\n\n**Author**: Firebase (**[https://firebase.google.com](https://firebase.google.com)**)\n\n**Description**: Sends realtime, incremental updates from a specified Cloud Firestore collection to BigQuery.\n\n\n\n**Details**: Use this extension to export the documents in a Cloud Firestore collection to BigQuery. Exports are realtime and incremental, so the data in BigQuery is a mirror of your content in Cloud Firestore.\n\nThe extension creates and updates a [dataset](https://cloud.google.com/bigquery/docs/datasets-intro) containing the following two BigQuery resources:\n\n- A [table](https://cloud.google.com/bigquery/docs/tables-intro) of raw data that stores a full change history of the documents within your collection. This table includes a number of metadata fields so that BigQuery can display the current state of your data. The principle metadata fields are `timestamp`, `document_name`, and the `operation` for the document change.\n- A [view](https://cloud.google.com/bigquery/docs/views-intro) which represents the current state of the data within your collection. It also shows a log of the latest `operation` for each document (`CREATE`, `UPDATE`, or `IMPORT`).\n\n*Warning*: A BigQuery table corresponding to your configuration will be automatically generated upon installing or updating this extension. Manual table creation may result in discrepancies with your configured settings.\n\nIf you create, update, or delete a document in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`). \n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in ${param:COLLECTION_PATH}. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\nNote: To enable partitioning for a Big Query database, the following fields are required:\n\n - Time Partitioning option type\n - Time partitioning column name\n - Time partiitioning table schema\n - Firestore document field name\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n\n\n#### Additional setup\n\nBefore installing this extension, you'll need to:\n\n- [Set up Cloud Firestore in your Firebase project.](https://firebase.google.com/docs/firestore/quickstart)\n- [Link your Firebase project to BigQuery.](https://support.google.com/firebase/answer/6318765)\n\n\n#### Import existing documents\n\nTo import existing documents you can run the external [import script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n**Important:** Run the external import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nWithout use of this import script, the extension only exports the content of documents that are created or changed after installation.\n\n#### Transform function\n\nPrior to sending the document change to BigQuery, you have an opportunity to transform the data with an HTTP function. The payload will contain the following:\n\n```\n{ \n  data: [{\n    insertId: int;\n    json: {\n      timestamp: int;\n      event_id: int;\n      document_name: string;\n      document_id: int;\n      operation: ChangeType;\n      data: string;\n    },\n  }]\n}\n```\n\nThe response should be indentical in structure.\n\n#### Materialized Views\n\nThis extension supports both regular views and materialized views in BigQuery. While regular views compute their results each time they're queried, materialized views store their query results, providing faster access at the cost of additional storage.\n\nThere are two types of materialized views available:\n\n1. **Non-incremental Materialized Views**: These views support more complex queries including filtering on aggregated fields, but require complete recomputation during refresh.\n\n2. **Incremental Materialized Views**: These views update more efficiently by processing only new or changed records, but come with query restrictions. Most notably, they don't allow filtering or partitioning on aggregated fields in their defining SQL, among other limitations.\n\n**Important Considerations:**\n- Neither type of materialized view in this extension currently supports partitioning or clustering\n- Both types allow you to configure refresh intervals and maximum staleness settings during extension installation or configuration\n- Once created, a materialized view's SQL definition cannot be modified. If you reconfigure the extension to change either the view type (incremental vs non-incremental) or the SQL query, the extension will drop the existing materialized view and recreate it\n- Carefully consider your use case before choosing materialized views:\n  - They incur additional storage costs as they cache query results\n  - Non-incremental views may have higher processing costs during refresh\n  - Incremental views have more query restrictions but are more efficient to update\n\nExample of a non-incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    allow_non_incremental_definition = true,\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\n  AS (\n    WITH latests AS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY document_name\n    )\n    SELECT *\n    FROM latests\n    WHERE operation != \"DELETE\"\n  )\n```\n\nExample of an incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\nAS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM\n        `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY\n        document_name\n    )\n```\n\nPlease review [BigQuery's documentation on materialized views](https://cloud.google.com/bigquery/docs/materialized-views-intro) to fully understand the implications for your use case.\n\n#### Using Customer Managed Encryption Keys\n\nBy default, BigQuery encrypts your content stored at rest. BigQuery handles and manages this default encryption for you without any additional actions on your part.\n\nIf you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery. Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS.\n\nFor more general information on this, see [the docs](https://cloud.google.com/bigquery/docs/customer-managed-encryption).\n\nTo use CMEK and the Key Management Service (KMS) with this extension\n1. [Enable the KMS API in your Google Cloud Project](https://console.cloud.google.com/apis/enableflow?apiid=cloudkms.googleapis.com).\n2. Create a keyring and keychain in the KMS. Note that the region of the keyring and key *must* match the region of your bigquery dataset\n3. Grant the BigQuery service account permission to encrypt and decrypt using that key. The Cloud KMS CryptoKey Encrypter/Decrypter role grants this permission. First find your project number. You can find this for example on the cloud console dashboard `https://console.cloud.google.com/home/dashboard?project={PROJECT_ID}`. The service account which needs the Encrypter/Decrypter role is then `bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com`. You can grant this role through the credentials service in the console, or through the CLI:\n```\ngcloud kms keys add-iam-policy-binding \\\n--project=KMS_PROJECT_ID \\\n--member serviceAccount:bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com \\\n--role roles/cloudkms.cryptoKeyEncrypterDecrypter \\\n--location=KMS_KEY_LOCATION \\\n--keyring=KMS_KEY_RING \\\nKMS_KEY\n```\n4. When installing this extension, enter the resource name of your key. It will look something like the following:\n```\nprojects/<YOUR PROJECT ID>/locations/<YOUR REGION>/keyRings/<YOUR KEY RING NAME>/cryptoKeys/<YOUR KEY NAME>\n```\nIf you follow these steps, your changelog table should be created using your customer-managed encryption.\n\n#### Generate schema views\n\nAfter your data is in BigQuery, you can run the [schema-views script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md) (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter to your target BigQuery project ID.\n\n2. After installation, you'll need to grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n#### Mitigating Data Loss During Extension Updates\n\nWhen updating or reconfiguring this extension, there may be a brief period where data streaming from Firestore to BigQuery is interrupted. While this limitation exists within the Extensions platform, we provide two strategies to mitigate potential data loss.\n\n##### Strategy 1: Post-Update Import\nAfter reconfiguring the extension, run the import script on your collection to ensure all data is captured. Refer to the \"Import Existing Documents\" section above for detailed steps.\n\n##### Strategy 2: Parallel Instance Method\n1. Install a second instance of the extension that streams to a new BigQuery table\n2. Reconfigure the original extension\n3. Once the original extension is properly configured and streaming events\n4. Uninstall the second instance\n5. Run a BigQuery merge job to combine the data from both tables\n\n##### Considerations\n- Strategy 1 is simpler but may result in duplicate records that need to be deduplicated\n- Strategy 2 requires more setup but provides better data continuity\n- Choose the strategy that best aligns with your data consistency requirements and operational constraints\n\n#### Billing\nTo install an extension, your project must be on the [Blaze (pay as you go) plan](https://firebase.google.com/pricing)\n\n- This extension uses other Firebase and Google Cloud Platform services, which have associated charges if you exceed the service’s no-cost tier:\n  - BigQuery (this extension writes to BigQuery with [streaming inserts](https://cloud.google.com/bigquery/pricing#streaming_pricing))\n  - Cloud Firestore\n  - Cloud Functions (Node.js 10+ runtime. [See FAQs](https://firebase.google.com/support/faq#extensions-pricing))\n\n\n\n**Configuration Parameters:**\n\n* BigQuery Dataset location: Where do you want to deploy the BigQuery dataset created for this extension? For help selecting a location, refer to the [location selection guide](https://cloud.google.com/bigquery/docs/locations).\n\n* BigQuery Project ID: Override the default project for BigQuery instance. This can allow updates to be directed to to a BigQuery instance on another GCP project.\n\n* Firestore Instance ID: The Firestore database to use. Use \"(default)\" for the default database. You can view your available Firestore databases at https://console.cloud.google.com/firestore/databases.\n\n\n* Firestore Instance Location: Where is the Firestore database located? You can check your current database location at https://console.cloud.google.com/firestore/databases.\n\n\n* Collection path: What is the path of the collection that you would like to export? You may use `{wildcard}` notation to match a subcollection of all documents in a collection (for example: `chatrooms/{chatid}/posts`). Parent Firestore Document IDs from `{wildcards}` can be returned in `path_params` as a JSON formatted string.\n\n* Enable Wildcard Column field with Parent Firestore Document IDs: If enabled, creates a column containing a JSON object of all wildcard ids from a documents path.\n\n* Dataset ID: What ID would you like to use for your BigQuery dataset? This extension will create the dataset, if it doesn't already exist.\n\n* Table ID: What identifying prefix would you like to use for your table and view inside your BigQuery dataset? This extension will create the table and view, if they don't already exist.\n\n* BigQuery SQL table Time Partitioning option type: This parameter will allow you to partition the BigQuery table and BigQuery view created by the extension based on data ingestion time. You may select the granularity of partitioning based upon one of: HOUR, DAY, MONTH, YEAR. This will generate one partition per day, hour, month or year, respectively.\n\n* BigQuery Time Partitioning column name: BigQuery table column/schema field name for TimePartitioning. You can choose schema available as `timestamp` OR a new custom defined column that will be assigned to the selected Firestore Document field below. Defaults to pseudo column _PARTITIONTIME if unspecified. Cannot be changed if Table is already partitioned.\n\n* Firestore Document field name for BigQuery SQL Time Partitioning field option: This parameter will allow you to partition the BigQuery table created by the extension based on selected. The Firestore Document field value must be a top-level TIMESTAMP, DATETIME, DATE field BigQuery string format or Firestore timestamp(will be converted to BigQuery TIMESTAMP). Cannot be changed if Table is already partitioned.\n example: `postDate`(Ensure that the Firestore-BigQuery export extension\ncreates the dataset and table before initiating any backfill scripts.\n This step is crucial for the partitioning to function correctly. It is\nessential for the script to insert data into an already partitioned table.)\n\n* BigQuery SQL Time Partitioning table schema field(column) type: Parameter for BigQuery SQL schema field type for the selected Time Partitioning Firestore Document field option. Cannot be changed if Table is already partitioned.\n\n* BigQuery SQL table clustering: This parameter allows you to set up clustering for the BigQuery table created by the extension. Specify up to 4 comma-separated fields (for example:  `data,document_id,timestamp` - no whitespaces). The order of the specified  columns determines the sort order of the data. \nNote: Cluster columns must be top-level, non-repeated columns of one of the  following types: BIGNUMERIC, BOOL, DATE, DATETIME, GEOGRAPHY, INT64, NUMERIC,  RANGE, STRING, TIMESTAMP. Clustering will not be added if a field with an invalid type is present in this parameter.\nAvailable schema extensions table fields for clustering include: `document_id, document_name, timestamp, event_id,  operation, data`.\n\n* Maximum number of synced documents per second: This parameter will set the maximum number of syncronised documents per second with BQ. Please note, any other external updates to a Big Query table will be included within this quota. Ensure that you have a set a low enough number to compensate. Defaults to 100.\n\n* View Type: Select the type of view to create in BigQuery. A regular view is a virtual table defined by a SQL query.  A materialized view persists the results of a query for faster access, with either incremental or  non-incremental updates. Please note that materialized views in this extension come with several  important caveats and limitations - carefully review the pre-install documentation before selecting  these options to ensure they are appropriate for your use case.\n\n* Maximum Staleness Duration: For materialized views only: Specifies the maximum staleness acceptable for the materialized view.  Should be specified as an INTERVAL value following BigQuery SQL syntax.  This parameter will only take effect if View Type is set to a materialized view option.\n\n* Refresh Interval (Minutes): For materialized views only: Specifies how often the materialized view should be refreshed, in minutes.  This parameter will only take effect if View Type is set to a materialized view option.\n\n* Backup Collection Name: This (optional) parameter will allow you to specify a collection for which failed BigQuery updates will be written to.\n\n* Transform function URL: Specify a function URL to call that will transform the payload that will be written to BigQuery. See the pre-install documentation for more details.\n\n* Use new query syntax for snapshots: If enabled, snapshots will be generated with the new query syntax, which should be more performant, and avoid potential resource limitations.\n\n* Exclude old data payloads: If enabled, table rows will never contain old data (document snapshot before the Firestore onDocumentUpdate event: `change.before.data()`). The reduction in data should be more performant, and avoid potential resource limitations.\n\n* Cloud KMS key name: Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS. If this parameter is set, the extension will specify the KMS key name when creating the BQ table. See the PREINSTALL.md for more details.\n\n* Maximum number of enqueue attempts: This parameter will set the maximum number of attempts to enqueue a document to cloud tasks for export to BigQuery.\n\n* Log level: The log level for the extension. The log level controls the verbosity of the extension's logs. The available log levels are: debug, info, warn, and error. To reduce the volume of logs, use a log level of warn or error.\n\n\n\n**Cloud Functions:**\n\n* **syncBigQuery:** A task-triggered function that gets called on BigQuery sync\n\n* **initBigQuerySync:** Runs configuration for sycning with BigQuery\n\n* **setupBigQuerySync:** Runs configuration for sycning with BigQuery\n\n\n\n**Other Resources**:\n\n* fsexportbigquery (firebaseextensions.v1beta.v2function)\n\n\n\n**APIs Used**:\n\n* bigquery.googleapis.com (Reason: Mirrors data from your Cloud Firestore collection in BigQuery.)\n\n\n\n**Access Required**:\n\n\n\nThis extension will operate with the following project IAM roles:\n\n* bigquery.dataEditor (Reason: Allows the extension to configure and export data into BigQuery.)\n\n* datastore.user (Reason: Allows the extension to write updates to the database.)\n\n* bigquery.user (Reason: Allows the extension to create and manage BigQuery materialized views.)\n","lifecycleEvents":[{"stage":"ON_INSTALL","processingMessage":"Configuring BigQuery Sync.","taskQueueTriggerFunction":"initBigQuerySync"},{"stage":"ON_UPDATE","processingMessage":"Configuring BigQuery Sync","taskQueueTriggerFunction":"setupBigQuerySync"},{"stage":"ON_CONFIGURE","processingMessage":"Configuring BigQuery Sync","taskQueueTriggerFunction":"setupBigQuerySync"}],"displayName":"Stream Firestore to BigQuery","events":[{"type":"firebase.extensions.firestore-counter.v1.onStart","description":"Occurs when a trigger has been called within the Extension, and will include data such as the context of the trigger request."},{"type":"firebase.extensions.firestore-counter.v1.onSuccess","description":"Occurs when a task completes successfully. The event will contain further details about specific results."},{"type":"firebase.extensions.firestore-counter.v1.onError","description":"Occurs when an issue has been experienced in the Extension. This will include any error data that has been included within the Error Exception."},{"type":"firebase.extensions.firestore-counter.v1.onCompletion","description":"Occurs when the function is settled. Provides no customized data other than the context."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onStart","description":"Occurs when a trigger has been called within the Extension, and will include data such as the context of the trigger request."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onSuccess","description":"Occurs when a task completes successfully. The event will contain further details about specific results."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onError","description":"Occurs when an issue has been experienced in the Extension. This will include any error data that has been included within the Error Exception."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onCompletion","description":"Occurs when the function is settled. Provides no customized data other than the context."},{"type":"firebase.extensions.big-query-export.v1.sync.start","description":"Occurs on a firestore document write event."}]},"fetchTime":"2025-04-22T16:19:47.253450Z","lastOperationName":"projects/firebaseextensions/operations/373aa99b-62f5-4774-9ed8-8f5c1eb00e33","state":"ACTIVE"},"params":{"DATABASE":"(default)","BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","EXCLUDE_OLD_DATA":"no","TABLE_PARTITIONING":"NONE","LOG_LEVEL":"silent","COLLECTION_PATH":"users","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","MAX_DISPATCHES_PER_SECOND":"100","DATASET_ID":"firestore_export","TABLE_ID":"bigqueryexampleproject_users","DATABASE_REGION":"nam5","WILDCARD_IDS":"false","VIEW_TYPE":"view","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","MAX_ENQUEUE_ATTEMPTS":"3","DATASET_LOCATION":"us"},"populatedPostinstallContent":"### See it in action\n\nYou can test out this extension right away!\n\n1.  Go to your [Cloud Firestore dashboard](https://console.firebase.google.com/project/bigqueryexampleproject-e4ef9/firestore/data) in the Firebase console.\n\n2.  If it doesn't already exist, create the collection you specified during installation: `users`\n\n3.  Create a document in the collection called `bigquery-mirror-test` that contains any fields with any values that you'd like.\n\n4.  Go to the [BigQuery web UI](https://console.cloud.google.com/bigquery?project=bigqueryexampleproject-e4ef9&p=bigqueryexampleproject-e4ef9&d=firestore_export) in the Google Cloud Platform console.\n\n5.  Query your **raw changelog table**, which should contain a single log of creating the `bigquery-mirror-test` document.\n\n    ```\n    SELECT *\n    FROM `bigqueryexampleproject-e4ef9.firestore_export.bigqueryexampleproject_users_raw_changelog`\n    ```\n\n6.  Query your **latest view**, which should return the latest change event for the only document present -- `bigquery-mirror-test`.\n\n    ```\n    SELECT *\n    FROM `bigqueryexampleproject-e4ef9.firestore_export.bigqueryexampleproject_users_raw_latest`\n    ```\n\n7.  Delete the `bigquery-mirror-test` document from [Cloud Firestore](https://console.firebase.google.com/project/bigqueryexampleproject-e4ef9/firestore/data).\n    The `bigquery-mirror-test` document will disappear from the **latest view** and a `DELETE` event will be added to the **raw changelog table**.\n\n8.  You can check the changelogs of a single document with this query:\n\n    ```\n    SELECT *\n    FROM `bigqueryexampleproject-e4ef9.firestore_export.bigqueryexampleproject_users_raw_changelog`\n    WHERE document_name = \"bigquery-mirror-test\"\n    ORDER BY TIMESTAMP ASC\n    ```\n\n### Using the extension\n\nWhenever a document is created, updated, imported, or deleted in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset which contains the following resources:\n\n- **raw changelog table:** [`bigqueryexampleproject_users_raw_changelog`](https://console.cloud.google.com/bigquery?project=bigqueryexampleproject-e4ef9&p=bigqueryexampleproject-e4ef9&d=firestore_export&t=bigqueryexampleproject_users_raw_changelog&page=table)\n- **latest view:** [`bigqueryexampleproject_users_raw_latest`](https://console.cloud.google.com/bigquery?project=bigqueryexampleproject-e4ef9&p=bigqueryexampleproject-e4ef9&d=firestore_export&t=bigqueryexampleproject_users_raw_latest&page=table)\n\nTo review the schema for these two resources, click the **Schema** tab for each resource in BigQuery.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`).\n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in users. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter as your target BigQuery project ID.\n\n2. Identify the service account on the source project associated with the extension. By default, it will be constructed as `ext-<extension-instance-id>@project-id.iam.gserviceaccount.com`. However, if the extension instance ID is too long, it may be truncated and 4 random characters appended to abide by service account length limits.\n\n3. To find the exact service account, navigate to IAM & Admin -> IAM in the Google Cloud Platform Console. Look for the service account listed with \"Name\" as \"Firebase Extensions <your extension instance ID> service account\". The value in the \"Principal\" column will be the service account that needs permissions granted in the target project.\n\n4. Grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID] [-s SERVICE_ACCOUNT]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID] [-ServiceAccount SERVICE_ACCOUNT]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-s`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-ServiceAccount`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n### _(Optional)_ Import existing documents\n\nYou can backfill your BigQuery dataset with all the documents in your collection using the import script.\n\nIf you don't run the import script, the extension only exports the content of documents that are created or changed after installation.\n\nThe import script can read all existing documents in a Cloud Firestore collection and insert them into the raw changelog table created by this extension. The script adds a special changelog for each document with the operation of `IMPORT` and the timestamp of epoch. This is to ensure that any operation on an imported document supersedes the `IMPORT`.\n\n**Important:** Run the import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nLearn more about using the import script to [backfill your existing collection](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n### _(Optional)_ Generate schema views\n\nAfter your data is in BigQuery, you can use the schema-views script (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\nLearn more about using the schema-views script to [generate schema views](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md).\n\n### Monitoring\n\nAs a best practice, you can [monitor the activity](https://firebase.google.com/docs/extensions/manage-installed-extensions#monitor) of your installed extension, including checks on its health, usage, and logs.\n","extensionRef":"firebase/firestore-bigquery-export","extensionVersion":"0.2.2","systemParams":{"firebaseextensions.v1beta.function/location":"us-central1","firebaseextensions.v1beta.function/memory":"256","firebaseextensions.v1beta.v2function/memory":"256Mi","firebaseextensions.v1beta.function/timeoutSeconds":"120","firebaseextensions.v1beta.function/vpcConnectorEgressSettings":"VPC_CONNECTOR_EGRESS_SETTINGS_UNSPECIFIED","firebaseextensions.v1beta.function/minInstances":"0"}},"lastOperationName":"projects/bigqueryexampleproject-e4ef9/operations/a61b124c-6b2a-4a85-ad68-8beffc0b723f","serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","lastOperationType":"CREATE","etag":"6f5055cc72d80ee6d7f93dc8315ae50a19612411f29d1ef679c70f1fde2da1ed","runtimeData":{"stateUpdateTime":"2025-04-24T19:01:43.791080021Z","processingState":{"state":"PROCESSING_COMPLETE","detailMessage":"Sync setup completed"}}}]}
[info] i  functions: preparing functions directory for uploading... 
[info] i  functions: packaged /home/wellington/Documentos/Git/BigQueryExampleProject/functions (64.97 KB) for uploading 
[debug] [2025-04-30T18:43:21.831Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:21.831Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:21.831Z] >>> [apiv2][query] GET https://cloudfunctions.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/-/functions [none]
[debug] [2025-04-30T18:43:23.465Z] <<< [apiv2][status] GET https://cloudfunctions.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/-/functions 200
[debug] [2025-04-30T18:43:23.466Z] <<< [apiv2][body] GET https://cloudfunctions.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/-/functions {"functions":[{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-initBigQuerySync","description":"Runs configuration for sycning with BigQuery","sourceArchiveUrl":"gs://firebase-mod-sources-prod/a8a3e54cd84beb56c7eb0f2379f70adbd1427513f860a1f544893c92df16a047","httpsTrigger":{"url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-initBigQuerySync","securityLevel":"SECURE_OPTIONAL"},"status":"ACTIVE","entryPoint":"initBigQuerySync","timeout":"540s","availableMemoryMb":256,"serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","updateTime":"2025-04-24T18:58:11.627Z","versionId":"2","labels":{"goog-dm":"firebase-ext-firestore-bigquery-export","goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions","goog-firebase-ext-iid":"firestore-bigquery-export","firebase-extensions-ar":"enabled"},"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false"},"runtime":"nodejs20","ingressSettings":"ALLOW_ALL","buildId":"539681f6-8d64-426e-b876-e7462e9cc571","buildEnvironmentVariables":{"GOOGLE_NODE_RUN_SCRIPTS":""},"buildName":"projects/893582587169/locations/us-central1/builds/539681f6-8d64-426e-b876-e7462e9cc571","dockerRegistry":"ARTIFACT_REGISTRY","automaticUpdatePolicy":{},"buildServiceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"},{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-syncBigQuery","description":"A task-triggered function that gets called on BigQuery sync","sourceArchiveUrl":"gs://firebase-mod-sources-prod/a8a3e54cd84beb56c7eb0f2379f70adbd1427513f860a1f544893c92df16a047","httpsTrigger":{"url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-syncBigQuery","securityLevel":"SECURE_OPTIONAL"},"status":"ACTIVE","entryPoint":"syncBigQuery","timeout":"540s","availableMemoryMb":256,"serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","updateTime":"2025-04-24T18:58:16.790Z","versionId":"2","labels":{"goog-dm":"firebase-ext-firestore-bigquery-export","goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions","goog-firebase-ext-iid":"firestore-bigquery-export","firebase-extensions-ar":"enabled"},"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false"},"runtime":"nodejs20","ingressSettings":"ALLOW_ALL","buildId":"aa720a7e-0c24-4c7a-928f-7299737e09a5","buildEnvironmentVariables":{"GOOGLE_NODE_RUN_SCRIPTS":""},"buildName":"projects/893582587169/locations/us-central1/builds/aa720a7e-0c24-4c7a-928f-7299737e09a5","dockerRegistry":"ARTIFACT_REGISTRY","automaticUpdatePolicy":{},"buildServiceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"},{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-setupBigQuerySync","description":"Runs configuration for sycning with BigQuery","sourceArchiveUrl":"gs://firebase-mod-sources-prod/a8a3e54cd84beb56c7eb0f2379f70adbd1427513f860a1f544893c92df16a047","httpsTrigger":{"url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-setupBigQuerySync","securityLevel":"SECURE_OPTIONAL"},"status":"ACTIVE","entryPoint":"setupBigQuerySync","timeout":"540s","availableMemoryMb":256,"serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","updateTime":"2025-04-24T18:57:30.170Z","versionId":"2","labels":{"goog-firebase-ext-iid":"firestore-bigquery-export","firebase-extensions-ar":"enabled","goog-dm":"firebase-ext-firestore-bigquery-export","goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions"},"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false"},"runtime":"nodejs20","ingressSettings":"ALLOW_ALL","buildId":"087f1c66-719a-427e-9758-ba59ba92c9fc","buildEnvironmentVariables":{"GOOGLE_NODE_RUN_SCRIPTS":""},"buildName":"projects/893582587169/locations/us-central1/builds/087f1c66-719a-427e-9758-ba59ba92c9fc","dockerRegistry":"ARTIFACT_REGISTRY","automaticUpdatePolicy":{},"buildServiceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"}]}
[debug] [2025-04-30T18:43:23.468Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:23.468Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:23.468Z] >>> [apiv2][query] GET https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/-/functions filter=environment%3D%22GEN_2%22
[debug] [2025-04-30T18:43:24.972Z] <<< [apiv2][status] GET https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/-/functions 200
[debug] [2025-04-30T18:43:24.972Z] <<< [apiv2][body] GET https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/-/functions {"functions":[{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-fsexportbigquery","description":"Listens for document changes in your specified Cloud Firestore collection, then exports the changes into BigQuery.","buildConfig":{"build":"projects/893582587169/locations/us-central1/builds/59c9e8df-cfe2-4ef4-b917-55b66938b533","runtime":"nodejs22","entryPoint":"fsexportbigquery","source":{"storageSource":{"bucket":"gcf-v2-sources-893582587169-us-central1","object":"ext-firestore-bigquery-export-fsexportbigquery/function-source.zip","generation":"1745520941600164"}},"dockerRepository":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts","sourceProvenance":{"resolvedStorageSource":{"bucket":"gcf-v2-sources-893582587169-us-central1","object":"ext-firestore-bigquery-export-fsexportbigquery/function-source.zip","generation":"1745520941600164"}},"dockerRegistry":"ARTIFACT_REGISTRY","serviceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"},"serviceConfig":{"service":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/services/ext-firestore-bigquery-export-fsexportbigquery","timeoutSeconds":120,"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","FUNCTION_SIGNATURE_TYPE":"cloudevent","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false","LOG_EXECUTION_ID":"true"},"maxInstanceCount":6,"ingressSettings":"ALLOW_INTERNAL_ONLY","uri":"https://ext-firestore-bigquery-export-fsexportbigquery-hijxxwf3ia-uc.a.run.app","serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","availableMemory":"256Mi","allTrafficOnLatestRevision":true,"revision":"ext-firestore-bigquery-export-fsexportbigquery-00002-cuw","maxInstanceRequestConcurrency":1,"availableCpu":"0.1666"},"eventTrigger":{"trigger":"projects/bigqueryexampleproject-e4ef9/locations/nam5/triggers/ext-firestore-bigquery-export-fsexportbigquery-548607","triggerRegion":"nam5","eventType":"google.cloud.firestore.document.v1.written","eventFilters":[{"attribute":"database","value":"(default)"},{"attribute":"document","value":"users/{documentId}","operator":"match-path-pattern"}],"pubsubTopic":"projects/bigqueryexampleproject-e4ef9/topics/eventarc-nam5-ext-firestore-bigquery-export-fsexportbigquery-548607-885","serviceAccountEmail":"893582587169-compute@developer.gserviceaccount.com","retryPolicy":"RETRY_POLICY_DO_NOT_RETRY"},"state":"ACTIVE","updateTime":"2025-04-24T18:57:21.075796035Z","labels":{"goog-dm":"firebase-ext-firestore-bigquery-export","goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions","goog-firebase-ext-iid":"firestore-bigquery-export"},"environment":"GEN_2","url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-fsexportbigquery","createTime":"2025-04-24T18:53:41.744810406Z","satisfiesPzi":true}]}
[info] i  functions: ensuring required API run.googleapis.com is enabled... 
[debug] [2025-04-30T18:43:24.978Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:24.979Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API eventarc.googleapis.com is enabled... 
[debug] [2025-04-30T18:43:24.979Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:24.979Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API pubsub.googleapis.com is enabled... 
[debug] [2025-04-30T18:43:24.979Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:24.980Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API storage.googleapis.com is enabled... 
[debug] [2025-04-30T18:43:24.980Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:24.980Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:24.980Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com [none]
[debug] [2025-04-30T18:43:24.980Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:43:24.984Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com [none]
[debug] [2025-04-30T18:43:24.984Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:43:24.987Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com [none]
[debug] [2025-04-30T18:43:24.988Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:43:24.992Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com [none]
[debug] [2025-04-30T18:43:24.992Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:43:26.181Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com 200
[debug] [2025-04-30T18:43:26.181Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com [omitted]
[info] ✔  functions: required API run.googleapis.com is enabled 
[debug] [2025-04-30T18:43:26.192Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com 200
[debug] [2025-04-30T18:43:26.192Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com [omitted]
[info] ✔  functions: required API eventarc.googleapis.com is enabled 
[debug] [2025-04-30T18:43:26.209Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com 200
[debug] [2025-04-30T18:43:26.209Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com [omitted]
[info] ✔  functions: required API storage.googleapis.com is enabled 
[debug] [2025-04-30T18:43:26.306Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com 200
[debug] [2025-04-30T18:43:26.306Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com [omitted]
[info] ✔  functions: required API pubsub.googleapis.com is enabled 
[info] i  functions: generating the service identity for pubsub.googleapis.com... 
[debug] [2025-04-30T18:43:26.307Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:26.307Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: generating the service identity for eventarc.googleapis.com... 
[debug] [2025-04-30T18:43:26.308Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:26.308Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:26.308Z] >>> [apiv2][query] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/pubsub.googleapis.com:generateServiceIdentity [none]
[debug] [2025-04-30T18:43:26.308Z] >>> [apiv2][query] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/eventarc.googleapis.com:generateServiceIdentity [none]
[debug] [2025-04-30T18:43:27.335Z] <<< [apiv2][status] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/eventarc.googleapis.com:generateServiceIdentity 200
[debug] [2025-04-30T18:43:27.335Z] <<< [apiv2][body] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/eventarc.googleapis.com:generateServiceIdentity {"name":"operations/finished.DONE_OPERATION","done":true,"response":{"@type":"type.googleapis.com/google.api.serviceusage.v1beta1.ServiceIdentity","email":"service-893582587169@gcp-sa-eventarc.iam.gserviceaccount.com","uniqueId":"102092505972539947691"}}
[debug] [2025-04-30T18:43:27.343Z] <<< [apiv2][status] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/pubsub.googleapis.com:generateServiceIdentity 200
[debug] [2025-04-30T18:43:27.344Z] <<< [apiv2][body] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/pubsub.googleapis.com:generateServiceIdentity {"name":"operations/finished.DONE_OPERATION","done":true,"response":{"@type":"type.googleapis.com/google.api.serviceusage.v1beta1.ServiceIdentity","email":"service-893582587169@gcp-sa-pubsub.iam.gserviceaccount.com","uniqueId":"114949547525185127171"}}
[debug] [2025-04-30T18:43:27.348Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:27.348Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:27.348Z] >>> [apiv2][query] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 [none]
[debug] [2025-04-30T18:43:28.362Z] <<< [apiv2][status] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 200
[debug] [2025-04-30T18:43:28.363Z] <<< [apiv2][body] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 {"projectNumber":"893582587169","projectId":"bigqueryexampleproject-e4ef9","lifecycleState":"ACTIVE","name":"BigQueryExampleProject","labels":{"firebase":"enabled","firebase-core":"disabled"},"createTime":"2025-04-24T16:34:06.379745Z"}
[debug] [2025-04-30T18:43:28.365Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:28.365Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:28.366Z] >>> [apiv2][query] GET https://cloudbilling.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/billingInfo [none]
[debug] [2025-04-30T18:43:29.903Z] <<< [apiv2][status] GET https://cloudbilling.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/billingInfo 200
[debug] [2025-04-30T18:43:29.903Z] <<< [apiv2][body] GET https://cloudbilling.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/billingInfo {"name":"projects/bigqueryexampleproject-e4ef9/billingInfo","projectId":"bigqueryexampleproject-e4ef9","billingAccountName":"billingAccounts/019DD6-64C0E7-5ED023","billingEnabled":true}
[debug] [2025-04-30T18:43:29.906Z] [functions] found 6 new HTTP functions, testing setIamPolicy permission...
[debug] [2025-04-30T18:43:29.907Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:29.907Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:29.907Z] >>> [apiv2][query] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions [none]
[debug] [2025-04-30T18:43:29.907Z] >>> [apiv2][(partial)header] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:43:29.907Z] >>> [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.setIamPolicy"]}
[debug] [2025-04-30T18:43:30.207Z] <<< [apiv2][status] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions 200
[debug] [2025-04-30T18:43:30.208Z] <<< [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.setIamPolicy"]}
[debug] [2025-04-30T18:43:30.208Z] [functions] found setIamPolicy permission, proceeding with deploy
[debug] [2025-04-30T18:43:30.208Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:30.209Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:30.209Z] >>> [apiv2][query] POST https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions:generateUploadUrl [none]
[debug] [2025-04-30T18:43:32.345Z] <<< [apiv2][status] POST https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions:generateUploadUrl 200
[debug] [2025-04-30T18:43:32.346Z] <<< [apiv2][body] POST https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions:generateUploadUrl {"uploadUrl":"https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/293e2440-4317-44d3-a819-8aff904e7e8a.zip?GoogleAccessId=service-893582587169@gcf-admin-robot.iam.gserviceaccount.com&Expires=1746040412&Signature=N4oAiTNWgPIsNUgmJM6mSribUA2oy1pKY1dVNjy0QGt%2F8KCJdaDEOrOSDa8STiXy1sle%2FdHfei%2FTAgi9FiOoleaLNJ2ki3Tc6ftxJ5xfmnw0dprWRLllv9gNJWpLef1gZSMjjnZOPqC7%2FgYNNvCRfZJrGn132ponnPi5vD0htEaTCqUE%2BH3SzVkFyahQWHNDuEZXCA4kzngBYPJPrJl9F%2B8WBmXiA6XVPhYLDN2StNWmeDUatC4foWAV4g83EajOnnkWffZpjUuxaiWuyK35lfoFJ9i52N%2FiLeznjzvY%2FkmDYgGEmDFk2xaGVazl73KBdXDwval3OzA4J7FppF2E4A%3D%3D","storageSource":{"bucket":"gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com","object":"293e2440-4317-44d3-a819-8aff904e7e8a.zip"}}
[debug] [2025-04-30T18:43:32.349Z] >>> [apiv2][query] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/293e2440-4317-44d3-a819-8aff904e7e8a.zip GoogleAccessId=service-893582587169%40gcf-admin-robot.iam.gserviceaccount.com&Expires=1746040412&Signature=N4oAiTNWgPIsNUgmJM6mSribUA2oy1pKY1dVNjy0QGt%2F8KCJdaDEOrOSDa8STiXy1sle%2FdHfei%2FTAgi9FiOoleaLNJ2ki3Tc6ftxJ5xfmnw0dprWRLllv9gNJWpLef1gZSMjjnZOPqC7%2FgYNNvCRfZJrGn132ponnPi5vD0htEaTCqUE%2BH3SzVkFyahQWHNDuEZXCA4kzngBYPJPrJl9F%2B8WBmXiA6XVPhYLDN2StNWmeDUatC4foWAV4g83EajOnnkWffZpjUuxaiWuyK35lfoFJ9i52N%2FiLeznjzvY%2FkmDYgGEmDFk2xaGVazl73KBdXDwval3OzA4J7FppF2E4A%3D%3D
[debug] [2025-04-30T18:43:32.349Z] >>> [apiv2][body] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/293e2440-4317-44d3-a819-8aff904e7e8a.zip [stream]
[debug] [2025-04-30T18:43:38.501Z] <<< [apiv2][status] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/293e2440-4317-44d3-a819-8aff904e7e8a.zip 200
[debug] [2025-04-30T18:43:38.502Z] <<< [apiv2][body] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/293e2440-4317-44d3-a819-8aff904e7e8a.zip [omitted]
[info] ✔  functions: functions folder uploaded successfully 
[info] i  functions: creating Node.js 22 (2nd Gen) function createuser(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function deleteuser(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function listusers(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function processdata(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function showuser(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function updateuser(us-central1)... 
[debug] [2025-04-30T18:43:38.559Z] Total Function Deployment time: 36
[debug] [2025-04-30T18:43:38.559Z] 6 Functions Deployed
[debug] [2025-04-30T18:43:38.559Z] 6 Functions Errored
[debug] [2025-04-30T18:43:38.559Z] 0 Function Deployments Aborted
[debug] [2025-04-30T18:43:38.559Z] Average Function Deployment time: 6.833333333333333
[info] 
[info] Functions deploy had errors with the following functions:
	createuser(us-central1)
	deleteuser(us-central1)
	listusers(us-central1)
	processdata(us-central1)
	showuser(us-central1)
	updateuser(us-central1)
[debug] [2025-04-30T18:43:39.018Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T18:43:39.018Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T18:43:39.018Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T18:43:39.018Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T18:43:39.018Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T18:43:39.018Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T18:43:39.019Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:39.019Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:43:39.019Z] >>> [apiv2][query] GET https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts [none]
[debug] [2025-04-30T18:43:40.561Z] <<< [apiv2][status] GET https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts 200
[debug] [2025-04-30T18:43:40.563Z] <<< [apiv2][body] GET https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts {"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts","format":"DOCKER","description":"This repository is created and used by Cloud Functions for storing function docker images.","labels":{"goog-managed-by":"cloudfunctions"},"createTime":"2025-04-24T18:45:25.809021Z","updateTime":"2025-04-24T18:58:38.433646Z","mode":"STANDARD_REPOSITORY","sizeBytes":"681973421","vulnerabilityScanningConfig":{"lastEnableTime":"2025-04-24T18:45:16.816360009Z","enablementState":"SCANNING_DISABLED","enablementStateReason":"API containerscanning.googleapis.com is not enabled."},"satisfiesPzi":true,"registryUri":"us-central1-docker.pkg.dev/bigqueryexampleproject-e4ef9/gcf-artifacts"}
[warn] ⚠  functions: No cleanup policy detected for repositories in us-central1. This may result in a small monthly bill as container images accumulate over time. 
[info] i  functions: Configuring cleanup policy for repository in us-central1. Images older than 1 days will be automatically deleted. 
[debug] [2025-04-30T18:44:11.390Z] Setting up artifact cleanup policy for region us-central1
[debug] [2025-04-30T18:44:11.391Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:11.391Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:11.392Z] >>> [apiv2][query] PATCH https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts updateMask=name%2CcleanupPolicies%2Clabels
[debug] [2025-04-30T18:44:11.392Z] >>> [apiv2][body] PATCH https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts {"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts","cleanupPolicies":{"firebase-functions-cleanup":{"id":"firebase-functions-cleanup","condition":{"tagState":"ANY","olderThan":"86400s"},"action":"DELETE"}},"labels":{"goog-managed-by":"cloudfunctions"}}
[debug] [2025-04-30T18:44:12.676Z] <<< [apiv2][status] PATCH https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts 200
[debug] [2025-04-30T18:44:12.676Z] <<< [apiv2][body] PATCH https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts {"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts","format":"DOCKER","description":"This repository is created and used by Cloud Functions for storing function docker images.","labels":{"goog-managed-by":"cloudfunctions"},"createTime":"2025-04-24T18:45:25.809021Z","updateTime":"2025-04-30T18:44:12.443453Z","mode":"STANDARD_REPOSITORY","cleanupPolicies":{"firebase-functions-cleanup":{"id":"firebase-functions-cleanup","action":"DELETE","condition":{"tagState":"ANY","olderThan":"86400s"}}},"vulnerabilityScanningConfig":{"lastEnableTime":"2025-04-24T18:45:16.816360009Z","enablementState":"SCANNING_DISABLED","enablementStateReason":"API containerscanning.googleapis.com is not enabled."},"satisfiesPzi":true}
[info] i  functions: Configured cleanup policy for repository in us-central1. 
[debug] [2025-04-30T18:44:12.677Z] Functions deploy failed.
[debug] [2025-04-30T18:44:12.677Z] {}
[debug] [2025-04-30T18:44:12.677Z] {}
[debug] [2025-04-30T18:44:12.677Z] {}
[debug] [2025-04-30T18:44:12.677Z] {}
[debug] [2025-04-30T18:44:12.677Z] {}
[debug] [2025-04-30T18:44:12.677Z] {}
[error] Error: There was an error deploying functions:
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[debug] [2025-04-30T18:44:54.624Z] ----------------------------------------------------------------------
[debug] [2025-04-30T18:44:54.627Z] Command:       /usr/local/bin/firebase /home/wellington/.cache/firebase/tools/lib/node_modules/firebase-tools/lib/bin/firebase deploy --only functions
[debug] [2025-04-30T18:44:54.627Z] CLI Version:   14.0.0
[debug] [2025-04-30T18:44:54.627Z] Platform:      linux
[debug] [2025-04-30T18:44:54.627Z] Node Version:  v20.18.2
[debug] [2025-04-30T18:44:54.627Z] Time:          Wed Apr 30 2025 15:44:54 GMT-0300 (GMT-03:00)
[debug] [2025-04-30T18:44:54.628Z] ----------------------------------------------------------------------
[debug] 
[debug] [2025-04-30T18:44:54.854Z] > command requires scopes: ["email","openid","https://www.googleapis.com/auth/cloudplatformprojects.readonly","https://www.googleapis.com/auth/firebase","https://www.googleapis.com/auth/cloud-platform"]
[debug] [2025-04-30T18:44:54.854Z] > authorizing via signed-in user (wellingtonsilva112000@gmail.com)
[debug] [2025-04-30T18:44:54.855Z] [iam] checking project bigqueryexampleproject-e4ef9 for permissions ["cloudfunctions.functions.create","cloudfunctions.functions.delete","cloudfunctions.functions.get","cloudfunctions.functions.list","cloudfunctions.functions.update","cloudfunctions.operations.get","firebase.projects.get"]
[debug] [2025-04-30T18:44:54.856Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:54.856Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:54.857Z] >>> [apiv2][query] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions [none]
[debug] [2025-04-30T18:44:54.857Z] >>> [apiv2][(partial)header] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:44:54.857Z] >>> [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.create","cloudfunctions.functions.delete","cloudfunctions.functions.get","cloudfunctions.functions.list","cloudfunctions.functions.update","cloudfunctions.operations.get","firebase.projects.get"]}
[debug] [2025-04-30T18:44:55.522Z] <<< [apiv2][status] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions 200
[debug] [2025-04-30T18:44:55.523Z] <<< [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.create","cloudfunctions.functions.delete","cloudfunctions.functions.get","cloudfunctions.functions.list","cloudfunctions.functions.update","cloudfunctions.operations.get","firebase.projects.get"]}
[debug] [2025-04-30T18:44:55.524Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:55.524Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:55.524Z] >>> [apiv2][query] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions [none]
[debug] [2025-04-30T18:44:55.524Z] >>> [apiv2][body] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions {"permissions":["iam.serviceAccounts.actAs"]}
[debug] [2025-04-30T18:44:56.796Z] <<< [apiv2][status] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions 404
[debug] [2025-04-30T18:44:56.796Z] <<< [apiv2][body] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions {"error":{"code":404,"message":"Unknown service account","status":"NOT_FOUND"}}
[debug] [2025-04-30T18:44:56.797Z] [functions] service account IAM check errored, deploy may fail: Request to https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions had HTTP Error: 404, Unknown service account {"name":"FirebaseError","children":[],"context":{"body":{"error":{"code":404,"message":"Unknown service account","status":"NOT_FOUND"}},"response":{"statusCode":404}},"exit":1,"message":"Request to https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions had HTTP Error: 404, Unknown service account","status":404}
[info] 
[info] === Deploying to 'bigqueryexampleproject-e4ef9'...
[info] 
[info] i  deploying functions 
[debug] [2025-04-30T18:44:56.801Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:56.801Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:56.801Z] >>> [apiv2][query] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 [none]
[debug] [2025-04-30T18:44:57.062Z] <<< [apiv2][status] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 200
[debug] [2025-04-30T18:44:57.062Z] <<< [apiv2][body] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 {"projectNumber":"893582587169","projectId":"bigqueryexampleproject-e4ef9","lifecycleState":"ACTIVE","name":"BigQueryExampleProject","labels":{"firebase":"enabled","firebase-core":"disabled"},"createTime":"2025-04-24T16:34:06.379745Z"}
[info] i  functions: preparing codebase default for deployment 
[info] i  functions: ensuring required API cloudfunctions.googleapis.com is enabled... 
[debug] [2025-04-30T18:44:57.065Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:57.065Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:57.065Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:57.065Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API cloudbuild.googleapis.com is enabled... 
[debug] [2025-04-30T18:44:57.066Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:57.067Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  artifactregistry: ensuring required API artifactregistry.googleapis.com is enabled... 
[debug] [2025-04-30T18:44:57.067Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:57.067Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:57.068Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com [none]
[debug] [2025-04-30T18:44:57.068Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:44:57.071Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com [none]
[debug] [2025-04-30T18:44:57.071Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:44:57.074Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com [none]
[debug] [2025-04-30T18:44:57.075Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:44:57.076Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com [none]
[debug] [2025-04-30T18:44:57.076Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:44:58.590Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com 200
[debug] [2025-04-30T18:44:58.591Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com [omitted]
[info] ✔  functions: required API cloudbuild.googleapis.com is enabled 
[debug] [2025-04-30T18:44:58.593Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com 200
[debug] [2025-04-30T18:44:58.593Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com [omitted]
[debug] [2025-04-30T18:44:58.596Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com 200
[debug] [2025-04-30T18:44:58.596Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com [omitted]
[info] ✔  functions: required API cloudfunctions.googleapis.com is enabled 
[debug] [2025-04-30T18:44:58.607Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com 200
[debug] [2025-04-30T18:44:58.608Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com [omitted]
[info] ✔  artifactregistry: required API artifactregistry.googleapis.com is enabled 
[debug] [2025-04-30T18:44:58.609Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:58.609Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:58.609Z] >>> [apiv2][query] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig [none]
[debug] [2025-04-30T18:44:59.589Z] <<< [apiv2][status] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig 200
[debug] [2025-04-30T18:44:59.590Z] <<< [apiv2][body] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig {"projectId":"bigqueryexampleproject-e4ef9","storageBucket":"bigqueryexampleproject-e4ef9.firebasestorage.app"}
[debug] [2025-04-30T18:44:59.591Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:59.591Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:44:59.591Z] >>> [apiv2][query] GET https://runtimeconfig.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/configs [none]
[debug] [2025-04-30T18:45:00.340Z] <<< [apiv2][status] GET https://runtimeconfig.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/configs 200
[debug] [2025-04-30T18:45:00.340Z] <<< [apiv2][body] GET https://runtimeconfig.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/configs {}
[debug] [2025-04-30T18:45:00.343Z] Validating nodejs source
[debug] [2025-04-30T18:45:01.043Z] > [functions] package.json contents: {
  "name": "functions",
  "description": "Cloud Functions for Firebase",
  "scripts": {
    "serve": "firebase emulators:start --only functions",
    "shell": "firebase functions:shell",
    "start": "npm run shell",
    "deploy": "firebase deploy --only functions",
    "logs": "firebase functions:log"
  },
  "engines": {
    "node": "22"
  },
  "main": "index.mjs",
  "dependencies": {
    "firebase-admin": "^12.6.0",
    "firebase-functions": "^6.0.1",
    "uuid": "^11.1.0"
  },
  "devDependencies": {
    "firebase-functions-test": "^3.1.0"
  },
  "private": true
}
[debug] [2025-04-30T18:45:01.043Z] Building nodejs source
[info] i  functions: Loading and analyzing source code for codebase default to determine what to deploy 
[debug] [2025-04-30T18:45:01.046Z] Could not find functions.yaml. Must use http discovery
[debug] [2025-04-30T18:45:01.063Z] Found firebase-functions binary at '/home/wellington/Documentos/Git/BigQueryExampleProject/functions/node_modules/.bin/firebase-functions'
[info] Serving at port 8492

[debug] [2025-04-30T18:45:01.866Z] Got response from /__/functions.yaml {"endpoints":{"createuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"createuser"},"deleteuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"deleteuser"},"listusers":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"listusers"},"processdata":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"processdata"},"showuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"showuser"},"updateuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"updateuser"}},"specVersion":"v1alpha1","requiredAPIs":[],"extensions":{}}
[info] i  extensions: ensuring required API firebaseextensions.googleapis.com is enabled... 
[debug] [2025-04-30T18:45:05.930Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:05.930Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:05.930Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com [none]
[debug] [2025-04-30T18:45:05.930Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:45:07.499Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com 200
[debug] [2025-04-30T18:45:07.499Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com [omitted]
[info] ✔  extensions: required API firebaseextensions.googleapis.com is enabled 
[debug] [2025-04-30T18:45:07.499Z] > command requires scopes: ["email","openid","https://www.googleapis.com/auth/cloudplatformprojects.readonly","https://www.googleapis.com/auth/firebase","https://www.googleapis.com/auth/cloud-platform"]
[debug] [2025-04-30T18:45:07.500Z] > authorizing via signed-in user (wellingtonsilva112000@gmail.com)
[debug] [2025-04-30T18:45:07.500Z] [iam] checking project bigqueryexampleproject-e4ef9 for permissions ["firebase.projects.get","firebaseextensions.instances.list"]
[debug] [2025-04-30T18:45:07.500Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:07.500Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:07.501Z] >>> [apiv2][query] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions [none]
[debug] [2025-04-30T18:45:07.501Z] >>> [apiv2][(partial)header] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:45:07.501Z] >>> [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["firebase.projects.get","firebaseextensions.instances.list"]}
[debug] [2025-04-30T18:45:08.723Z] <<< [apiv2][status] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions 200
[debug] [2025-04-30T18:45:08.724Z] <<< [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["firebase.projects.get","firebaseextensions.instances.list"]}
[debug] [2025-04-30T18:45:08.725Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:08.725Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:08.726Z] >>> [apiv2][query] GET https://firebaseextensions.googleapis.com/v1beta/projects/bigqueryexampleproject-e4ef9/instances pageSize=100&pageToken=
[debug] [2025-04-30T18:45:10.359Z] <<< [apiv2][status] GET https://firebaseextensions.googleapis.com/v1beta/projects/bigqueryexampleproject-e4ef9/instances 200
[debug] [2025-04-30T18:45:10.360Z] <<< [apiv2][body] GET https://firebaseextensions.googleapis.com/v1beta/projects/bigqueryexampleproject-e4ef9/instances {"instances":[{"name":"projects/bigqueryexampleproject-e4ef9/instances/firestore-bigquery-export","createTime":"2025-04-24T18:43:47.251262Z","updateTime":"2025-04-24T18:58:40.150355Z","state":"ACTIVE","config":{"name":"projects/bigqueryexampleproject-e4ef9/instances/firestore-bigquery-export/configurations/2661e7fb-2095-49c8-9edb-f7aeeb3fe7d6","createTime":"2025-04-24T18:43:47.251262Z","source":{"name":"projects/firebaseextensions/sources/1bcd5ca9-fd21-4b21-9984-8d6ee3a0a7c8","packageUri":"https://storage.googleapis.com/firebase-extensions-packages-prod/firebase-firestore-bigquery-export-0.2.2-713b5ff6-19ab-459d-8d21-b6eec0e8d5f9.zip","hash":"5df1592f3afe939bcf4a63279f7de800aade1153433d37d17a65109cf854dfa5","extensionRoot":"/","spec":{"specVersion":"v1beta","name":"firestore-bigquery-export","version":"0.2.2","description":"Sends realtime, incremental updates from a specified Cloud Firestore collection to BigQuery.","apis":[{"apiName":"bigquery.googleapis.com","reason":"Mirrors data from your Cloud Firestore collection in BigQuery."}],"roles":[{"role":"bigquery.dataEditor","reason":"Allows the extension to configure and export data into BigQuery."},{"role":"datastore.user","reason":"Allows the extension to write updates to the database."},{"role":"bigquery.user","reason":"Allows the extension to create and manage BigQuery materialized views."}],"resources":[{"name":"fsexportbigquery","type":"firebaseextensions.v1beta.v2function","propertiesYaml":"buildConfig:\n  runtime: nodejs22\neventTrigger:\n  eventFilters:\n  - attribute: database\n    value: ${DATABASE}\n  - attribute: document\n    operator: match-path-pattern\n    value: ${COLLECTION_PATH}/{documentId}\n  eventType: google.cloud.firestore.document.v1.written\n  triggerRegion: ${DATABASE_REGION}\nsourceDirectory: functions\n","description":"Listens for document changes in your specified Cloud Firestore collection, then exports the changes into BigQuery.","deletionPolicy":"DELETE"},{"name":"syncBigQuery","type":"firebaseextensions.v1beta.function","propertiesYaml":"runtime: nodejs20\ntaskQueueTrigger:\n  rateLimits:\n    maxConcurrentDispatches: 500\n    maxDispatchesPerSecond: ${param:MAX_DISPATCHES_PER_SECOND}\n  retryConfig:\n    maxAttempts: 5\n    minBackoffSeconds: 60\n","description":"A task-triggered function that gets called on BigQuery sync","deletionPolicy":"DELETE"},{"name":"initBigQuerySync","type":"firebaseextensions.v1beta.function","propertiesYaml":"runtime: nodejs20\ntaskQueueTrigger:\n  retryConfig:\n    maxAttempts: 15\n    minBackoffSeconds: 60\n","description":"Runs configuration for sycning with BigQuery","deletionPolicy":"DELETE"},{"name":"setupBigQuerySync","type":"firebaseextensions.v1beta.function","propertiesYaml":"runtime: nodejs20\ntaskQueueTrigger:\n  retryConfig:\n    maxAttempts: 15\n    minBackoffSeconds: 60\n","description":"Runs configuration for sycning with BigQuery","deletionPolicy":"DELETE"}],"billingRequired":true,"author":{"authorName":"Firebase","url":"https://firebase.google.com"},"contributors":[{"authorName":"Jan Wyszynski","email":"wyszynski@google.com","url":"https://github.com/IanWyszynski"}],"license":"Apache-2.0","releaseNotesUrl":"https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/CHANGELOG.md","sourceUrl":"https://github.com/firebase/extensions/tree/master/firestore-bigquery-export","params":[{"param":"DATASET_LOCATION","label":"BigQuery Dataset location","type":"SELECT","description":"Where do you want to deploy the BigQuery dataset created for this extension? For help selecting a location, refer to the [location selection guide](https://cloud.google.com/bigquery/docs/locations).","required":true,"options":[{"value":"us-central1","label":"Iowa (us-central1)"},{"value":"us-west4","label":"Las Vegas (us-west4)"},{"value":"europe-central2","label":"Warsaw (europe-central2)"},{"value":"us-west2","label":"Los Angeles (us-west2)"},{"value":"northamerica-northeast1","label":"Montreal (northamerica-northeast1)"},{"value":"us-east4","label":"Northern Virginia (us-east4)"},{"value":"us-west1","label":"Oregon (us-west1)"},{"value":"us-west3","label":"Salt Lake City (us-west3)"},{"value":"southamerica-east1","label":"Sao Paulo (southamerica-east1)"},{"value":"us-east1","label":"South Carolina (us-east1)"},{"value":"europe-west1","label":"Belgium (europe-west1)"},{"value":"europe-north1","label":"Finland (europe-north1)"},{"value":"europe-west3","label":"Frankfurt (europe-west3)"},{"value":"europe-west2","label":"London (europe-west2)"},{"value":"europe-west4","label":"Netherlands (europe-west4)"},{"value":"europe-west6","label":"Zurich (europe-west6)"},{"value":"asia-east1","label":"Taiwan (asia-east1)"},{"value":"asia-east2","label":"Hong Kong (asia-east2)"},{"value":"asia-southeast2","label":"Jakarta (asia-southeast2)"},{"value":"asia-south1","label":"Mumbai (asia-south1)"},{"value":"asia-southeast1","label":"Singapore (asia-southeast1)"},{"value":"asia-northeast2","label":"Osaka (asia-northeast2)"},{"value":"asia-northeast3","label":"Seoul (asia-northeast3)"},{"value":"asia-southeast1","label":"Singapore (asia-southeast1)"},{"value":"australia-southeast1","label":"Sydney (australia-southeast1)"},{"value":"asia-east1","label":"Taiwan (asia-east1)"},{"value":"asia-northeast1","label":"Tokyo (asia-northeast1)"},{"value":"us","label":"United States (multi-regional)"},{"value":"eu","label":"Europe (multi-regional)"}],"default":"us","immutable":true},{"param":"BIGQUERY_PROJECT_ID","label":"BigQuery Project ID","type":"STRING","description":"Override the default project for BigQuery instance. This can allow updates to be directed to to a BigQuery instance on another GCP project.","required":true,"default":"${PROJECT_ID}"},{"param":"DATABASE","label":"Firestore Instance ID","type":"STRING","description":"The Firestore database to use. Use \"(default)\" for the default database. You can view your available Firestore databases at https://console.cloud.google.com/firestore/databases.\n","required":true,"default":"(default)","example":"(default)"},{"param":"DATABASE_REGION","label":"Firestore Instance Location","type":"SELECT","description":"Where is the Firestore database located? You can check your current database location at https://console.cloud.google.com/firestore/databases.\n","required":true,"options":[{"value":"eur3","label":"Multi-region (Europe - Belgium and Netherlands)"},{"value":"nam5","label":"Multi-region (United States)"},{"value":"nam7","label":"Multi-region (Iowa, North Virginia, and Oklahoma)"},{"value":"us-central1","label":"Iowa (us-central1)"},{"value":"us-west1","label":"Oregon (us-west1)"},{"value":"us-west2","label":"Los Angeles (us-west2)"},{"value":"us-west3","label":"Salt Lake City (us-west3)"},{"value":"us-west4","label":"Las Vegas (us-west4)"},{"value":"us-east1","label":"South Carolina (us-east1)"},{"value":"us-east4","label":"Northern Virginia (us-east4)"},{"value":"us-east5","label":"Columbus (us-east5)"},{"value":"us-south1","label":"Dallas (us-south1)"},{"value":"northamerica-northeast1","label":"Montreal (northamerica-northeast1)"},{"value":"northamerica-northeast2","label":"Toronto (northamerica-northeast2)"},{"value":"northamerica-south1","label":"Queretaro (northamerica-south1)"},{"value":"southamerica-east1","label":"Sao Paulo (southamerica-east1)"},{"value":"southamerica-west1","label":"Santiago (southamerica-west1)"},{"value":"europe-west1","label":"Belgium (europe-west1)"},{"value":"europe-west2","label":"London (europe-west2)"},{"value":"europe-west3","label":"Frankfurt (europe-west3)"},{"value":"europe-west4","label":"Netherlands (europe-west4)"},{"value":"europe-west6","label":"Zurich (europe-west6)"},{"value":"europe-west8","label":"Milan (europe-west8)"},{"value":"europe-west9","label":"Paris (europe-west9)"},{"value":"europe-west10","label":"Berlin (europe-west10)"},{"value":"europe-west12","label":"Turin (europe-west12)"},{"value":"europe-southwest1","label":"Madrid (europe-southwest1)"},{"value":"europe-north1","label":"Finland (europe-north1)"},{"value":"europe-north2","label":"Stockholm (europe-north2)"},{"value":"europe-central2","label":"Warsaw (europe-central2)"},{"value":"me-central1","label":"Doha (me-central1)"},{"value":"me-central2","label":"Dammam (me-central2)"},{"value":"me-west1","label":"Tel Aviv (me-west1)"},{"value":"asia-south1","label":"Mumbai (asia-south1)"},{"value":"asia-south2","label":"Delhi (asia-south2)"},{"value":"asia-southeast1","label":"Singapore (asia-southeast1)"},{"value":"asia-southeast2","label":"Jakarta (asia-southeast2)"},{"value":"asia-east1","label":"Taiwan (asia-east1)"},{"value":"asia-east2","label":"Hong Kong (asia-east2)"},{"value":"asia-northeast1","label":"Tokyo (asia-northeast1)"},{"value":"asia-northeast2","label":"Osaka (asia-northeast2)"},{"value":"asia-northeast3","label":"Seoul (asia-northeast3)"},{"value":"australia-southeast1","label":"Sydney (australia-southeast1)"},{"value":"australia-southeast2","label":"Melbourne (australia-southeast2)"},{"value":"africa-south1","label":"Johannesburg (africa-south1)"}]},{"param":"COLLECTION_PATH","label":"Collection path","type":"STRING","description":"What is the path of the collection that you would like to export? You may use `{wildcard}` notation to match a subcollection of all documents in a collection (for example: `chatrooms/{chatid}/posts`). Parent Firestore Document IDs from `{wildcards}` can be returned in `path_params` as a JSON formatted string.","required":true,"default":"posts","example":"posts","validationRegex":"^[^/]+(/[^/]+/[^/]+)*$","validationErrorMessage":"Firestore collection paths must be an odd number of segments separated by slashes, e.g. \"path/to/collection\"."},{"param":"WILDCARD_IDS","label":"Enable Wildcard Column field with Parent Firestore Document IDs","type":"SELECT","description":"If enabled, creates a column containing a JSON object of all wildcard ids from a documents path.","options":[{"value":"false","label":"No"},{"value":"true","label":"Yes"}],"default":"false"},{"param":"DATASET_ID","label":"Dataset ID","type":"STRING","description":"What ID would you like to use for your BigQuery dataset? This extension will create the dataset, if it doesn't already exist.","required":true,"default":"firestore_export","example":"firestore_export","validationRegex":"^[a-zA-Z0-9_]+$","validationErrorMessage":"BigQuery dataset IDs must be alphanumeric (plus underscores) and must be no more than 1024 characters.\n"},{"param":"TABLE_ID","label":"Table ID","type":"STRING","description":"What identifying prefix would you like to use for your table and view inside your BigQuery dataset? This extension will create the table and view, if they don't already exist.","required":true,"default":"posts","example":"posts","validationRegex":"^[a-zA-Z0-9_]+$","validationErrorMessage":"BigQuery table IDs must be alphanumeric (plus underscores) and must be no more than 1024 characters.\n"},{"param":"TABLE_PARTITIONING","label":"BigQuery SQL table Time Partitioning option type","type":"SELECT","description":"This parameter will allow you to partition the BigQuery table and BigQuery view created by the extension based on data ingestion time. You may select the granularity of partitioning based upon one of: HOUR, DAY, MONTH, YEAR. This will generate one partition per day, hour, month or year, respectively.","options":[{"value":"HOUR","label":"hour"},{"value":"DAY","label":"day"},{"value":"MONTH","label":"month"},{"value":"YEAR","label":"year"},{"value":"NONE","label":"none"}],"default":"NONE"},{"param":"TIME_PARTITIONING_FIELD","label":"BigQuery Time Partitioning column name","type":"STRING","description":"BigQuery table column/schema field name for TimePartitioning. You can choose schema available as `timestamp` OR a new custom defined column that will be assigned to the selected Firestore Document field below. Defaults to pseudo column _PARTITIONTIME if unspecified. Cannot be changed if Table is already partitioned."},{"param":"TIME_PARTITIONING_FIRESTORE_FIELD","label":"Firestore Document field name for BigQuery SQL Time Partitioning field option","type":"STRING","description":"This parameter will allow you to partition the BigQuery table created by the extension based on selected. The Firestore Document field value must be a top-level TIMESTAMP, DATETIME, DATE field BigQuery string format or Firestore timestamp(will be converted to BigQuery TIMESTAMP). Cannot be changed if Table is already partitioned.\n example: `postDate`(Ensure that the Firestore-BigQuery export extension\ncreates the dataset and table before initiating any backfill scripts.\n This step is crucial for the partitioning to function correctly. It is\nessential for the script to insert data into an already partitioned table.)"},{"param":"TIME_PARTITIONING_FIELD_TYPE","label":"BigQuery SQL Time Partitioning table schema field(column) type","type":"SELECT","description":"Parameter for BigQuery SQL schema field type for the selected Time Partitioning Firestore Document field option. Cannot be changed if Table is already partitioned.","options":[{"value":"TIMESTAMP","label":"TIMESTAMP"},{"value":"DATETIME","label":"DATETIME"},{"value":"DATE","label":"DATE"},{"value":"omit","label":"omit"}],"default":"omit"},{"param":"CLUSTERING","label":"BigQuery SQL table clustering","type":"STRING","description":"This parameter allows you to set up clustering for the BigQuery table created by the extension. Specify up to 4 comma-separated fields (for example:  `data,document_id,timestamp` - no whitespaces). The order of the specified  columns determines the sort order of the data. \nnote: Cluster columns must be top-level, non-repeated columns of one of the  following types: BIGNUMERIC, BOOL, DATE, DATETIME, GEOGRAPHY, INT64, NUMERIC,  RANGE, STRING, TIMESTAMP. Clustering will not be added if a field with an invalid type is present in this parameter.\nAvailable schema extensions table fields for clustering include: `document_id, document_name, timestamp, event_id,  operation, data`.","example":"data,document_id,timestamp","validationRegex":"^[^,\\s]+(?:,[^,\\s]+){0,3}$","validationErrorMessage":"No whitespaces. Max 4 fields. e.g. `data,timestamp,event_id,operation`"},{"param":"MAX_DISPATCHES_PER_SECOND","label":"Maximum number of synced documents per second","type":"STRING","description":"This parameter will set the maximum number of syncronised documents per second with BQ. Please note, any other external updates to a Big Query table will be included within this quota. Ensure that you have a set a low enough number to compensate. Defaults to 100.","default":"100","validationRegex":"^([1-9]|[1-9][0-9]|[1-4][0-9]{2}|500)$","validationErrorMessage":"Please select a number between 1 and 500"},{"param":"VIEW_TYPE","label":"View Type","type":"SELECT","description":"Select the type of view to create in BigQuery. A regular view is a virtual table defined by a SQL query.  A materialized view persists the results of a query for faster access, with either incremental or  non-incremental updates. Please note that materialized views in this extension come with several  important caveats and limitations - carefully review the pre-install documentation before selecting  these options to ensure they are appropriate for your use case.","required":true,"options":[{"value":"view","label":"View"},{"value":"materialized_incremental","label":"Materialized View (Incremental)"},{"value":"materialized_non_incremental","label":"Materialized View (Non-incremental)"}],"default":"view"},{"param":"MAX_STALENESS","label":"Maximum Staleness Duration","type":"STRING","description":"For materialized views only: Specifies the maximum staleness acceptable for the materialized view.  Should be specified as an INTERVAL value following BigQuery SQL syntax.  This parameter will only take effect if View Type is set to a materialized view option.","example":"INTERVAL \"8:0:0\" HOUR TO SECOND"},{"param":"REFRESH_INTERVAL_MINUTES","label":"Refresh Interval (Minutes)","type":"STRING","description":"For materialized views only: Specifies how often the materialized view should be refreshed, in minutes.  This parameter will only take effect if View Type is set to a materialized view option.","example":"60","validationRegex":"^[1-9][0-9]*$","validationErrorMessage":"Must be a positive integer"},{"param":"BACKUP_COLLECTION","label":"Backup Collection Name","type":"STRING","description":"This (optional) parameter will allow you to specify a collection for which failed BigQuery updates will be written to."},{"param":"TRANSFORM_FUNCTION","label":"Transform function URL","type":"STRING","description":"Specify a function URL to call that will transform the payload that will be written to BigQuery. See the pre-install documentation for more details.","example":"https://us-west1-my-project-id.cloudfunctions.net/myTransformFunction"},{"param":"USE_NEW_SNAPSHOT_QUERY_SYNTAX","label":"Use new query syntax for snapshots","type":"SELECT","description":"If enabled, snapshots will be generated with the new query syntax, which should be more performant, and avoid potential resource limitations.","required":true,"options":[{"value":"yes","label":"Yes"},{"value":"no","label":"No"}],"default":"no"},{"param":"EXCLUDE_OLD_DATA","label":"Exclude old data payloads","type":"SELECT","description":"If enabled, table rows will never contain old data (document snapshot before the Firestore onDocumentUpdate event: `change.before.data()`). The reduction in data should be more performant, and avoid potential resource limitations.","options":[{"value":"yes","label":"Yes"},{"value":"no","label":"No"}],"default":"no"},{"param":"KMS_KEY_NAME","label":"Cloud KMS key name","type":"STRING","description":"Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS. If this parameter is set, the extension will specify the KMS key name when creating the BQ table. See the PREINSTALL.md for more details.","validationRegex":"projects/([^/]+)/locations/([^/]+)/keyRings/([^/]+)/cryptoKeys/([^/]+)","validationErrorMessage":"The key name must be of the format 'projects/PROJECT_NAME/locations/KEY_RING_LOCATION/keyRings/KEY_RING_ID/cryptoKeys/KEY_ID'."},{"param":"MAX_ENQUEUE_ATTEMPTS","label":"Maximum number of enqueue attempts","type":"STRING","description":"This parameter will set the maximum number of attempts to enqueue a document to cloud tasks for export to BigQuery.","default":"3","validationRegex":"^(10|[1-9])$","validationErrorMessage":"Please select an integer between 1 and 10"},{"param":"LOG_LEVEL","label":"Log level","type":"SELECT","description":"The log level for the extension. The log level controls the verbosity of the extension's logs. The available log levels are: debug, info, warn, and error. To reduce the volume of logs, use a log level of warn or error.","required":true,"options":[{"value":"debug","label":"Debug"},{"value":"info","label":"Info"},{"value":"warn","label":"Warn"},{"value":"error","label":"Error"},{"value":"silent","label":"Silent"}],"default":"info"}],"preinstallContent":"Use this extension to export the documents in a Cloud Firestore collection to BigQuery. Exports are realtime and incremental, so the data in BigQuery is a mirror of your content in Cloud Firestore.\n\nThe extension creates and updates a [dataset](https://cloud.google.com/bigquery/docs/datasets-intro) containing the following two BigQuery resources:\n\n- A [table](https://cloud.google.com/bigquery/docs/tables-intro) of raw data that stores a full change history of the documents within your collection. This table includes a number of metadata fields so that BigQuery can display the current state of your data. The principle metadata fields are `timestamp`, `document_name`, and the `operation` for the document change.\n- A [view](https://cloud.google.com/bigquery/docs/views-intro) which represents the current state of the data within your collection. It also shows a log of the latest `operation` for each document (`CREATE`, `UPDATE`, or `IMPORT`).\n\n*Warning*: A BigQuery table corresponding to your configuration will be automatically generated upon installing or updating this extension. Manual table creation may result in discrepancies with your configured settings.\n\nIf you create, update, or delete a document in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`). \n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in ${param:COLLECTION_PATH}. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\nNote: To enable partitioning for a Big Query database, the following fields are required:\n\n - Time Partitioning option type\n - Time partitioning column name\n - Time partiitioning table schema\n - Firestore document field name\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n\n\n#### Additional setup\n\nBefore installing this extension, you'll need to:\n\n- [Set up Cloud Firestore in your Firebase project.](https://firebase.google.com/docs/firestore/quickstart)\n- [Link your Firebase project to BigQuery.](https://support.google.com/firebase/answer/6318765)\n\n\n#### Import existing documents\n\nTo import existing documents you can run the external [import script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n**Important:** Run the external import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nWithout use of this import script, the extension only exports the content of documents that are created or changed after installation.\n\n#### Transform function\n\nPrior to sending the document change to BigQuery, you have an opportunity to transform the data with an HTTP function. The payload will contain the following:\n\n```\n{ \n  data: [{\n    insertId: int;\n    json: {\n      timestamp: int;\n      event_id: int;\n      document_name: string;\n      document_id: int;\n      operation: ChangeType;\n      data: string;\n    },\n  }]\n}\n```\n\nThe response should be indentical in structure.\n\n#### Materialized Views\n\nThis extension supports both regular views and materialized views in BigQuery. While regular views compute their results each time they're queried, materialized views store their query results, providing faster access at the cost of additional storage.\n\nThere are two types of materialized views available:\n\n1. **Non-incremental Materialized Views**: These views support more complex queries including filtering on aggregated fields, but require complete recomputation during refresh.\n\n2. **Incremental Materialized Views**: These views update more efficiently by processing only new or changed records, but come with query restrictions. Most notably, they don't allow filtering or partitioning on aggregated fields in their defining SQL, among other limitations.\n\n**Important Considerations:**\n- Neither type of materialized view in this extension currently supports partitioning or clustering\n- Both types allow you to configure refresh intervals and maximum staleness settings during extension installation or configuration\n- Once created, a materialized view's SQL definition cannot be modified. If you reconfigure the extension to change either the view type (incremental vs non-incremental) or the SQL query, the extension will drop the existing materialized view and recreate it\n- Carefully consider your use case before choosing materialized views:\n  - They incur additional storage costs as they cache query results\n  - Non-incremental views may have higher processing costs during refresh\n  - Incremental views have more query restrictions but are more efficient to update\n\nExample of a non-incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    allow_non_incremental_definition = true,\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\n  AS (\n    WITH latests AS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY document_name\n    )\n    SELECT *\n    FROM latests\n    WHERE operation != \"DELETE\"\n  )\n```\n\nExample of an incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\nAS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM\n        `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY\n        document_name\n    )\n```\n\nPlease review [BigQuery's documentation on materialized views](https://cloud.google.com/bigquery/docs/materialized-views-intro) to fully understand the implications for your use case.\n\n#### Using Customer Managed Encryption Keys\n\nBy default, BigQuery encrypts your content stored at rest. BigQuery handles and manages this default encryption for you without any additional actions on your part.\n\nIf you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery. Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS.\n\nFor more general information on this, see [the docs](https://cloud.google.com/bigquery/docs/customer-managed-encryption).\n\nTo use CMEK and the Key Management Service (KMS) with this extension\n1. [Enable the KMS API in your Google Cloud Project](https://console.cloud.google.com/apis/enableflow?apiid=cloudkms.googleapis.com).\n2. Create a keyring and keychain in the KMS. Note that the region of the keyring and key *must* match the region of your bigquery dataset\n3. Grant the BigQuery service account permission to encrypt and decrypt using that key. The Cloud KMS CryptoKey Encrypter/Decrypter role grants this permission. First find your project number. You can find this for example on the cloud console dashboard `https://console.cloud.google.com/home/dashboard?project={PROJECT_ID}`. The service account which needs the Encrypter/Decrypter role is then `bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com`. You can grant this role through the credentials service in the console, or through the CLI:\n```\ngcloud kms keys add-iam-policy-binding \\\n--project=KMS_PROJECT_ID \\\n--member serviceAccount:bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com \\\n--role roles/cloudkms.cryptoKeyEncrypterDecrypter \\\n--location=KMS_KEY_LOCATION \\\n--keyring=KMS_KEY_RING \\\nKMS_KEY\n```\n4. When installing this extension, enter the resource name of your key. It will look something like the following:\n```\nprojects/<YOUR PROJECT ID>/locations/<YOUR REGION>/keyRings/<YOUR KEY RING NAME>/cryptoKeys/<YOUR KEY NAME>\n```\nIf you follow these steps, your changelog table should be created using your customer-managed encryption.\n\n#### Generate schema views\n\nAfter your data is in BigQuery, you can run the [schema-views script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md) (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter to your target BigQuery project ID.\n\n2. After installation, you'll need to grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n#### Mitigating Data Loss During Extension Updates\n\nWhen updating or reconfiguring this extension, there may be a brief period where data streaming from Firestore to BigQuery is interrupted. While this limitation exists within the Extensions platform, we provide two strategies to mitigate potential data loss.\n\n##### Strategy 1: Post-Update Import\nAfter reconfiguring the extension, run the import script on your collection to ensure all data is captured. Refer to the \"Import Existing Documents\" section above for detailed steps.\n\n##### Strategy 2: Parallel Instance Method\n1. Install a second instance of the extension that streams to a new BigQuery table\n2. Reconfigure the original extension\n3. Once the original extension is properly configured and streaming events\n4. Uninstall the second instance\n5. Run a BigQuery merge job to combine the data from both tables\n\n##### Considerations\n- Strategy 1 is simpler but may result in duplicate records that need to be deduplicated\n- Strategy 2 requires more setup but provides better data continuity\n- Choose the strategy that best aligns with your data consistency requirements and operational constraints\n\n#### Billing\nTo install an extension, your project must be on the [Blaze (pay as you go) plan](https://firebase.google.com/pricing)\n\n- This extension uses other Firebase and Google Cloud Platform services, which have associated charges if you exceed the service’s no-cost tier:\n  - BigQuery (this extension writes to BigQuery with [streaming inserts](https://cloud.google.com/bigquery/pricing#streaming_pricing))\n  - Cloud Firestore\n  - Cloud Functions (Node.js 10+ runtime. [See FAQs](https://firebase.google.com/support/faq#extensions-pricing))","postinstallContent":"### See it in action\n\nYou can test out this extension right away!\n\n1.  Go to your [Cloud Firestore dashboard](https://console.firebase.google.com/project/${param:BIGQUERY_PROJECT_ID}/firestore/data) in the Firebase console.\n\n2.  If it doesn't already exist, create the collection you specified during installation: `${param:COLLECTION_PATH}`\n\n3.  Create a document in the collection called `bigquery-mirror-test` that contains any fields with any values that you'd like.\n\n4.  Go to the [BigQuery web UI](https://console.cloud.google.com/bigquery?project=${param:BIGQUERY_PROJECT_ID}&p=${param:BIGQUERY_PROJECT_ID}&d=${param:DATASET_ID}) in the Google Cloud Platform console.\n\n5.  Query your **raw changelog table**, which should contain a single log of creating the `bigquery-mirror-test` document.\n\n    ```\n    SELECT *\n    FROM `${param:BIGQUERY_PROJECT_ID}.${param:DATASET_ID}.${param:TABLE_ID}_raw_changelog`\n    ```\n\n6.  Query your **latest view**, which should return the latest change event for the only document present -- `bigquery-mirror-test`.\n\n    ```\n    SELECT *\n    FROM `${param:BIGQUERY_PROJECT_ID}.${param:DATASET_ID}.${param:TABLE_ID}_raw_latest`\n    ```\n\n7.  Delete the `bigquery-mirror-test` document from [Cloud Firestore](https://console.firebase.google.com/project/${param:BIGQUERY_PROJECT_ID}/firestore/data).\n    The `bigquery-mirror-test` document will disappear from the **latest view** and a `DELETE` event will be added to the **raw changelog table**.\n\n8.  You can check the changelogs of a single document with this query:\n\n    ```\n    SELECT *\n    FROM `${param:BIGQUERY_PROJECT_ID}.${param:DATASET_ID}.${param:TABLE_ID}_raw_changelog`\n    WHERE document_name = \"bigquery-mirror-test\"\n    ORDER BY TIMESTAMP ASC\n    ```\n\n### Using the extension\n\nWhenever a document is created, updated, imported, or deleted in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset which contains the following resources:\n\n- **raw changelog table:** [`${param:TABLE_ID}_raw_changelog`](https://console.cloud.google.com/bigquery?project=${param:BIGQUERY_PROJECT_ID}&p=${param:BIGQUERY_PROJECT_ID}&d=${param:DATASET_ID}&t=${param:TABLE_ID}_raw_changelog&page=table)\n- **latest view:** [`${param:TABLE_ID}_raw_latest`](https://console.cloud.google.com/bigquery?project=${param:BIGQUERY_PROJECT_ID}&p=${param:BIGQUERY_PROJECT_ID}&d=${param:DATASET_ID}&t=${param:TABLE_ID}_raw_latest&page=table)\n\nTo review the schema for these two resources, click the **Schema** tab for each resource in BigQuery.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`).\n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in ${param:COLLECTION_PATH}. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter as your target BigQuery project ID.\n\n2. Identify the service account on the source project associated with the extension. By default, it will be constructed as `ext-<extension-instance-id>@project-id.iam.gserviceaccount.com`. However, if the extension instance ID is too long, it may be truncated and 4 random characters appended to abide by service account length limits.\n\n3. To find the exact service account, navigate to IAM & Admin -> IAM in the Google Cloud Platform Console. Look for the service account listed with \"Name\" as \"Firebase Extensions <your extension instance ID> service account\". The value in the \"Principal\" column will be the service account that needs permissions granted in the target project.\n\n4. Grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID] [-s SERVICE_ACCOUNT]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID] [-ServiceAccount SERVICE_ACCOUNT]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-s`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-ServiceAccount`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n### _(Optional)_ Import existing documents\n\nYou can backfill your BigQuery dataset with all the documents in your collection using the import script.\n\nIf you don't run the import script, the extension only exports the content of documents that are created or changed after installation.\n\nThe import script can read all existing documents in a Cloud Firestore collection and insert them into the raw changelog table created by this extension. The script adds a special changelog for each document with the operation of `IMPORT` and the timestamp of epoch. This is to ensure that any operation on an imported document supersedes the `IMPORT`.\n\n**Important:** Run the import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nLearn more about using the import script to [backfill your existing collection](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n### _(Optional)_ Generate schema views\n\nAfter your data is in BigQuery, you can use the schema-views script (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\nLearn more about using the schema-views script to [generate schema views](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md).\n\n### Monitoring\n\nAs a best practice, you can [monitor the activity](https://firebase.google.com/docs/extensions/manage-installed-extensions#monitor) of your installed extension, including checks on its health, usage, and logs.\n","readmeContent":"# Stream Firestore to BigQuery\n\n**Author**: Firebase (**[https://firebase.google.com](https://firebase.google.com)**)\n\n**Description**: Sends realtime, incremental updates from a specified Cloud Firestore collection to BigQuery.\n\n\n\n**Details**: Use this extension to export the documents in a Cloud Firestore collection to BigQuery. Exports are realtime and incremental, so the data in BigQuery is a mirror of your content in Cloud Firestore.\n\nThe extension creates and updates a [dataset](https://cloud.google.com/bigquery/docs/datasets-intro) containing the following two BigQuery resources:\n\n- A [table](https://cloud.google.com/bigquery/docs/tables-intro) of raw data that stores a full change history of the documents within your collection. This table includes a number of metadata fields so that BigQuery can display the current state of your data. The principle metadata fields are `timestamp`, `document_name`, and the `operation` for the document change.\n- A [view](https://cloud.google.com/bigquery/docs/views-intro) which represents the current state of the data within your collection. It also shows a log of the latest `operation` for each document (`CREATE`, `UPDATE`, or `IMPORT`).\n\n*Warning*: A BigQuery table corresponding to your configuration will be automatically generated upon installing or updating this extension. Manual table creation may result in discrepancies with your configured settings.\n\nIf you create, update, or delete a document in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`). \n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in ${param:COLLECTION_PATH}. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\nNote: To enable partitioning for a Big Query database, the following fields are required:\n\n - Time Partitioning option type\n - Time partitioning column name\n - Time partiitioning table schema\n - Firestore document field name\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n\n\n#### Additional setup\n\nBefore installing this extension, you'll need to:\n\n- [Set up Cloud Firestore in your Firebase project.](https://firebase.google.com/docs/firestore/quickstart)\n- [Link your Firebase project to BigQuery.](https://support.google.com/firebase/answer/6318765)\n\n\n#### Import existing documents\n\nTo import existing documents you can run the external [import script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n**Important:** Run the external import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nWithout use of this import script, the extension only exports the content of documents that are created or changed after installation.\n\n#### Transform function\n\nPrior to sending the document change to BigQuery, you have an opportunity to transform the data with an HTTP function. The payload will contain the following:\n\n```\n{ \n  data: [{\n    insertId: int;\n    json: {\n      timestamp: int;\n      event_id: int;\n      document_name: string;\n      document_id: int;\n      operation: ChangeType;\n      data: string;\n    },\n  }]\n}\n```\n\nThe response should be indentical in structure.\n\n#### Materialized Views\n\nThis extension supports both regular views and materialized views in BigQuery. While regular views compute their results each time they're queried, materialized views store their query results, providing faster access at the cost of additional storage.\n\nThere are two types of materialized views available:\n\n1. **Non-incremental Materialized Views**: These views support more complex queries including filtering on aggregated fields, but require complete recomputation during refresh.\n\n2. **Incremental Materialized Views**: These views update more efficiently by processing only new or changed records, but come with query restrictions. Most notably, they don't allow filtering or partitioning on aggregated fields in their defining SQL, among other limitations.\n\n**Important Considerations:**\n- Neither type of materialized view in this extension currently supports partitioning or clustering\n- Both types allow you to configure refresh intervals and maximum staleness settings during extension installation or configuration\n- Once created, a materialized view's SQL definition cannot be modified. If you reconfigure the extension to change either the view type (incremental vs non-incremental) or the SQL query, the extension will drop the existing materialized view and recreate it\n- Carefully consider your use case before choosing materialized views:\n  - They incur additional storage costs as they cache query results\n  - Non-incremental views may have higher processing costs during refresh\n  - Incremental views have more query restrictions but are more efficient to update\n\nExample of a non-incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    allow_non_incremental_definition = true,\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\n  AS (\n    WITH latests AS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY document_name\n    )\n    SELECT *\n    FROM latests\n    WHERE operation != \"DELETE\"\n  )\n```\n\nExample of an incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\nAS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM\n        `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY\n        document_name\n    )\n```\n\nPlease review [BigQuery's documentation on materialized views](https://cloud.google.com/bigquery/docs/materialized-views-intro) to fully understand the implications for your use case.\n\n#### Using Customer Managed Encryption Keys\n\nBy default, BigQuery encrypts your content stored at rest. BigQuery handles and manages this default encryption for you without any additional actions on your part.\n\nIf you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery. Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS.\n\nFor more general information on this, see [the docs](https://cloud.google.com/bigquery/docs/customer-managed-encryption).\n\nTo use CMEK and the Key Management Service (KMS) with this extension\n1. [Enable the KMS API in your Google Cloud Project](https://console.cloud.google.com/apis/enableflow?apiid=cloudkms.googleapis.com).\n2. Create a keyring and keychain in the KMS. Note that the region of the keyring and key *must* match the region of your bigquery dataset\n3. Grant the BigQuery service account permission to encrypt and decrypt using that key. The Cloud KMS CryptoKey Encrypter/Decrypter role grants this permission. First find your project number. You can find this for example on the cloud console dashboard `https://console.cloud.google.com/home/dashboard?project={PROJECT_ID}`. The service account which needs the Encrypter/Decrypter role is then `bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com`. You can grant this role through the credentials service in the console, or through the CLI:\n```\ngcloud kms keys add-iam-policy-binding \\\n--project=KMS_PROJECT_ID \\\n--member serviceAccount:bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com \\\n--role roles/cloudkms.cryptoKeyEncrypterDecrypter \\\n--location=KMS_KEY_LOCATION \\\n--keyring=KMS_KEY_RING \\\nKMS_KEY\n```\n4. When installing this extension, enter the resource name of your key. It will look something like the following:\n```\nprojects/<YOUR PROJECT ID>/locations/<YOUR REGION>/keyRings/<YOUR KEY RING NAME>/cryptoKeys/<YOUR KEY NAME>\n```\nIf you follow these steps, your changelog table should be created using your customer-managed encryption.\n\n#### Generate schema views\n\nAfter your data is in BigQuery, you can run the [schema-views script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md) (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter to your target BigQuery project ID.\n\n2. After installation, you'll need to grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n#### Mitigating Data Loss During Extension Updates\n\nWhen updating or reconfiguring this extension, there may be a brief period where data streaming from Firestore to BigQuery is interrupted. While this limitation exists within the Extensions platform, we provide two strategies to mitigate potential data loss.\n\n##### Strategy 1: Post-Update Import\nAfter reconfiguring the extension, run the import script on your collection to ensure all data is captured. Refer to the \"Import Existing Documents\" section above for detailed steps.\n\n##### Strategy 2: Parallel Instance Method\n1. Install a second instance of the extension that streams to a new BigQuery table\n2. Reconfigure the original extension\n3. Once the original extension is properly configured and streaming events\n4. Uninstall the second instance\n5. Run a BigQuery merge job to combine the data from both tables\n\n##### Considerations\n- Strategy 1 is simpler but may result in duplicate records that need to be deduplicated\n- Strategy 2 requires more setup but provides better data continuity\n- Choose the strategy that best aligns with your data consistency requirements and operational constraints\n\n#### Billing\nTo install an extension, your project must be on the [Blaze (pay as you go) plan](https://firebase.google.com/pricing)\n\n- This extension uses other Firebase and Google Cloud Platform services, which have associated charges if you exceed the service’s no-cost tier:\n  - BigQuery (this extension writes to BigQuery with [streaming inserts](https://cloud.google.com/bigquery/pricing#streaming_pricing))\n  - Cloud Firestore\n  - Cloud Functions (Node.js 10+ runtime. [See FAQs](https://firebase.google.com/support/faq#extensions-pricing))\n\n\n\n**Configuration Parameters:**\n\n* BigQuery Dataset location: Where do you want to deploy the BigQuery dataset created for this extension? For help selecting a location, refer to the [location selection guide](https://cloud.google.com/bigquery/docs/locations).\n\n* BigQuery Project ID: Override the default project for BigQuery instance. This can allow updates to be directed to to a BigQuery instance on another GCP project.\n\n* Firestore Instance ID: The Firestore database to use. Use \"(default)\" for the default database. You can view your available Firestore databases at https://console.cloud.google.com/firestore/databases.\n\n\n* Firestore Instance Location: Where is the Firestore database located? You can check your current database location at https://console.cloud.google.com/firestore/databases.\n\n\n* Collection path: What is the path of the collection that you would like to export? You may use `{wildcard}` notation to match a subcollection of all documents in a collection (for example: `chatrooms/{chatid}/posts`). Parent Firestore Document IDs from `{wildcards}` can be returned in `path_params` as a JSON formatted string.\n\n* Enable Wildcard Column field with Parent Firestore Document IDs: If enabled, creates a column containing a JSON object of all wildcard ids from a documents path.\n\n* Dataset ID: What ID would you like to use for your BigQuery dataset? This extension will create the dataset, if it doesn't already exist.\n\n* Table ID: What identifying prefix would you like to use for your table and view inside your BigQuery dataset? This extension will create the table and view, if they don't already exist.\n\n* BigQuery SQL table Time Partitioning option type: This parameter will allow you to partition the BigQuery table and BigQuery view created by the extension based on data ingestion time. You may select the granularity of partitioning based upon one of: HOUR, DAY, MONTH, YEAR. This will generate one partition per day, hour, month or year, respectively.\n\n* BigQuery Time Partitioning column name: BigQuery table column/schema field name for TimePartitioning. You can choose schema available as `timestamp` OR a new custom defined column that will be assigned to the selected Firestore Document field below. Defaults to pseudo column _PARTITIONTIME if unspecified. Cannot be changed if Table is already partitioned.\n\n* Firestore Document field name for BigQuery SQL Time Partitioning field option: This parameter will allow you to partition the BigQuery table created by the extension based on selected. The Firestore Document field value must be a top-level TIMESTAMP, DATETIME, DATE field BigQuery string format or Firestore timestamp(will be converted to BigQuery TIMESTAMP). Cannot be changed if Table is already partitioned.\n example: `postDate`(Ensure that the Firestore-BigQuery export extension\ncreates the dataset and table before initiating any backfill scripts.\n This step is crucial for the partitioning to function correctly. It is\nessential for the script to insert data into an already partitioned table.)\n\n* BigQuery SQL Time Partitioning table schema field(column) type: Parameter for BigQuery SQL schema field type for the selected Time Partitioning Firestore Document field option. Cannot be changed if Table is already partitioned.\n\n* BigQuery SQL table clustering: This parameter allows you to set up clustering for the BigQuery table created by the extension. Specify up to 4 comma-separated fields (for example:  `data,document_id,timestamp` - no whitespaces). The order of the specified  columns determines the sort order of the data. \nNote: Cluster columns must be top-level, non-repeated columns of one of the  following types: BIGNUMERIC, BOOL, DATE, DATETIME, GEOGRAPHY, INT64, NUMERIC,  RANGE, STRING, TIMESTAMP. Clustering will not be added if a field with an invalid type is present in this parameter.\nAvailable schema extensions table fields for clustering include: `document_id, document_name, timestamp, event_id,  operation, data`.\n\n* Maximum number of synced documents per second: This parameter will set the maximum number of syncronised documents per second with BQ. Please note, any other external updates to a Big Query table will be included within this quota. Ensure that you have a set a low enough number to compensate. Defaults to 100.\n\n* View Type: Select the type of view to create in BigQuery. A regular view is a virtual table defined by a SQL query.  A materialized view persists the results of a query for faster access, with either incremental or  non-incremental updates. Please note that materialized views in this extension come with several  important caveats and limitations - carefully review the pre-install documentation before selecting  these options to ensure they are appropriate for your use case.\n\n* Maximum Staleness Duration: For materialized views only: Specifies the maximum staleness acceptable for the materialized view.  Should be specified as an INTERVAL value following BigQuery SQL syntax.  This parameter will only take effect if View Type is set to a materialized view option.\n\n* Refresh Interval (Minutes): For materialized views only: Specifies how often the materialized view should be refreshed, in minutes.  This parameter will only take effect if View Type is set to a materialized view option.\n\n* Backup Collection Name: This (optional) parameter will allow you to specify a collection for which failed BigQuery updates will be written to.\n\n* Transform function URL: Specify a function URL to call that will transform the payload that will be written to BigQuery. See the pre-install documentation for more details.\n\n* Use new query syntax for snapshots: If enabled, snapshots will be generated with the new query syntax, which should be more performant, and avoid potential resource limitations.\n\n* Exclude old data payloads: If enabled, table rows will never contain old data (document snapshot before the Firestore onDocumentUpdate event: `change.before.data()`). The reduction in data should be more performant, and avoid potential resource limitations.\n\n* Cloud KMS key name: Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS. If this parameter is set, the extension will specify the KMS key name when creating the BQ table. See the PREINSTALL.md for more details.\n\n* Maximum number of enqueue attempts: This parameter will set the maximum number of attempts to enqueue a document to cloud tasks for export to BigQuery.\n\n* Log level: The log level for the extension. The log level controls the verbosity of the extension's logs. The available log levels are: debug, info, warn, and error. To reduce the volume of logs, use a log level of warn or error.\n\n\n\n**Cloud Functions:**\n\n* **syncBigQuery:** A task-triggered function that gets called on BigQuery sync\n\n* **initBigQuerySync:** Runs configuration for sycning with BigQuery\n\n* **setupBigQuerySync:** Runs configuration for sycning with BigQuery\n\n\n\n**Other Resources**:\n\n* fsexportbigquery (firebaseextensions.v1beta.v2function)\n\n\n\n**APIs Used**:\n\n* bigquery.googleapis.com (Reason: Mirrors data from your Cloud Firestore collection in BigQuery.)\n\n\n\n**Access Required**:\n\n\n\nThis extension will operate with the following project IAM roles:\n\n* bigquery.dataEditor (Reason: Allows the extension to configure and export data into BigQuery.)\n\n* datastore.user (Reason: Allows the extension to write updates to the database.)\n\n* bigquery.user (Reason: Allows the extension to create and manage BigQuery materialized views.)\n","lifecycleEvents":[{"stage":"ON_INSTALL","processingMessage":"Configuring BigQuery Sync.","taskQueueTriggerFunction":"initBigQuerySync"},{"stage":"ON_UPDATE","processingMessage":"Configuring BigQuery Sync","taskQueueTriggerFunction":"setupBigQuerySync"},{"stage":"ON_CONFIGURE","processingMessage":"Configuring BigQuery Sync","taskQueueTriggerFunction":"setupBigQuerySync"}],"displayName":"Stream Firestore to BigQuery","events":[{"type":"firebase.extensions.firestore-counter.v1.onStart","description":"Occurs when a trigger has been called within the Extension, and will include data such as the context of the trigger request."},{"type":"firebase.extensions.firestore-counter.v1.onSuccess","description":"Occurs when a task completes successfully. The event will contain further details about specific results."},{"type":"firebase.extensions.firestore-counter.v1.onError","description":"Occurs when an issue has been experienced in the Extension. This will include any error data that has been included within the Error Exception."},{"type":"firebase.extensions.firestore-counter.v1.onCompletion","description":"Occurs when the function is settled. Provides no customized data other than the context."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onStart","description":"Occurs when a trigger has been called within the Extension, and will include data such as the context of the trigger request."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onSuccess","description":"Occurs when a task completes successfully. The event will contain further details about specific results."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onError","description":"Occurs when an issue has been experienced in the Extension. This will include any error data that has been included within the Error Exception."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onCompletion","description":"Occurs when the function is settled. Provides no customized data other than the context."},{"type":"firebase.extensions.big-query-export.v1.sync.start","description":"Occurs on a firestore document write event."}]},"fetchTime":"2025-04-22T16:19:47.253450Z","lastOperationName":"projects/firebaseextensions/operations/373aa99b-62f5-4774-9ed8-8f5c1eb00e33","state":"ACTIVE"},"params":{"DATABASE":"(default)","DATASET_ID":"firestore_export","TABLE_ID":"bigqueryexampleproject_users","DATABASE_REGION":"nam5","WILDCARD_IDS":"false","VIEW_TYPE":"view","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","DATASET_LOCATION":"us","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","MAX_ENQUEUE_ATTEMPTS":"3","BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","TABLE_PARTITIONING":"NONE","EXCLUDE_OLD_DATA":"no"},"populatedPostinstallContent":"### See it in action\n\nYou can test out this extension right away!\n\n1.  Go to your [Cloud Firestore dashboard](https://console.firebase.google.com/project/bigqueryexampleproject-e4ef9/firestore/data) in the Firebase console.\n\n2.  If it doesn't already exist, create the collection you specified during installation: `users`\n\n3.  Create a document in the collection called `bigquery-mirror-test` that contains any fields with any values that you'd like.\n\n4.  Go to the [BigQuery web UI](https://console.cloud.google.com/bigquery?project=bigqueryexampleproject-e4ef9&p=bigqueryexampleproject-e4ef9&d=firestore_export) in the Google Cloud Platform console.\n\n5.  Query your **raw changelog table**, which should contain a single log of creating the `bigquery-mirror-test` document.\n\n    ```\n    SELECT *\n    FROM `bigqueryexampleproject-e4ef9.firestore_export.bigqueryexampleproject_users_raw_changelog`\n    ```\n\n6.  Query your **latest view**, which should return the latest change event for the only document present -- `bigquery-mirror-test`.\n\n    ```\n    SELECT *\n    FROM `bigqueryexampleproject-e4ef9.firestore_export.bigqueryexampleproject_users_raw_latest`\n    ```\n\n7.  Delete the `bigquery-mirror-test` document from [Cloud Firestore](https://console.firebase.google.com/project/bigqueryexampleproject-e4ef9/firestore/data).\n    The `bigquery-mirror-test` document will disappear from the **latest view** and a `DELETE` event will be added to the **raw changelog table**.\n\n8.  You can check the changelogs of a single document with this query:\n\n    ```\n    SELECT *\n    FROM `bigqueryexampleproject-e4ef9.firestore_export.bigqueryexampleproject_users_raw_changelog`\n    WHERE document_name = \"bigquery-mirror-test\"\n    ORDER BY TIMESTAMP ASC\n    ```\n\n### Using the extension\n\nWhenever a document is created, updated, imported, or deleted in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset which contains the following resources:\n\n- **raw changelog table:** [`bigqueryexampleproject_users_raw_changelog`](https://console.cloud.google.com/bigquery?project=bigqueryexampleproject-e4ef9&p=bigqueryexampleproject-e4ef9&d=firestore_export&t=bigqueryexampleproject_users_raw_changelog&page=table)\n- **latest view:** [`bigqueryexampleproject_users_raw_latest`](https://console.cloud.google.com/bigquery?project=bigqueryexampleproject-e4ef9&p=bigqueryexampleproject-e4ef9&d=firestore_export&t=bigqueryexampleproject_users_raw_latest&page=table)\n\nTo review the schema for these two resources, click the **Schema** tab for each resource in BigQuery.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`).\n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in users. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter as your target BigQuery project ID.\n\n2. Identify the service account on the source project associated with the extension. By default, it will be constructed as `ext-<extension-instance-id>@project-id.iam.gserviceaccount.com`. However, if the extension instance ID is too long, it may be truncated and 4 random characters appended to abide by service account length limits.\n\n3. To find the exact service account, navigate to IAM & Admin -> IAM in the Google Cloud Platform Console. Look for the service account listed with \"Name\" as \"Firebase Extensions <your extension instance ID> service account\". The value in the \"Principal\" column will be the service account that needs permissions granted in the target project.\n\n4. Grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID] [-s SERVICE_ACCOUNT]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID] [-ServiceAccount SERVICE_ACCOUNT]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-s`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-ServiceAccount`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n### _(Optional)_ Import existing documents\n\nYou can backfill your BigQuery dataset with all the documents in your collection using the import script.\n\nIf you don't run the import script, the extension only exports the content of documents that are created or changed after installation.\n\nThe import script can read all existing documents in a Cloud Firestore collection and insert them into the raw changelog table created by this extension. The script adds a special changelog for each document with the operation of `IMPORT` and the timestamp of epoch. This is to ensure that any operation on an imported document supersedes the `IMPORT`.\n\n**Important:** Run the import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nLearn more about using the import script to [backfill your existing collection](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n### _(Optional)_ Generate schema views\n\nAfter your data is in BigQuery, you can use the schema-views script (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\nLearn more about using the schema-views script to [generate schema views](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md).\n\n### Monitoring\n\nAs a best practice, you can [monitor the activity](https://firebase.google.com/docs/extensions/manage-installed-extensions#monitor) of your installed extension, including checks on its health, usage, and logs.\n","extensionRef":"firebase/firestore-bigquery-export","extensionVersion":"0.2.2","systemParams":{"firebaseextensions.v1beta.function/memory":"256","firebaseextensions.v1beta.v2function/memory":"256Mi","firebaseextensions.v1beta.function/timeoutSeconds":"120","firebaseextensions.v1beta.function/vpcConnectorEgressSettings":"VPC_CONNECTOR_EGRESS_SETTINGS_UNSPECIFIED","firebaseextensions.v1beta.function/minInstances":"0","firebaseextensions.v1beta.function/location":"us-central1"}},"lastOperationName":"projects/bigqueryexampleproject-e4ef9/operations/a61b124c-6b2a-4a85-ad68-8beffc0b723f","serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","lastOperationType":"CREATE","etag":"6f5055cc72d80ee6d7f93dc8315ae50a19612411f29d1ef679c70f1fde2da1ed","runtimeData":{"stateUpdateTime":"2025-04-24T19:01:43.791080021Z","processingState":{"state":"PROCESSING_COMPLETE","detailMessage":"Sync setup completed"}}}]}
[info] i  functions: preparing functions directory for uploading... 
[info] i  functions: packaged /home/wellington/Documentos/Git/BigQueryExampleProject/functions (64.97 KB) for uploading 
[debug] [2025-04-30T18:45:10.440Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:10.440Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:10.440Z] >>> [apiv2][query] GET https://cloudfunctions.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/-/functions [none]
[debug] [2025-04-30T18:45:11.701Z] <<< [apiv2][status] GET https://cloudfunctions.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/-/functions 200
[debug] [2025-04-30T18:45:11.701Z] <<< [apiv2][body] GET https://cloudfunctions.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/-/functions {"functions":[{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-initBigQuerySync","description":"Runs configuration for sycning with BigQuery","sourceArchiveUrl":"gs://firebase-mod-sources-prod/a8a3e54cd84beb56c7eb0f2379f70adbd1427513f860a1f544893c92df16a047","httpsTrigger":{"url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-initBigQuerySync","securityLevel":"SECURE_OPTIONAL"},"status":"ACTIVE","entryPoint":"initBigQuerySync","timeout":"540s","availableMemoryMb":256,"serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","updateTime":"2025-04-24T18:58:11.627Z","versionId":"2","labels":{"firebase-extensions-ar":"enabled","goog-dm":"firebase-ext-firestore-bigquery-export","goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions","goog-firebase-ext-iid":"firestore-bigquery-export"},"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false"},"runtime":"nodejs20","ingressSettings":"ALLOW_ALL","buildId":"539681f6-8d64-426e-b876-e7462e9cc571","buildEnvironmentVariables":{"GOOGLE_NODE_RUN_SCRIPTS":""},"buildName":"projects/893582587169/locations/us-central1/builds/539681f6-8d64-426e-b876-e7462e9cc571","dockerRegistry":"ARTIFACT_REGISTRY","automaticUpdatePolicy":{},"buildServiceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"},{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-syncBigQuery","description":"A task-triggered function that gets called on BigQuery sync","sourceArchiveUrl":"gs://firebase-mod-sources-prod/a8a3e54cd84beb56c7eb0f2379f70adbd1427513f860a1f544893c92df16a047","httpsTrigger":{"url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-syncBigQuery","securityLevel":"SECURE_OPTIONAL"},"status":"ACTIVE","entryPoint":"syncBigQuery","timeout":"540s","availableMemoryMb":256,"serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","updateTime":"2025-04-24T18:58:16.790Z","versionId":"2","labels":{"firebase-extensions-ar":"enabled","goog-dm":"firebase-ext-firestore-bigquery-export","goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions","goog-firebase-ext-iid":"firestore-bigquery-export"},"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false"},"runtime":"nodejs20","ingressSettings":"ALLOW_ALL","buildId":"aa720a7e-0c24-4c7a-928f-7299737e09a5","buildEnvironmentVariables":{"GOOGLE_NODE_RUN_SCRIPTS":""},"buildName":"projects/893582587169/locations/us-central1/builds/aa720a7e-0c24-4c7a-928f-7299737e09a5","dockerRegistry":"ARTIFACT_REGISTRY","automaticUpdatePolicy":{},"buildServiceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"},{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-setupBigQuerySync","description":"Runs configuration for sycning with BigQuery","sourceArchiveUrl":"gs://firebase-mod-sources-prod/a8a3e54cd84beb56c7eb0f2379f70adbd1427513f860a1f544893c92df16a047","httpsTrigger":{"url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-setupBigQuerySync","securityLevel":"SECURE_OPTIONAL"},"status":"ACTIVE","entryPoint":"setupBigQuerySync","timeout":"540s","availableMemoryMb":256,"serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","updateTime":"2025-04-24T18:57:30.170Z","versionId":"2","labels":{"goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions","goog-firebase-ext-iid":"firestore-bigquery-export","firebase-extensions-ar":"enabled","goog-dm":"firebase-ext-firestore-bigquery-export"},"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false"},"runtime":"nodejs20","ingressSettings":"ALLOW_ALL","buildId":"087f1c66-719a-427e-9758-ba59ba92c9fc","buildEnvironmentVariables":{"GOOGLE_NODE_RUN_SCRIPTS":""},"buildName":"projects/893582587169/locations/us-central1/builds/087f1c66-719a-427e-9758-ba59ba92c9fc","dockerRegistry":"ARTIFACT_REGISTRY","automaticUpdatePolicy":{},"buildServiceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"}]}
[debug] [2025-04-30T18:45:11.703Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:11.704Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:11.704Z] >>> [apiv2][query] GET https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/-/functions filter=environment%3D%22GEN_2%22
[debug] [2025-04-30T18:45:13.067Z] <<< [apiv2][status] GET https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/-/functions 200
[debug] [2025-04-30T18:45:13.068Z] <<< [apiv2][body] GET https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/-/functions {"functions":[{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-fsexportbigquery","description":"Listens for document changes in your specified Cloud Firestore collection, then exports the changes into BigQuery.","buildConfig":{"build":"projects/893582587169/locations/us-central1/builds/59c9e8df-cfe2-4ef4-b917-55b66938b533","runtime":"nodejs22","entryPoint":"fsexportbigquery","source":{"storageSource":{"bucket":"gcf-v2-sources-893582587169-us-central1","object":"ext-firestore-bigquery-export-fsexportbigquery/function-source.zip","generation":"1745520941600164"}},"dockerRepository":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts","sourceProvenance":{"resolvedStorageSource":{"bucket":"gcf-v2-sources-893582587169-us-central1","object":"ext-firestore-bigquery-export-fsexportbigquery/function-source.zip","generation":"1745520941600164"}},"dockerRegistry":"ARTIFACT_REGISTRY","serviceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"},"serviceConfig":{"service":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/services/ext-firestore-bigquery-export-fsexportbigquery","timeoutSeconds":120,"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","FUNCTION_SIGNATURE_TYPE":"cloudevent","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false","LOG_EXECUTION_ID":"true"},"maxInstanceCount":6,"ingressSettings":"ALLOW_INTERNAL_ONLY","uri":"https://ext-firestore-bigquery-export-fsexportbigquery-hijxxwf3ia-uc.a.run.app","serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","availableMemory":"256Mi","allTrafficOnLatestRevision":true,"revision":"ext-firestore-bigquery-export-fsexportbigquery-00002-cuw","maxInstanceRequestConcurrency":1,"availableCpu":"0.1666"},"eventTrigger":{"trigger":"projects/bigqueryexampleproject-e4ef9/locations/nam5/triggers/ext-firestore-bigquery-export-fsexportbigquery-548607","triggerRegion":"nam5","eventType":"google.cloud.firestore.document.v1.written","eventFilters":[{"attribute":"database","value":"(default)"},{"attribute":"document","value":"users/{documentId}","operator":"match-path-pattern"}],"pubsubTopic":"projects/bigqueryexampleproject-e4ef9/topics/eventarc-nam5-ext-firestore-bigquery-export-fsexportbigquery-548607-885","serviceAccountEmail":"893582587169-compute@developer.gserviceaccount.com","retryPolicy":"RETRY_POLICY_DO_NOT_RETRY"},"state":"ACTIVE","updateTime":"2025-04-24T18:57:21.075796035Z","labels":{"goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions","goog-firebase-ext-iid":"firestore-bigquery-export","goog-dm":"firebase-ext-firestore-bigquery-export"},"environment":"GEN_2","url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-fsexportbigquery","createTime":"2025-04-24T18:53:41.744810406Z","satisfiesPzi":true}]}
[info] i  functions: ensuring required API run.googleapis.com is enabled... 
[debug] [2025-04-30T18:45:13.075Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:13.075Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API eventarc.googleapis.com is enabled... 
[debug] [2025-04-30T18:45:13.075Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:13.075Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API pubsub.googleapis.com is enabled... 
[debug] [2025-04-30T18:45:13.076Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:13.076Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API storage.googleapis.com is enabled... 
[debug] [2025-04-30T18:45:13.076Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:13.077Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:13.077Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com [none]
[debug] [2025-04-30T18:45:13.077Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:45:13.081Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com [none]
[debug] [2025-04-30T18:45:13.081Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:45:13.085Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com [none]
[debug] [2025-04-30T18:45:13.085Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:45:13.089Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com [none]
[debug] [2025-04-30T18:45:13.089Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:45:14.252Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com 200
[debug] [2025-04-30T18:45:14.253Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com [omitted]
[info] ✔  functions: required API pubsub.googleapis.com is enabled 
[debug] [2025-04-30T18:45:14.254Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com 200
[debug] [2025-04-30T18:45:14.254Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com [omitted]
[info] ✔  functions: required API eventarc.googleapis.com is enabled 
[debug] [2025-04-30T18:45:14.256Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com 200
[debug] [2025-04-30T18:45:14.257Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com [omitted]
[info] ✔  functions: required API run.googleapis.com is enabled 
[debug] [2025-04-30T18:45:14.558Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com 200
[debug] [2025-04-30T18:45:14.558Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com [omitted]
[info] ✔  functions: required API storage.googleapis.com is enabled 
[info] i  functions: generating the service identity for pubsub.googleapis.com... 
[debug] [2025-04-30T18:45:14.559Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:14.559Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: generating the service identity for eventarc.googleapis.com... 
[debug] [2025-04-30T18:45:14.560Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:14.560Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:14.560Z] >>> [apiv2][query] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/pubsub.googleapis.com:generateServiceIdentity [none]
[debug] [2025-04-30T18:45:14.561Z] >>> [apiv2][query] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/eventarc.googleapis.com:generateServiceIdentity [none]
[debug] [2025-04-30T18:45:15.068Z] <<< [apiv2][status] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/eventarc.googleapis.com:generateServiceIdentity 200
[debug] [2025-04-30T18:45:15.069Z] <<< [apiv2][body] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/eventarc.googleapis.com:generateServiceIdentity {"name":"operations/finished.DONE_OPERATION","done":true,"response":{"@type":"type.googleapis.com/google.api.serviceusage.v1beta1.ServiceIdentity","email":"service-893582587169@gcp-sa-eventarc.iam.gserviceaccount.com","uniqueId":"102092505972539947691"}}
[debug] [2025-04-30T18:45:15.660Z] <<< [apiv2][status] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/pubsub.googleapis.com:generateServiceIdentity 200
[debug] [2025-04-30T18:45:15.660Z] <<< [apiv2][body] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/pubsub.googleapis.com:generateServiceIdentity {"name":"operations/finished.DONE_OPERATION","done":true,"response":{"@type":"type.googleapis.com/google.api.serviceusage.v1beta1.ServiceIdentity","email":"service-893582587169@gcp-sa-pubsub.iam.gserviceaccount.com","uniqueId":"114949547525185127171"}}
[debug] [2025-04-30T18:45:15.664Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:15.664Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:15.664Z] >>> [apiv2][query] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 [none]
[debug] [2025-04-30T18:45:16.793Z] <<< [apiv2][status] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 200
[debug] [2025-04-30T18:45:16.793Z] <<< [apiv2][body] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 {"projectNumber":"893582587169","projectId":"bigqueryexampleproject-e4ef9","lifecycleState":"ACTIVE","name":"BigQueryExampleProject","labels":{"firebase":"enabled","firebase-core":"disabled"},"createTime":"2025-04-24T16:34:06.379745Z"}
[debug] [2025-04-30T18:45:16.796Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:16.796Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:16.796Z] >>> [apiv2][query] GET https://cloudbilling.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/billingInfo [none]
[debug] [2025-04-30T18:45:17.618Z] <<< [apiv2][status] GET https://cloudbilling.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/billingInfo 200
[debug] [2025-04-30T18:45:17.619Z] <<< [apiv2][body] GET https://cloudbilling.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/billingInfo {"name":"projects/bigqueryexampleproject-e4ef9/billingInfo","projectId":"bigqueryexampleproject-e4ef9","billingAccountName":"billingAccounts/019DD6-64C0E7-5ED023","billingEnabled":true}
[debug] [2025-04-30T18:45:17.621Z] [functions] found 6 new HTTP functions, testing setIamPolicy permission...
[debug] [2025-04-30T18:45:17.622Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:17.622Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:17.622Z] >>> [apiv2][query] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions [none]
[debug] [2025-04-30T18:45:17.622Z] >>> [apiv2][(partial)header] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T18:45:17.622Z] >>> [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.setIamPolicy"]}
[debug] [2025-04-30T18:45:17.913Z] <<< [apiv2][status] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions 200
[debug] [2025-04-30T18:45:17.913Z] <<< [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.setIamPolicy"]}
[debug] [2025-04-30T18:45:17.914Z] [functions] found setIamPolicy permission, proceeding with deploy
[debug] [2025-04-30T18:45:17.915Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:17.915Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:17.915Z] >>> [apiv2][query] POST https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions:generateUploadUrl [none]
[debug] [2025-04-30T18:45:18.928Z] <<< [apiv2][status] POST https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions:generateUploadUrl 200
[debug] [2025-04-30T18:45:18.928Z] <<< [apiv2][body] POST https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions:generateUploadUrl {"uploadUrl":"https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/4a74a5e8-e4f0-440b-af00-6cb61a3b5206.zip?GoogleAccessId=service-893582587169@gcf-admin-robot.iam.gserviceaccount.com&Expires=1746040518&Signature=DI5ydra6CgzfLYhkefYl%2BTx47A9IOWSUl35ygrXNySRiLwE4ELSrlCSePcT18zMB8mzKysVOyT7LAL%2FJJoE5Pf3GAy%2Fkmi%2F8t%2FJskFlk2A4Qr%2BbU58NBz5KXD570KqXuwJAuuTUqmbaxTzuWbTFF%2Fj3pLYh9q1StrLXrBDdCzXWSev4gz5C0pF%2BxXM8EzVxRZ%2FIjbNgc7jBLjUJaTobtxhhxjF%2FVTDOiG19KwbTOvu3ID6qtcjC4r9NN1wjDNzcuGloe8FIwSPqZ30i8R0P9V2NWnnsbXjODjgntEDk%2Fep5ux2WgF60Mz7Q2IKaywGImelMcC5oyhNVoCV2x5rU85g%3D%3D","storageSource":{"bucket":"gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com","object":"4a74a5e8-e4f0-440b-af00-6cb61a3b5206.zip"}}
[debug] [2025-04-30T18:45:18.931Z] >>> [apiv2][query] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/4a74a5e8-e4f0-440b-af00-6cb61a3b5206.zip GoogleAccessId=service-893582587169%40gcf-admin-robot.iam.gserviceaccount.com&Expires=1746040518&Signature=DI5ydra6CgzfLYhkefYl%2BTx47A9IOWSUl35ygrXNySRiLwE4ELSrlCSePcT18zMB8mzKysVOyT7LAL%2FJJoE5Pf3GAy%2Fkmi%2F8t%2FJskFlk2A4Qr%2BbU58NBz5KXD570KqXuwJAuuTUqmbaxTzuWbTFF%2Fj3pLYh9q1StrLXrBDdCzXWSev4gz5C0pF%2BxXM8EzVxRZ%2FIjbNgc7jBLjUJaTobtxhhxjF%2FVTDOiG19KwbTOvu3ID6qtcjC4r9NN1wjDNzcuGloe8FIwSPqZ30i8R0P9V2NWnnsbXjODjgntEDk%2Fep5ux2WgF60Mz7Q2IKaywGImelMcC5oyhNVoCV2x5rU85g%3D%3D
[debug] [2025-04-30T18:45:18.931Z] >>> [apiv2][body] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/4a74a5e8-e4f0-440b-af00-6cb61a3b5206.zip [stream]
[debug] [2025-04-30T18:45:20.506Z] <<< [apiv2][status] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/4a74a5e8-e4f0-440b-af00-6cb61a3b5206.zip 200
[debug] [2025-04-30T18:45:20.506Z] <<< [apiv2][body] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/4a74a5e8-e4f0-440b-af00-6cb61a3b5206.zip [omitted]
[info] ✔  functions: functions folder uploaded successfully 
[info] i  functions: creating Node.js 22 (2nd Gen) function createuser(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function deleteuser(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function listusers(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function processdata(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function showuser(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function updateuser(us-central1)... 
[debug] [2025-04-30T18:45:20.533Z] Total Function Deployment time: 7
[debug] [2025-04-30T18:45:20.533Z] 6 Functions Deployed
[debug] [2025-04-30T18:45:20.533Z] 6 Functions Errored
[debug] [2025-04-30T18:45:20.533Z] 0 Function Deployments Aborted
[debug] [2025-04-30T18:45:20.533Z] Average Function Deployment time: 2.1666666666666665
[info] 
[info] Functions deploy had errors with the following functions:
	createuser(us-central1)
	deleteuser(us-central1)
	listusers(us-central1)
	processdata(us-central1)
	showuser(us-central1)
	updateuser(us-central1)
[debug] [2025-04-30T18:45:20.979Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T18:45:20.979Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T18:45:20.979Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T18:45:20.979Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T18:45:20.979Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T18:45:20.979Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T18:45:20.981Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:20.981Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T18:45:20.981Z] >>> [apiv2][query] GET https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts [none]
[debug] [2025-04-30T18:45:22.507Z] <<< [apiv2][status] GET https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts 200
[debug] [2025-04-30T18:45:22.507Z] <<< [apiv2][body] GET https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts {"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts","format":"DOCKER","description":"This repository is created and used by Cloud Functions for storing function docker images.","labels":{"goog-managed-by":"cloudfunctions"},"createTime":"2025-04-24T18:45:25.809021Z","updateTime":"2025-04-30T18:44:12.443453Z","mode":"STANDARD_REPOSITORY","cleanupPolicies":{"firebase-functions-cleanup":{"id":"firebase-functions-cleanup","action":"DELETE","condition":{"tagState":"ANY","olderThan":"86400s"}}},"sizeBytes":"681973421","vulnerabilityScanningConfig":{"lastEnableTime":"2025-04-24T18:45:16.816360009Z","enablementState":"SCANNING_DISABLED","enablementStateReason":"API containerscanning.googleapis.com is not enabled."},"satisfiesPzi":true,"registryUri":"us-central1-docker.pkg.dev/bigqueryexampleproject-e4ef9/gcf-artifacts"}
[debug] [2025-04-30T18:45:22.508Z] Functions deploy failed.
[debug] [2025-04-30T18:45:22.508Z] {}
[debug] [2025-04-30T18:45:22.508Z] {}
[debug] [2025-04-30T18:45:22.508Z] {}
[debug] [2025-04-30T18:45:22.508Z] {}
[debug] [2025-04-30T18:45:22.508Z] {}
[debug] [2025-04-30T18:45:22.508Z] {}
[error] Error: There was an error deploying functions:
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[debug] [2025-04-30T19:02:15.223Z] ----------------------------------------------------------------------
[debug] [2025-04-30T19:02:15.225Z] Command:       /usr/local/bin/firebase /home/wellington/.cache/firebase/tools/lib/node_modules/firebase-tools/lib/bin/firebase emulators:start
[debug] [2025-04-30T19:02:15.226Z] CLI Version:   14.0.0
[debug] [2025-04-30T19:02:15.226Z] Platform:      linux
[debug] [2025-04-30T19:02:15.226Z] Node Version:  v20.18.2
[debug] [2025-04-30T19:02:15.226Z] Time:          Wed Apr 30 2025 16:02:15 GMT-0300 (GMT-03:00)
[debug] [2025-04-30T19:02:15.226Z] ----------------------------------------------------------------------
[debug] 
[debug] [2025-04-30T19:02:15.512Z] > command requires scopes: ["email","openid","https://www.googleapis.com/auth/cloudplatformprojects.readonly","https://www.googleapis.com/auth/firebase","https://www.googleapis.com/auth/cloud-platform"]
[debug] [2025-04-30T19:02:15.513Z] > authorizing via signed-in user (wellingtonsilva112000@gmail.com)
[info] i  emulators: Starting emulators: functions, extensions {"metadata":{"emulator":{"name":"hub"},"message":"Starting emulators: functions, extensions"}}
[debug] [2025-04-30T19:02:15.517Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:02:15.517Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:02:15.518Z] >>> [apiv2][query] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 [none]
[debug] [2025-04-30T19:02:17.840Z] <<< [apiv2][status] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 200
[debug] [2025-04-30T19:02:17.841Z] <<< [apiv2][body] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 {"projectNumber":"893582587169","projectId":"bigqueryexampleproject-e4ef9","lifecycleState":"ACTIVE","name":"BigQueryExampleProject","labels":{"firebase":"enabled","firebase-core":"disabled"},"createTime":"2025-04-24T16:34:06.379745Z"}
[debug] [2025-04-30T19:02:17.851Z] [logging] Logging Emulator only supports listening on one address (127.0.0.1). Not listening on ::1
[debug] [2025-04-30T19:02:17.851Z] assigned listening specs for emulators {"user":{"hub":[{"address":"127.0.0.1","family":"IPv4","port":4400},{"address":"::1","family":"IPv6","port":4400}],"ui":[{"address":"127.0.0.1","family":"IPv4","port":4000},{"address":"::1","family":"IPv6","port":4000}],"logging":[{"address":"127.0.0.1","family":"IPv4","port":4500}]},"metadata":{"message":"assigned listening specs for emulators"}}
[debug] [2025-04-30T19:02:17.857Z] [hub] writing locator at /tmp/hub-bigqueryexampleproject-e4ef9.json
[debug] [2025-04-30T19:02:17.865Z] [Extensions] Started Extensions emulator, this is a noop.
[debug] [2025-04-30T19:02:17.867Z] [functions] Functions Emulator only supports listening on one address (127.0.0.1). Not listening on ::1
[debug] [2025-04-30T19:02:17.868Z] [eventarc] Eventarc Emulator only supports listening on one address (127.0.0.1). Not listening on ::1
[debug] [2025-04-30T19:02:17.868Z] [tasks] Cloud Tasks Emulator only supports listening on one address (127.0.0.1). Not listening on ::1
[debug] [2025-04-30T19:02:17.868Z] late-assigned ports for functions and eventarc emulators {"user":{"hub":[{"address":"127.0.0.1","family":"IPv4","port":4400},{"address":"::1","family":"IPv6","port":4400}],"ui":[{"address":"127.0.0.1","family":"IPv4","port":4000},{"address":"::1","family":"IPv6","port":4000}],"logging":[{"address":"127.0.0.1","family":"IPv4","port":4500}],"functions":[{"address":"127.0.0.1","family":"IPv4","port":5001}],"eventarc":[{"address":"127.0.0.1","family":"IPv4","port":9299}],"tasks":[{"address":"127.0.0.1","family":"IPv4","port":9499}]},"metadata":{"message":"late-assigned ports for functions and eventarc emulators"}}
[warn] ⚠  functions: The following emulators are not running, calls to these services from the Functions emulator will affect production: apphosting, auth, firestore, database, hosting, pubsub, storage, dataconnect {"metadata":{"emulator":{"name":"functions"},"message":"The following emulators are not running, calls to these services from the Functions emulator will affect production: \u001b[1mapphosting, auth, firestore, database, hosting, pubsub, storage, dataconnect\u001b[22m"}}
[debug] [2025-04-30T19:02:18.018Z] defaultcredentials: writing to file /home/wellington/.config/firebase/wellingtonsilva112000_gmail_com_application_default_credentials.json
[debug] [2025-04-30T19:02:18.053Z] Setting GAC to /home/wellington/.config/firebase/wellingtonsilva112000_gmail_com_application_default_credentials.json {"metadata":{"emulator":{"name":"functions"},"message":"Setting GAC to /home/wellington/.config/firebase/wellingtonsilva112000_gmail_com_application_default_credentials.json"}}
[debug] [2025-04-30T19:02:18.054Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:02:18.054Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:02:18.054Z] >>> [apiv2][query] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig [none]
[debug] [2025-04-30T19:02:18.766Z] <<< [apiv2][status] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig 200
[debug] [2025-04-30T19:02:18.766Z] <<< [apiv2][body] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig {"projectId":"bigqueryexampleproject-e4ef9","storageBucket":"bigqueryexampleproject-e4ef9.firebasestorage.app"}
[debug] [2025-04-30T19:02:18.825Z] [Extensions] Connecting Extensions emulator, this is a noop.
[info] i  functions: Watching "/home/wellington/Documentos/Git/BigQueryExample/Functions-Firebase/functions" for Cloud Functions... {"metadata":{"emulator":{"name":"functions"},"message":"Watching \"/home/wellington/Documentos/Git/BigQueryExample/Functions-Firebase/functions\" for Cloud Functions..."}}
[debug] [2025-04-30T19:02:18.840Z] Validating nodejs source
[debug] [2025-04-30T19:02:19.546Z] > [functions] package.json contents: {
  "name": "functions",
  "description": "Cloud Functions for Firebase",
  "scripts": {
    "serve": "firebase emulators:start --only functions",
    "shell": "firebase functions:shell",
    "start": "npm run shell",
    "deploy": "firebase deploy --only functions",
    "logs": "firebase functions:log"
  },
  "engines": {
    "node": "22"
  },
  "main": "index.mjs",
  "dependencies": {
    "firebase-admin": "^12.6.0",
    "firebase-functions": "^6.0.1",
    "uuid": "^11.1.0"
  },
  "devDependencies": {
    "firebase-functions-test": "^3.1.0"
  },
  "private": true
}
[debug] [2025-04-30T19:02:19.546Z] Building nodejs source
[debug] [2025-04-30T19:02:19.547Z] Failed to find version of module node: reached end of search path /home/wellington/Documentos/Git/BigQueryExample/Functions-Firebase/functions/node_modules
[warn] ⚠  functions: You've requested "node" version "22", but the standalone Firebase CLI comes with bundled Node "20". 
[info] ✔  functions: To use a different Node.js version, consider removing the standalone Firebase CLI and switching to "firebase-tools" on npm. 
[debug] [2025-04-30T19:02:19.551Z] Could not find functions.yaml. Must use http discovery
[debug] [2025-04-30T19:02:19.562Z] Found firebase-functions binary at '/home/wellington/Documentos/Git/BigQueryExample/Functions-Firebase/functions/node_modules/.bin/firebase-functions'
[info] Serving at port 8510

[debug] [2025-04-30T19:02:20.362Z] Got response from /__/functions.yaml {"endpoints":{"createuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"createuser"},"deleteuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"deleteuser"},"listusers":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"listusers"},"processdata":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"processdata"},"showuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"showuser"},"updateuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"updateuser"}},"specVersion":"v1alpha1","requiredAPIs":[],"extensions":{}}
[debug] [2025-04-30T19:02:24.423Z] defaultcredentials: writing to file /home/wellington/.config/firebase/wellingtonsilva112000_gmail_com_application_default_credentials.json
[debug] [2025-04-30T19:02:24.424Z] Setting GAC to /home/wellington/.config/firebase/wellingtonsilva112000_gmail_com_application_default_credentials.json {"metadata":{"emulator":{"name":"functions"},"message":"Setting GAC to /home/wellington/.config/firebase/wellingtonsilva112000_gmail_com_application_default_credentials.json"}}
[info] ✔  functions: Loaded functions definitions from source: createuser, deleteuser, listusers, processdata, showuser, updateuser. {"metadata":{"emulator":{"name":"functions"},"message":"Loaded functions definitions from source: createuser, deleteuser, listusers, processdata, showuser, updateuser."}}
[info] ✔  functions[us-central1-createuser]: http function initialized (http://127.0.0.1:5001/bigqueryexampleproject-e4ef9/us-central1/createuser). {"metadata":{"emulator":{"name":"functions"},"message":"\u001b[1mhttp\u001b[22m function initialized (http://127.0.0.1:5001/bigqueryexampleproject-e4ef9/us-central1/createuser)."}}
[info] ✔  functions[us-central1-deleteuser]: http function initialized (http://127.0.0.1:5001/bigqueryexampleproject-e4ef9/us-central1/deleteuser). {"metadata":{"emulator":{"name":"functions"},"message":"\u001b[1mhttp\u001b[22m function initialized (http://127.0.0.1:5001/bigqueryexampleproject-e4ef9/us-central1/deleteuser)."}}
[info] ✔  functions[us-central1-listusers]: http function initialized (http://127.0.0.1:5001/bigqueryexampleproject-e4ef9/us-central1/listusers). {"metadata":{"emulator":{"name":"functions"},"message":"\u001b[1mhttp\u001b[22m function initialized (http://127.0.0.1:5001/bigqueryexampleproject-e4ef9/us-central1/listusers)."}}
[info] ✔  functions[us-central1-processdata]: http function initialized (http://127.0.0.1:5001/bigqueryexampleproject-e4ef9/us-central1/processdata). {"metadata":{"emulator":{"name":"functions"},"message":"\u001b[1mhttp\u001b[22m function initialized (http://127.0.0.1:5001/bigqueryexampleproject-e4ef9/us-central1/processdata)."}}
[info] ✔  functions[us-central1-showuser]: http function initialized (http://127.0.0.1:5001/bigqueryexampleproject-e4ef9/us-central1/showuser). {"metadata":{"emulator":{"name":"functions"},"message":"\u001b[1mhttp\u001b[22m function initialized (http://127.0.0.1:5001/bigqueryexampleproject-e4ef9/us-central1/showuser)."}}
[info] ✔  functions[us-central1-updateuser]: http function initialized (http://127.0.0.1:5001/bigqueryexampleproject-e4ef9/us-central1/updateuser). {"metadata":{"emulator":{"name":"functions"},"message":"\u001b[1mhttp\u001b[22m function initialized (http://127.0.0.1:5001/bigqueryexampleproject-e4ef9/us-central1/updateuser)."}}
[debug] [2025-04-30T19:02:24.445Z] Could not find VSCode notification endpoint: FetchError: request to http://localhost:40001/vscode/notify failed, reason: connect ECONNREFUSED 127.0.0.1:40001. If you are not running the Firebase Data Connect VSCode extension, this is expected and not an issue.
[info] 
┌─────────────────────────────────────────────────────────────┐
│ ✔  All emulators ready! It is now safe to connect your app. │
│ i  View Emulator UI at http://127.0.0.1:4000/               │
└─────────────────────────────────────────────────────────────┘

┌────────────┬────────────────┬──────────────────────────────────┐
│ Emulator   │ Host:Port      │ View in Emulator UI              │
├────────────┼────────────────┼──────────────────────────────────┤
│ Functions  │ 127.0.0.1:5001 │ http://127.0.0.1:4000/functions  │
├────────────┼────────────────┼──────────────────────────────────┤
│ Extensions │ 127.0.0.1:5001 │ http://127.0.0.1:4000/extensions │
└────────────┴────────────────┴──────────────────────────────────┘
  Emulator Hub host: 127.0.0.1 port: 4400
  Other reserved ports: 4500
┌─────────────────────────┬───────────────┬─────────────────────┐
│ Extension Instance Name │ Extension Ref │ View in Emulator UI │
└─────────────────────────┴───────────────┴─────────────────────┘
Issues? Report them at https://github.com/firebase/firebase-tools/issues and attach the *-debug.log files.
 
[debug] [2025-04-30T19:02:50.748Z] Received signal SIGINT (Ctrl-C) 1
[info]  
[info] i  emulators: Received SIGINT (Ctrl-C) for the first time. Starting a clean shutdown. 
[info] i  emulators: Please wait for a clean shutdown or send the SIGINT (Ctrl-C) signal again to stop right now. 
[info] i  emulators: Shutting down emulators. {"metadata":{"emulator":{"name":"hub"},"message":"Shutting down emulators."}}
[info] i  ui: Stopping Emulator UI {"metadata":{"emulator":{"name":"ui"},"message":"Stopping Emulator UI"}}
[info] i  extensions: Stopping Extensions Emulator {"metadata":{"emulator":{"name":"extensions"},"message":"Stopping Extensions Emulator"}}
[debug] [2025-04-30T19:02:50.754Z] [Extensions] Stopping Extensions emulator, this is a noop.
[info] i  functions: Stopping Functions Emulator {"metadata":{"emulator":{"name":"functions"},"message":"Stopping Functions Emulator"}}
[info] i  eventarc: Stopping Eventarc Emulator {"metadata":{"emulator":{"name":"eventarc"},"message":"Stopping Eventarc Emulator"}}
[info] i  tasks: Stopping Cloud Tasks Emulator {"metadata":{"emulator":{"name":"tasks"},"message":"Stopping Cloud Tasks Emulator"}}
[info] i  hub: Stopping emulator hub {"metadata":{"emulator":{"name":"hub"},"message":"Stopping emulator hub"}}
[info] i  logging: Stopping Logging Emulator {"metadata":{"emulator":{"name":"logging"},"message":"Stopping Logging Emulator"}}
[debug] [2025-04-30T19:02:50.767Z] Could not find VSCode notification endpoint: FetchError: request to http://localhost:40001/vscode/notify failed, reason: connect ECONNREFUSED 127.0.0.1:40001. If you are not running the Firebase Data Connect VSCode extension, this is expected and not an issue.
[debug] [2025-04-30T19:02:56.255Z] Error: Timed out.
    at Timeout._onTimeout (/home/wellington/.cache/firebase/tools/lib/node_modules/firebase-tools/lib/utils.js:269:49)
    at listOnTimeout (node:internal/timers:581:17)
    at process.processTimers (node:internal/timers:519:7)
[error] 
[error] Error: An unexpected error has occurred.
[debug] [2025-04-30T19:03:07.353Z] ----------------------------------------------------------------------
[debug] [2025-04-30T19:03:07.355Z] Command:       /usr/local/bin/firebase /home/wellington/.cache/firebase/tools/lib/node_modules/firebase-tools/lib/bin/firebase deploy
[debug] [2025-04-30T19:03:07.355Z] CLI Version:   14.0.0
[debug] [2025-04-30T19:03:07.356Z] Platform:      linux
[debug] [2025-04-30T19:03:07.356Z] Node Version:  v20.18.2
[debug] [2025-04-30T19:03:07.356Z] Time:          Wed Apr 30 2025 16:03:07 GMT-0300 (GMT-03:00)
[debug] [2025-04-30T19:03:07.356Z] ----------------------------------------------------------------------
[debug] 
[debug] [2025-04-30T19:03:07.590Z] > command requires scopes: ["email","openid","https://www.googleapis.com/auth/cloudplatformprojects.readonly","https://www.googleapis.com/auth/firebase","https://www.googleapis.com/auth/cloud-platform"]
[debug] [2025-04-30T19:03:07.591Z] > authorizing via signed-in user (wellingtonsilva112000@gmail.com)
[debug] [2025-04-30T19:03:07.591Z] [iam] checking project bigqueryexampleproject-e4ef9 for permissions ["cloudfunctions.functions.create","cloudfunctions.functions.delete","cloudfunctions.functions.get","cloudfunctions.functions.list","cloudfunctions.functions.update","cloudfunctions.operations.get","firebase.projects.get"]
[debug] [2025-04-30T19:03:07.592Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:07.592Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:07.593Z] >>> [apiv2][query] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions [none]
[debug] [2025-04-30T19:03:07.593Z] >>> [apiv2][(partial)header] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:03:07.594Z] >>> [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.create","cloudfunctions.functions.delete","cloudfunctions.functions.get","cloudfunctions.functions.list","cloudfunctions.functions.update","cloudfunctions.operations.get","firebase.projects.get"]}
[debug] [2025-04-30T19:03:08.763Z] <<< [apiv2][status] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions 200
[debug] [2025-04-30T19:03:08.764Z] <<< [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.create","cloudfunctions.functions.delete","cloudfunctions.functions.get","cloudfunctions.functions.list","cloudfunctions.functions.update","cloudfunctions.operations.get","firebase.projects.get"]}
[debug] [2025-04-30T19:03:08.764Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:08.764Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:08.765Z] >>> [apiv2][query] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions [none]
[debug] [2025-04-30T19:03:08.765Z] >>> [apiv2][body] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions {"permissions":["iam.serviceAccounts.actAs"]}
[debug] [2025-04-30T19:03:10.182Z] <<< [apiv2][status] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions 404
[debug] [2025-04-30T19:03:10.182Z] <<< [apiv2][body] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions {"error":{"code":404,"message":"Unknown service account","status":"NOT_FOUND"}}
[debug] [2025-04-30T19:03:10.183Z] [functions] service account IAM check errored, deploy may fail: Request to https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions had HTTP Error: 404, Unknown service account {"name":"FirebaseError","children":[],"context":{"body":{"error":{"code":404,"message":"Unknown service account","status":"NOT_FOUND"}},"response":{"statusCode":404}},"exit":1,"message":"Request to https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions had HTTP Error: 404, Unknown service account","status":404}
[info] 
[info] === Deploying to 'bigqueryexampleproject-e4ef9'...
[info] 
[info] i  deploying functions 
[debug] [2025-04-30T19:03:10.190Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:10.190Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:10.191Z] >>> [apiv2][query] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 [none]
[debug] [2025-04-30T19:03:10.490Z] <<< [apiv2][status] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 200
[debug] [2025-04-30T19:03:10.490Z] <<< [apiv2][body] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 {"projectNumber":"893582587169","projectId":"bigqueryexampleproject-e4ef9","lifecycleState":"ACTIVE","name":"BigQueryExampleProject","labels":{"firebase":"enabled","firebase-core":"disabled"},"createTime":"2025-04-24T16:34:06.379745Z"}
[info] i  functions: preparing codebase default for deployment 
[info] i  functions: ensuring required API cloudfunctions.googleapis.com is enabled... 
[debug] [2025-04-30T19:03:10.493Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:10.494Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:10.494Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:10.494Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API cloudbuild.googleapis.com is enabled... 
[debug] [2025-04-30T19:03:10.495Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:10.495Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  artifactregistry: ensuring required API artifactregistry.googleapis.com is enabled... 
[debug] [2025-04-30T19:03:10.496Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:10.496Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:10.497Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com [none]
[debug] [2025-04-30T19:03:10.497Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:03:10.500Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com [none]
[debug] [2025-04-30T19:03:10.501Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:03:10.505Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com [none]
[debug] [2025-04-30T19:03:10.505Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:03:10.509Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com [none]
[debug] [2025-04-30T19:03:10.509Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:03:12.036Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com 200
[debug] [2025-04-30T19:03:12.036Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com [omitted]
[debug] [2025-04-30T19:03:12.040Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com 200
[debug] [2025-04-30T19:03:12.040Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com [omitted]
[info] ✔  artifactregistry: required API artifactregistry.googleapis.com is enabled 
[debug] [2025-04-30T19:03:12.043Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com 200
[debug] [2025-04-30T19:03:12.044Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com [omitted]
[info] ✔  functions: required API cloudfunctions.googleapis.com is enabled 
[debug] [2025-04-30T19:03:12.128Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com 200
[debug] [2025-04-30T19:03:12.129Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com [omitted]
[info] ✔  functions: required API cloudbuild.googleapis.com is enabled 
[debug] [2025-04-30T19:03:12.130Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:12.130Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:12.130Z] >>> [apiv2][query] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig [none]
[debug] [2025-04-30T19:03:13.049Z] <<< [apiv2][status] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig 200
[debug] [2025-04-30T19:03:13.049Z] <<< [apiv2][body] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig {"projectId":"bigqueryexampleproject-e4ef9","storageBucket":"bigqueryexampleproject-e4ef9.firebasestorage.app"}
[debug] [2025-04-30T19:03:13.050Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:13.050Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:13.050Z] >>> [apiv2][query] GET https://runtimeconfig.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/configs [none]
[debug] [2025-04-30T19:03:13.982Z] <<< [apiv2][status] GET https://runtimeconfig.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/configs 200
[debug] [2025-04-30T19:03:13.982Z] <<< [apiv2][body] GET https://runtimeconfig.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/configs {}
[debug] [2025-04-30T19:03:13.985Z] Validating nodejs source
[debug] [2025-04-30T19:03:14.619Z] > [functions] package.json contents: {
  "name": "functions",
  "description": "Cloud Functions for Firebase",
  "scripts": {
    "serve": "firebase emulators:start --only functions",
    "shell": "firebase functions:shell",
    "start": "npm run shell",
    "deploy": "firebase deploy --only functions",
    "logs": "firebase functions:log"
  },
  "engines": {
    "node": "22"
  },
  "main": "index.mjs",
  "dependencies": {
    "firebase-admin": "^12.6.0",
    "firebase-functions": "^6.0.1",
    "uuid": "^11.1.0"
  },
  "devDependencies": {
    "firebase-functions-test": "^3.1.0"
  },
  "private": true
}
[debug] [2025-04-30T19:03:14.619Z] Building nodejs source
[info] i  functions: Loading and analyzing source code for codebase default to determine what to deploy 
[debug] [2025-04-30T19:03:14.622Z] Could not find functions.yaml. Must use http discovery
[debug] [2025-04-30T19:03:14.639Z] Found firebase-functions binary at '/home/wellington/Documentos/Git/BigQueryExample/Functions-Firebase/functions/node_modules/.bin/firebase-functions'
[info] Serving at port 8185

[debug] [2025-04-30T19:03:15.408Z] Got response from /__/functions.yaml {"endpoints":{"createuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"createuser"},"deleteuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"deleteuser"},"listusers":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"listusers"},"processdata":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"processdata"},"showuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"showuser"},"updateuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"updateuser"}},"specVersion":"v1alpha1","requiredAPIs":[],"extensions":{}}
[info] i  extensions: ensuring required API firebaseextensions.googleapis.com is enabled... 
[debug] [2025-04-30T19:03:19.465Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:19.465Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:19.465Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com [none]
[debug] [2025-04-30T19:03:19.465Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:03:20.832Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com 200
[debug] [2025-04-30T19:03:20.832Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com [omitted]
[info] ✔  extensions: required API firebaseextensions.googleapis.com is enabled 
[debug] [2025-04-30T19:03:20.832Z] > command requires scopes: ["email","openid","https://www.googleapis.com/auth/cloudplatformprojects.readonly","https://www.googleapis.com/auth/firebase","https://www.googleapis.com/auth/cloud-platform"]
[debug] [2025-04-30T19:03:20.833Z] > authorizing via signed-in user (wellingtonsilva112000@gmail.com)
[debug] [2025-04-30T19:03:20.833Z] [iam] checking project bigqueryexampleproject-e4ef9 for permissions ["firebase.projects.get","firebaseextensions.instances.list"]
[debug] [2025-04-30T19:03:20.833Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:20.833Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:20.834Z] >>> [apiv2][query] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions [none]
[debug] [2025-04-30T19:03:20.834Z] >>> [apiv2][(partial)header] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:03:20.834Z] >>> [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["firebase.projects.get","firebaseextensions.instances.list"]}
[debug] [2025-04-30T19:03:21.855Z] <<< [apiv2][status] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions 200
[debug] [2025-04-30T19:03:21.856Z] <<< [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["firebase.projects.get","firebaseextensions.instances.list"]}
[debug] [2025-04-30T19:03:21.857Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:21.857Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:21.858Z] >>> [apiv2][query] GET https://firebaseextensions.googleapis.com/v1beta/projects/bigqueryexampleproject-e4ef9/instances pageSize=100&pageToken=
[debug] [2025-04-30T19:03:23.699Z] <<< [apiv2][status] GET https://firebaseextensions.googleapis.com/v1beta/projects/bigqueryexampleproject-e4ef9/instances 200
[debug] [2025-04-30T19:03:23.700Z] <<< [apiv2][body] GET https://firebaseextensions.googleapis.com/v1beta/projects/bigqueryexampleproject-e4ef9/instances {"instances":[{"name":"projects/bigqueryexampleproject-e4ef9/instances/firestore-bigquery-export","createTime":"2025-04-24T18:43:47.251262Z","updateTime":"2025-04-24T18:58:40.150355Z","state":"ACTIVE","config":{"name":"projects/bigqueryexampleproject-e4ef9/instances/firestore-bigquery-export/configurations/2661e7fb-2095-49c8-9edb-f7aeeb3fe7d6","createTime":"2025-04-24T18:43:47.251262Z","source":{"name":"projects/firebaseextensions/sources/1bcd5ca9-fd21-4b21-9984-8d6ee3a0a7c8","packageUri":"https://storage.googleapis.com/firebase-extensions-packages-prod/firebase-firestore-bigquery-export-0.2.2-713b5ff6-19ab-459d-8d21-b6eec0e8d5f9.zip","hash":"5df1592f3afe939bcf4a63279f7de800aade1153433d37d17a65109cf854dfa5","extensionRoot":"/","spec":{"specVersion":"v1beta","name":"firestore-bigquery-export","version":"0.2.2","description":"Sends realtime, incremental updates from a specified Cloud Firestore collection to BigQuery.","apis":[{"apiName":"bigquery.googleapis.com","reason":"Mirrors data from your Cloud Firestore collection in BigQuery."}],"roles":[{"role":"bigquery.dataEditor","reason":"Allows the extension to configure and export data into BigQuery."},{"role":"datastore.user","reason":"Allows the extension to write updates to the database."},{"role":"bigquery.user","reason":"Allows the extension to create and manage BigQuery materialized views."}],"resources":[{"name":"fsexportbigquery","type":"firebaseextensions.v1beta.v2function","propertiesYaml":"buildConfig:\n  runtime: nodejs22\neventTrigger:\n  eventFilters:\n  - attribute: database\n    value: ${DATABASE}\n  - attribute: document\n    operator: match-path-pattern\n    value: ${COLLECTION_PATH}/{documentId}\n  eventType: google.cloud.firestore.document.v1.written\n  triggerRegion: ${DATABASE_REGION}\nsourceDirectory: functions\n","description":"Listens for document changes in your specified Cloud Firestore collection, then exports the changes into BigQuery.","deletionPolicy":"DELETE"},{"name":"syncBigQuery","type":"firebaseextensions.v1beta.function","propertiesYaml":"runtime: nodejs20\ntaskQueueTrigger:\n  rateLimits:\n    maxConcurrentDispatches: 500\n    maxDispatchesPerSecond: ${param:MAX_DISPATCHES_PER_SECOND}\n  retryConfig:\n    maxAttempts: 5\n    minBackoffSeconds: 60\n","description":"A task-triggered function that gets called on BigQuery sync","deletionPolicy":"DELETE"},{"name":"initBigQuerySync","type":"firebaseextensions.v1beta.function","propertiesYaml":"runtime: nodejs20\ntaskQueueTrigger:\n  retryConfig:\n    maxAttempts: 15\n    minBackoffSeconds: 60\n","description":"Runs configuration for sycning with BigQuery","deletionPolicy":"DELETE"},{"name":"setupBigQuerySync","type":"firebaseextensions.v1beta.function","propertiesYaml":"runtime: nodejs20\ntaskQueueTrigger:\n  retryConfig:\n    maxAttempts: 15\n    minBackoffSeconds: 60\n","description":"Runs configuration for sycning with BigQuery","deletionPolicy":"DELETE"}],"billingRequired":true,"author":{"authorName":"Firebase","url":"https://firebase.google.com"},"contributors":[{"authorName":"Jan Wyszynski","email":"wyszynski@google.com","url":"https://github.com/IanWyszynski"}],"license":"Apache-2.0","releaseNotesUrl":"https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/CHANGELOG.md","sourceUrl":"https://github.com/firebase/extensions/tree/master/firestore-bigquery-export","params":[{"param":"DATASET_LOCATION","label":"BigQuery Dataset location","type":"SELECT","description":"Where do you want to deploy the BigQuery dataset created for this extension? For help selecting a location, refer to the [location selection guide](https://cloud.google.com/bigquery/docs/locations).","required":true,"options":[{"value":"us-central1","label":"Iowa (us-central1)"},{"value":"us-west4","label":"Las Vegas (us-west4)"},{"value":"europe-central2","label":"Warsaw (europe-central2)"},{"value":"us-west2","label":"Los Angeles (us-west2)"},{"value":"northamerica-northeast1","label":"Montreal (northamerica-northeast1)"},{"value":"us-east4","label":"Northern Virginia (us-east4)"},{"value":"us-west1","label":"Oregon (us-west1)"},{"value":"us-west3","label":"Salt Lake City (us-west3)"},{"value":"southamerica-east1","label":"Sao Paulo (southamerica-east1)"},{"value":"us-east1","label":"South Carolina (us-east1)"},{"value":"europe-west1","label":"Belgium (europe-west1)"},{"value":"europe-north1","label":"Finland (europe-north1)"},{"value":"europe-west3","label":"Frankfurt (europe-west3)"},{"value":"europe-west2","label":"London (europe-west2)"},{"value":"europe-west4","label":"Netherlands (europe-west4)"},{"value":"europe-west6","label":"Zurich (europe-west6)"},{"value":"asia-east1","label":"Taiwan (asia-east1)"},{"value":"asia-east2","label":"Hong Kong (asia-east2)"},{"value":"asia-southeast2","label":"Jakarta (asia-southeast2)"},{"value":"asia-south1","label":"Mumbai (asia-south1)"},{"value":"asia-southeast1","label":"Singapore (asia-southeast1)"},{"value":"asia-northeast2","label":"Osaka (asia-northeast2)"},{"value":"asia-northeast3","label":"Seoul (asia-northeast3)"},{"value":"asia-southeast1","label":"Singapore (asia-southeast1)"},{"value":"australia-southeast1","label":"Sydney (australia-southeast1)"},{"value":"asia-east1","label":"Taiwan (asia-east1)"},{"value":"asia-northeast1","label":"Tokyo (asia-northeast1)"},{"value":"us","label":"United States (multi-regional)"},{"value":"eu","label":"Europe (multi-regional)"}],"default":"us","immutable":true},{"param":"BIGQUERY_PROJECT_ID","label":"BigQuery Project ID","type":"STRING","description":"Override the default project for BigQuery instance. This can allow updates to be directed to to a BigQuery instance on another GCP project.","required":true,"default":"${PROJECT_ID}"},{"param":"DATABASE","label":"Firestore Instance ID","type":"STRING","description":"The Firestore database to use. Use \"(default)\" for the default database. You can view your available Firestore databases at https://console.cloud.google.com/firestore/databases.\n","required":true,"default":"(default)","example":"(default)"},{"param":"DATABASE_REGION","label":"Firestore Instance Location","type":"SELECT","description":"Where is the Firestore database located? You can check your current database location at https://console.cloud.google.com/firestore/databases.\n","required":true,"options":[{"value":"eur3","label":"Multi-region (Europe - Belgium and Netherlands)"},{"value":"nam5","label":"Multi-region (United States)"},{"value":"nam7","label":"Multi-region (Iowa, North Virginia, and Oklahoma)"},{"value":"us-central1","label":"Iowa (us-central1)"},{"value":"us-west1","label":"Oregon (us-west1)"},{"value":"us-west2","label":"Los Angeles (us-west2)"},{"value":"us-west3","label":"Salt Lake City (us-west3)"},{"value":"us-west4","label":"Las Vegas (us-west4)"},{"value":"us-east1","label":"South Carolina (us-east1)"},{"value":"us-east4","label":"Northern Virginia (us-east4)"},{"value":"us-east5","label":"Columbus (us-east5)"},{"value":"us-south1","label":"Dallas (us-south1)"},{"value":"northamerica-northeast1","label":"Montreal (northamerica-northeast1)"},{"value":"northamerica-northeast2","label":"Toronto (northamerica-northeast2)"},{"value":"northamerica-south1","label":"Queretaro (northamerica-south1)"},{"value":"southamerica-east1","label":"Sao Paulo (southamerica-east1)"},{"value":"southamerica-west1","label":"Santiago (southamerica-west1)"},{"value":"europe-west1","label":"Belgium (europe-west1)"},{"value":"europe-west2","label":"London (europe-west2)"},{"value":"europe-west3","label":"Frankfurt (europe-west3)"},{"value":"europe-west4","label":"Netherlands (europe-west4)"},{"value":"europe-west6","label":"Zurich (europe-west6)"},{"value":"europe-west8","label":"Milan (europe-west8)"},{"value":"europe-west9","label":"Paris (europe-west9)"},{"value":"europe-west10","label":"Berlin (europe-west10)"},{"value":"europe-west12","label":"Turin (europe-west12)"},{"value":"europe-southwest1","label":"Madrid (europe-southwest1)"},{"value":"europe-north1","label":"Finland (europe-north1)"},{"value":"europe-north2","label":"Stockholm (europe-north2)"},{"value":"europe-central2","label":"Warsaw (europe-central2)"},{"value":"me-central1","label":"Doha (me-central1)"},{"value":"me-central2","label":"Dammam (me-central2)"},{"value":"me-west1","label":"Tel Aviv (me-west1)"},{"value":"asia-south1","label":"Mumbai (asia-south1)"},{"value":"asia-south2","label":"Delhi (asia-south2)"},{"value":"asia-southeast1","label":"Singapore (asia-southeast1)"},{"value":"asia-southeast2","label":"Jakarta (asia-southeast2)"},{"value":"asia-east1","label":"Taiwan (asia-east1)"},{"value":"asia-east2","label":"Hong Kong (asia-east2)"},{"value":"asia-northeast1","label":"Tokyo (asia-northeast1)"},{"value":"asia-northeast2","label":"Osaka (asia-northeast2)"},{"value":"asia-northeast3","label":"Seoul (asia-northeast3)"},{"value":"australia-southeast1","label":"Sydney (australia-southeast1)"},{"value":"australia-southeast2","label":"Melbourne (australia-southeast2)"},{"value":"africa-south1","label":"Johannesburg (africa-south1)"}]},{"param":"COLLECTION_PATH","label":"Collection path","type":"STRING","description":"What is the path of the collection that you would like to export? You may use `{wildcard}` notation to match a subcollection of all documents in a collection (for example: `chatrooms/{chatid}/posts`). Parent Firestore Document IDs from `{wildcards}` can be returned in `path_params` as a JSON formatted string.","required":true,"default":"posts","example":"posts","validationRegex":"^[^/]+(/[^/]+/[^/]+)*$","validationErrorMessage":"Firestore collection paths must be an odd number of segments separated by slashes, e.g. \"path/to/collection\"."},{"param":"WILDCARD_IDS","label":"Enable Wildcard Column field with Parent Firestore Document IDs","type":"SELECT","description":"If enabled, creates a column containing a JSON object of all wildcard ids from a documents path.","options":[{"value":"false","label":"No"},{"value":"true","label":"Yes"}],"default":"false"},{"param":"DATASET_ID","label":"Dataset ID","type":"STRING","description":"What ID would you like to use for your BigQuery dataset? This extension will create the dataset, if it doesn't already exist.","required":true,"default":"firestore_export","example":"firestore_export","validationRegex":"^[a-zA-Z0-9_]+$","validationErrorMessage":"BigQuery dataset IDs must be alphanumeric (plus underscores) and must be no more than 1024 characters.\n"},{"param":"TABLE_ID","label":"Table ID","type":"STRING","description":"What identifying prefix would you like to use for your table and view inside your BigQuery dataset? This extension will create the table and view, if they don't already exist.","required":true,"default":"posts","example":"posts","validationRegex":"^[a-zA-Z0-9_]+$","validationErrorMessage":"BigQuery table IDs must be alphanumeric (plus underscores) and must be no more than 1024 characters.\n"},{"param":"TABLE_PARTITIONING","label":"BigQuery SQL table Time Partitioning option type","type":"SELECT","description":"This parameter will allow you to partition the BigQuery table and BigQuery view created by the extension based on data ingestion time. You may select the granularity of partitioning based upon one of: HOUR, DAY, MONTH, YEAR. This will generate one partition per day, hour, month or year, respectively.","options":[{"value":"HOUR","label":"hour"},{"value":"DAY","label":"day"},{"value":"MONTH","label":"month"},{"value":"YEAR","label":"year"},{"value":"NONE","label":"none"}],"default":"NONE"},{"param":"TIME_PARTITIONING_FIELD","label":"BigQuery Time Partitioning column name","type":"STRING","description":"BigQuery table column/schema field name for TimePartitioning. You can choose schema available as `timestamp` OR a new custom defined column that will be assigned to the selected Firestore Document field below. Defaults to pseudo column _PARTITIONTIME if unspecified. Cannot be changed if Table is already partitioned."},{"param":"TIME_PARTITIONING_FIRESTORE_FIELD","label":"Firestore Document field name for BigQuery SQL Time Partitioning field option","type":"STRING","description":"This parameter will allow you to partition the BigQuery table created by the extension based on selected. The Firestore Document field value must be a top-level TIMESTAMP, DATETIME, DATE field BigQuery string format or Firestore timestamp(will be converted to BigQuery TIMESTAMP). Cannot be changed if Table is already partitioned.\n example: `postDate`(Ensure that the Firestore-BigQuery export extension\ncreates the dataset and table before initiating any backfill scripts.\n This step is crucial for the partitioning to function correctly. It is\nessential for the script to insert data into an already partitioned table.)"},{"param":"TIME_PARTITIONING_FIELD_TYPE","label":"BigQuery SQL Time Partitioning table schema field(column) type","type":"SELECT","description":"Parameter for BigQuery SQL schema field type for the selected Time Partitioning Firestore Document field option. Cannot be changed if Table is already partitioned.","options":[{"value":"TIMESTAMP","label":"TIMESTAMP"},{"value":"DATETIME","label":"DATETIME"},{"value":"DATE","label":"DATE"},{"value":"omit","label":"omit"}],"default":"omit"},{"param":"CLUSTERING","label":"BigQuery SQL table clustering","type":"STRING","description":"This parameter allows you to set up clustering for the BigQuery table created by the extension. Specify up to 4 comma-separated fields (for example:  `data,document_id,timestamp` - no whitespaces). The order of the specified  columns determines the sort order of the data. \nnote: Cluster columns must be top-level, non-repeated columns of one of the  following types: BIGNUMERIC, BOOL, DATE, DATETIME, GEOGRAPHY, INT64, NUMERIC,  RANGE, STRING, TIMESTAMP. Clustering will not be added if a field with an invalid type is present in this parameter.\nAvailable schema extensions table fields for clustering include: `document_id, document_name, timestamp, event_id,  operation, data`.","example":"data,document_id,timestamp","validationRegex":"^[^,\\s]+(?:,[^,\\s]+){0,3}$","validationErrorMessage":"No whitespaces. Max 4 fields. e.g. `data,timestamp,event_id,operation`"},{"param":"MAX_DISPATCHES_PER_SECOND","label":"Maximum number of synced documents per second","type":"STRING","description":"This parameter will set the maximum number of syncronised documents per second with BQ. Please note, any other external updates to a Big Query table will be included within this quota. Ensure that you have a set a low enough number to compensate. Defaults to 100.","default":"100","validationRegex":"^([1-9]|[1-9][0-9]|[1-4][0-9]{2}|500)$","validationErrorMessage":"Please select a number between 1 and 500"},{"param":"VIEW_TYPE","label":"View Type","type":"SELECT","description":"Select the type of view to create in BigQuery. A regular view is a virtual table defined by a SQL query.  A materialized view persists the results of a query for faster access, with either incremental or  non-incremental updates. Please note that materialized views in this extension come with several  important caveats and limitations - carefully review the pre-install documentation before selecting  these options to ensure they are appropriate for your use case.","required":true,"options":[{"value":"view","label":"View"},{"value":"materialized_incremental","label":"Materialized View (Incremental)"},{"value":"materialized_non_incremental","label":"Materialized View (Non-incremental)"}],"default":"view"},{"param":"MAX_STALENESS","label":"Maximum Staleness Duration","type":"STRING","description":"For materialized views only: Specifies the maximum staleness acceptable for the materialized view.  Should be specified as an INTERVAL value following BigQuery SQL syntax.  This parameter will only take effect if View Type is set to a materialized view option.","example":"INTERVAL \"8:0:0\" HOUR TO SECOND"},{"param":"REFRESH_INTERVAL_MINUTES","label":"Refresh Interval (Minutes)","type":"STRING","description":"For materialized views only: Specifies how often the materialized view should be refreshed, in minutes.  This parameter will only take effect if View Type is set to a materialized view option.","example":"60","validationRegex":"^[1-9][0-9]*$","validationErrorMessage":"Must be a positive integer"},{"param":"BACKUP_COLLECTION","label":"Backup Collection Name","type":"STRING","description":"This (optional) parameter will allow you to specify a collection for which failed BigQuery updates will be written to."},{"param":"TRANSFORM_FUNCTION","label":"Transform function URL","type":"STRING","description":"Specify a function URL to call that will transform the payload that will be written to BigQuery. See the pre-install documentation for more details.","example":"https://us-west1-my-project-id.cloudfunctions.net/myTransformFunction"},{"param":"USE_NEW_SNAPSHOT_QUERY_SYNTAX","label":"Use new query syntax for snapshots","type":"SELECT","description":"If enabled, snapshots will be generated with the new query syntax, which should be more performant, and avoid potential resource limitations.","required":true,"options":[{"value":"yes","label":"Yes"},{"value":"no","label":"No"}],"default":"no"},{"param":"EXCLUDE_OLD_DATA","label":"Exclude old data payloads","type":"SELECT","description":"If enabled, table rows will never contain old data (document snapshot before the Firestore onDocumentUpdate event: `change.before.data()`). The reduction in data should be more performant, and avoid potential resource limitations.","options":[{"value":"yes","label":"Yes"},{"value":"no","label":"No"}],"default":"no"},{"param":"KMS_KEY_NAME","label":"Cloud KMS key name","type":"STRING","description":"Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS. If this parameter is set, the extension will specify the KMS key name when creating the BQ table. See the PREINSTALL.md for more details.","validationRegex":"projects/([^/]+)/locations/([^/]+)/keyRings/([^/]+)/cryptoKeys/([^/]+)","validationErrorMessage":"The key name must be of the format 'projects/PROJECT_NAME/locations/KEY_RING_LOCATION/keyRings/KEY_RING_ID/cryptoKeys/KEY_ID'."},{"param":"MAX_ENQUEUE_ATTEMPTS","label":"Maximum number of enqueue attempts","type":"STRING","description":"This parameter will set the maximum number of attempts to enqueue a document to cloud tasks for export to BigQuery.","default":"3","validationRegex":"^(10|[1-9])$","validationErrorMessage":"Please select an integer between 1 and 10"},{"param":"LOG_LEVEL","label":"Log level","type":"SELECT","description":"The log level for the extension. The log level controls the verbosity of the extension's logs. The available log levels are: debug, info, warn, and error. To reduce the volume of logs, use a log level of warn or error.","required":true,"options":[{"value":"debug","label":"Debug"},{"value":"info","label":"Info"},{"value":"warn","label":"Warn"},{"value":"error","label":"Error"},{"value":"silent","label":"Silent"}],"default":"info"}],"preinstallContent":"Use this extension to export the documents in a Cloud Firestore collection to BigQuery. Exports are realtime and incremental, so the data in BigQuery is a mirror of your content in Cloud Firestore.\n\nThe extension creates and updates a [dataset](https://cloud.google.com/bigquery/docs/datasets-intro) containing the following two BigQuery resources:\n\n- A [table](https://cloud.google.com/bigquery/docs/tables-intro) of raw data that stores a full change history of the documents within your collection. This table includes a number of metadata fields so that BigQuery can display the current state of your data. The principle metadata fields are `timestamp`, `document_name`, and the `operation` for the document change.\n- A [view](https://cloud.google.com/bigquery/docs/views-intro) which represents the current state of the data within your collection. It also shows a log of the latest `operation` for each document (`CREATE`, `UPDATE`, or `IMPORT`).\n\n*Warning*: A BigQuery table corresponding to your configuration will be automatically generated upon installing or updating this extension. Manual table creation may result in discrepancies with your configured settings.\n\nIf you create, update, or delete a document in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`). \n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in ${param:COLLECTION_PATH}. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\nNote: To enable partitioning for a Big Query database, the following fields are required:\n\n - Time Partitioning option type\n - Time partitioning column name\n - Time partiitioning table schema\n - Firestore document field name\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n\n\n#### Additional setup\n\nBefore installing this extension, you'll need to:\n\n- [Set up Cloud Firestore in your Firebase project.](https://firebase.google.com/docs/firestore/quickstart)\n- [Link your Firebase project to BigQuery.](https://support.google.com/firebase/answer/6318765)\n\n\n#### Import existing documents\n\nTo import existing documents you can run the external [import script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n**Important:** Run the external import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nWithout use of this import script, the extension only exports the content of documents that are created or changed after installation.\n\n#### Transform function\n\nPrior to sending the document change to BigQuery, you have an opportunity to transform the data with an HTTP function. The payload will contain the following:\n\n```\n{ \n  data: [{\n    insertId: int;\n    json: {\n      timestamp: int;\n      event_id: int;\n      document_name: string;\n      document_id: int;\n      operation: ChangeType;\n      data: string;\n    },\n  }]\n}\n```\n\nThe response should be indentical in structure.\n\n#### Materialized Views\n\nThis extension supports both regular views and materialized views in BigQuery. While regular views compute their results each time they're queried, materialized views store their query results, providing faster access at the cost of additional storage.\n\nThere are two types of materialized views available:\n\n1. **Non-incremental Materialized Views**: These views support more complex queries including filtering on aggregated fields, but require complete recomputation during refresh.\n\n2. **Incremental Materialized Views**: These views update more efficiently by processing only new or changed records, but come with query restrictions. Most notably, they don't allow filtering or partitioning on aggregated fields in their defining SQL, among other limitations.\n\n**Important Considerations:**\n- Neither type of materialized view in this extension currently supports partitioning or clustering\n- Both types allow you to configure refresh intervals and maximum staleness settings during extension installation or configuration\n- Once created, a materialized view's SQL definition cannot be modified. If you reconfigure the extension to change either the view type (incremental vs non-incremental) or the SQL query, the extension will drop the existing materialized view and recreate it\n- Carefully consider your use case before choosing materialized views:\n  - They incur additional storage costs as they cache query results\n  - Non-incremental views may have higher processing costs during refresh\n  - Incremental views have more query restrictions but are more efficient to update\n\nExample of a non-incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    allow_non_incremental_definition = true,\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\n  AS (\n    WITH latests AS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY document_name\n    )\n    SELECT *\n    FROM latests\n    WHERE operation != \"DELETE\"\n  )\n```\n\nExample of an incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\nAS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM\n        `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY\n        document_name\n    )\n```\n\nPlease review [BigQuery's documentation on materialized views](https://cloud.google.com/bigquery/docs/materialized-views-intro) to fully understand the implications for your use case.\n\n#### Using Customer Managed Encryption Keys\n\nBy default, BigQuery encrypts your content stored at rest. BigQuery handles and manages this default encryption for you without any additional actions on your part.\n\nIf you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery. Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS.\n\nFor more general information on this, see [the docs](https://cloud.google.com/bigquery/docs/customer-managed-encryption).\n\nTo use CMEK and the Key Management Service (KMS) with this extension\n1. [Enable the KMS API in your Google Cloud Project](https://console.cloud.google.com/apis/enableflow?apiid=cloudkms.googleapis.com).\n2. Create a keyring and keychain in the KMS. Note that the region of the keyring and key *must* match the region of your bigquery dataset\n3. Grant the BigQuery service account permission to encrypt and decrypt using that key. The Cloud KMS CryptoKey Encrypter/Decrypter role grants this permission. First find your project number. You can find this for example on the cloud console dashboard `https://console.cloud.google.com/home/dashboard?project={PROJECT_ID}`. The service account which needs the Encrypter/Decrypter role is then `bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com`. You can grant this role through the credentials service in the console, or through the CLI:\n```\ngcloud kms keys add-iam-policy-binding \\\n--project=KMS_PROJECT_ID \\\n--member serviceAccount:bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com \\\n--role roles/cloudkms.cryptoKeyEncrypterDecrypter \\\n--location=KMS_KEY_LOCATION \\\n--keyring=KMS_KEY_RING \\\nKMS_KEY\n```\n4. When installing this extension, enter the resource name of your key. It will look something like the following:\n```\nprojects/<YOUR PROJECT ID>/locations/<YOUR REGION>/keyRings/<YOUR KEY RING NAME>/cryptoKeys/<YOUR KEY NAME>\n```\nIf you follow these steps, your changelog table should be created using your customer-managed encryption.\n\n#### Generate schema views\n\nAfter your data is in BigQuery, you can run the [schema-views script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md) (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter to your target BigQuery project ID.\n\n2. After installation, you'll need to grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n#### Mitigating Data Loss During Extension Updates\n\nWhen updating or reconfiguring this extension, there may be a brief period where data streaming from Firestore to BigQuery is interrupted. While this limitation exists within the Extensions platform, we provide two strategies to mitigate potential data loss.\n\n##### Strategy 1: Post-Update Import\nAfter reconfiguring the extension, run the import script on your collection to ensure all data is captured. Refer to the \"Import Existing Documents\" section above for detailed steps.\n\n##### Strategy 2: Parallel Instance Method\n1. Install a second instance of the extension that streams to a new BigQuery table\n2. Reconfigure the original extension\n3. Once the original extension is properly configured and streaming events\n4. Uninstall the second instance\n5. Run a BigQuery merge job to combine the data from both tables\n\n##### Considerations\n- Strategy 1 is simpler but may result in duplicate records that need to be deduplicated\n- Strategy 2 requires more setup but provides better data continuity\n- Choose the strategy that best aligns with your data consistency requirements and operational constraints\n\n#### Billing\nTo install an extension, your project must be on the [Blaze (pay as you go) plan](https://firebase.google.com/pricing)\n\n- This extension uses other Firebase and Google Cloud Platform services, which have associated charges if you exceed the service’s no-cost tier:\n  - BigQuery (this extension writes to BigQuery with [streaming inserts](https://cloud.google.com/bigquery/pricing#streaming_pricing))\n  - Cloud Firestore\n  - Cloud Functions (Node.js 10+ runtime. [See FAQs](https://firebase.google.com/support/faq#extensions-pricing))","postinstallContent":"### See it in action\n\nYou can test out this extension right away!\n\n1.  Go to your [Cloud Firestore dashboard](https://console.firebase.google.com/project/${param:BIGQUERY_PROJECT_ID}/firestore/data) in the Firebase console.\n\n2.  If it doesn't already exist, create the collection you specified during installation: `${param:COLLECTION_PATH}`\n\n3.  Create a document in the collection called `bigquery-mirror-test` that contains any fields with any values that you'd like.\n\n4.  Go to the [BigQuery web UI](https://console.cloud.google.com/bigquery?project=${param:BIGQUERY_PROJECT_ID}&p=${param:BIGQUERY_PROJECT_ID}&d=${param:DATASET_ID}) in the Google Cloud Platform console.\n\n5.  Query your **raw changelog table**, which should contain a single log of creating the `bigquery-mirror-test` document.\n\n    ```\n    SELECT *\n    FROM `${param:BIGQUERY_PROJECT_ID}.${param:DATASET_ID}.${param:TABLE_ID}_raw_changelog`\n    ```\n\n6.  Query your **latest view**, which should return the latest change event for the only document present -- `bigquery-mirror-test`.\n\n    ```\n    SELECT *\n    FROM `${param:BIGQUERY_PROJECT_ID}.${param:DATASET_ID}.${param:TABLE_ID}_raw_latest`\n    ```\n\n7.  Delete the `bigquery-mirror-test` document from [Cloud Firestore](https://console.firebase.google.com/project/${param:BIGQUERY_PROJECT_ID}/firestore/data).\n    The `bigquery-mirror-test` document will disappear from the **latest view** and a `DELETE` event will be added to the **raw changelog table**.\n\n8.  You can check the changelogs of a single document with this query:\n\n    ```\n    SELECT *\n    FROM `${param:BIGQUERY_PROJECT_ID}.${param:DATASET_ID}.${param:TABLE_ID}_raw_changelog`\n    WHERE document_name = \"bigquery-mirror-test\"\n    ORDER BY TIMESTAMP ASC\n    ```\n\n### Using the extension\n\nWhenever a document is created, updated, imported, or deleted in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset which contains the following resources:\n\n- **raw changelog table:** [`${param:TABLE_ID}_raw_changelog`](https://console.cloud.google.com/bigquery?project=${param:BIGQUERY_PROJECT_ID}&p=${param:BIGQUERY_PROJECT_ID}&d=${param:DATASET_ID}&t=${param:TABLE_ID}_raw_changelog&page=table)\n- **latest view:** [`${param:TABLE_ID}_raw_latest`](https://console.cloud.google.com/bigquery?project=${param:BIGQUERY_PROJECT_ID}&p=${param:BIGQUERY_PROJECT_ID}&d=${param:DATASET_ID}&t=${param:TABLE_ID}_raw_latest&page=table)\n\nTo review the schema for these two resources, click the **Schema** tab for each resource in BigQuery.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`).\n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in ${param:COLLECTION_PATH}. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter as your target BigQuery project ID.\n\n2. Identify the service account on the source project associated with the extension. By default, it will be constructed as `ext-<extension-instance-id>@project-id.iam.gserviceaccount.com`. However, if the extension instance ID is too long, it may be truncated and 4 random characters appended to abide by service account length limits.\n\n3. To find the exact service account, navigate to IAM & Admin -> IAM in the Google Cloud Platform Console. Look for the service account listed with \"Name\" as \"Firebase Extensions <your extension instance ID> service account\". The value in the \"Principal\" column will be the service account that needs permissions granted in the target project.\n\n4. Grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID] [-s SERVICE_ACCOUNT]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID] [-ServiceAccount SERVICE_ACCOUNT]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-s`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-ServiceAccount`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n### _(Optional)_ Import existing documents\n\nYou can backfill your BigQuery dataset with all the documents in your collection using the import script.\n\nIf you don't run the import script, the extension only exports the content of documents that are created or changed after installation.\n\nThe import script can read all existing documents in a Cloud Firestore collection and insert them into the raw changelog table created by this extension. The script adds a special changelog for each document with the operation of `IMPORT` and the timestamp of epoch. This is to ensure that any operation on an imported document supersedes the `IMPORT`.\n\n**Important:** Run the import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nLearn more about using the import script to [backfill your existing collection](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n### _(Optional)_ Generate schema views\n\nAfter your data is in BigQuery, you can use the schema-views script (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\nLearn more about using the schema-views script to [generate schema views](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md).\n\n### Monitoring\n\nAs a best practice, you can [monitor the activity](https://firebase.google.com/docs/extensions/manage-installed-extensions#monitor) of your installed extension, including checks on its health, usage, and logs.\n","readmeContent":"# Stream Firestore to BigQuery\n\n**Author**: Firebase (**[https://firebase.google.com](https://firebase.google.com)**)\n\n**Description**: Sends realtime, incremental updates from a specified Cloud Firestore collection to BigQuery.\n\n\n\n**Details**: Use this extension to export the documents in a Cloud Firestore collection to BigQuery. Exports are realtime and incremental, so the data in BigQuery is a mirror of your content in Cloud Firestore.\n\nThe extension creates and updates a [dataset](https://cloud.google.com/bigquery/docs/datasets-intro) containing the following two BigQuery resources:\n\n- A [table](https://cloud.google.com/bigquery/docs/tables-intro) of raw data that stores a full change history of the documents within your collection. This table includes a number of metadata fields so that BigQuery can display the current state of your data. The principle metadata fields are `timestamp`, `document_name`, and the `operation` for the document change.\n- A [view](https://cloud.google.com/bigquery/docs/views-intro) which represents the current state of the data within your collection. It also shows a log of the latest `operation` for each document (`CREATE`, `UPDATE`, or `IMPORT`).\n\n*Warning*: A BigQuery table corresponding to your configuration will be automatically generated upon installing or updating this extension. Manual table creation may result in discrepancies with your configured settings.\n\nIf you create, update, or delete a document in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`). \n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in ${param:COLLECTION_PATH}. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\nNote: To enable partitioning for a Big Query database, the following fields are required:\n\n - Time Partitioning option type\n - Time partitioning column name\n - Time partiitioning table schema\n - Firestore document field name\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n\n\n#### Additional setup\n\nBefore installing this extension, you'll need to:\n\n- [Set up Cloud Firestore in your Firebase project.](https://firebase.google.com/docs/firestore/quickstart)\n- [Link your Firebase project to BigQuery.](https://support.google.com/firebase/answer/6318765)\n\n\n#### Import existing documents\n\nTo import existing documents you can run the external [import script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n**Important:** Run the external import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nWithout use of this import script, the extension only exports the content of documents that are created or changed after installation.\n\n#### Transform function\n\nPrior to sending the document change to BigQuery, you have an opportunity to transform the data with an HTTP function. The payload will contain the following:\n\n```\n{ \n  data: [{\n    insertId: int;\n    json: {\n      timestamp: int;\n      event_id: int;\n      document_name: string;\n      document_id: int;\n      operation: ChangeType;\n      data: string;\n    },\n  }]\n}\n```\n\nThe response should be indentical in structure.\n\n#### Materialized Views\n\nThis extension supports both regular views and materialized views in BigQuery. While regular views compute their results each time they're queried, materialized views store their query results, providing faster access at the cost of additional storage.\n\nThere are two types of materialized views available:\n\n1. **Non-incremental Materialized Views**: These views support more complex queries including filtering on aggregated fields, but require complete recomputation during refresh.\n\n2. **Incremental Materialized Views**: These views update more efficiently by processing only new or changed records, but come with query restrictions. Most notably, they don't allow filtering or partitioning on aggregated fields in their defining SQL, among other limitations.\n\n**Important Considerations:**\n- Neither type of materialized view in this extension currently supports partitioning or clustering\n- Both types allow you to configure refresh intervals and maximum staleness settings during extension installation or configuration\n- Once created, a materialized view's SQL definition cannot be modified. If you reconfigure the extension to change either the view type (incremental vs non-incremental) or the SQL query, the extension will drop the existing materialized view and recreate it\n- Carefully consider your use case before choosing materialized views:\n  - They incur additional storage costs as they cache query results\n  - Non-incremental views may have higher processing costs during refresh\n  - Incremental views have more query restrictions but are more efficient to update\n\nExample of a non-incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    allow_non_incremental_definition = true,\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\n  AS (\n    WITH latests AS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY document_name\n    )\n    SELECT *\n    FROM latests\n    WHERE operation != \"DELETE\"\n  )\n```\n\nExample of an incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\nAS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM\n        `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY\n        document_name\n    )\n```\n\nPlease review [BigQuery's documentation on materialized views](https://cloud.google.com/bigquery/docs/materialized-views-intro) to fully understand the implications for your use case.\n\n#### Using Customer Managed Encryption Keys\n\nBy default, BigQuery encrypts your content stored at rest. BigQuery handles and manages this default encryption for you without any additional actions on your part.\n\nIf you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery. Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS.\n\nFor more general information on this, see [the docs](https://cloud.google.com/bigquery/docs/customer-managed-encryption).\n\nTo use CMEK and the Key Management Service (KMS) with this extension\n1. [Enable the KMS API in your Google Cloud Project](https://console.cloud.google.com/apis/enableflow?apiid=cloudkms.googleapis.com).\n2. Create a keyring and keychain in the KMS. Note that the region of the keyring and key *must* match the region of your bigquery dataset\n3. Grant the BigQuery service account permission to encrypt and decrypt using that key. The Cloud KMS CryptoKey Encrypter/Decrypter role grants this permission. First find your project number. You can find this for example on the cloud console dashboard `https://console.cloud.google.com/home/dashboard?project={PROJECT_ID}`. The service account which needs the Encrypter/Decrypter role is then `bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com`. You can grant this role through the credentials service in the console, or through the CLI:\n```\ngcloud kms keys add-iam-policy-binding \\\n--project=KMS_PROJECT_ID \\\n--member serviceAccount:bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com \\\n--role roles/cloudkms.cryptoKeyEncrypterDecrypter \\\n--location=KMS_KEY_LOCATION \\\n--keyring=KMS_KEY_RING \\\nKMS_KEY\n```\n4. When installing this extension, enter the resource name of your key. It will look something like the following:\n```\nprojects/<YOUR PROJECT ID>/locations/<YOUR REGION>/keyRings/<YOUR KEY RING NAME>/cryptoKeys/<YOUR KEY NAME>\n```\nIf you follow these steps, your changelog table should be created using your customer-managed encryption.\n\n#### Generate schema views\n\nAfter your data is in BigQuery, you can run the [schema-views script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md) (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter to your target BigQuery project ID.\n\n2. After installation, you'll need to grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n#### Mitigating Data Loss During Extension Updates\n\nWhen updating or reconfiguring this extension, there may be a brief period where data streaming from Firestore to BigQuery is interrupted. While this limitation exists within the Extensions platform, we provide two strategies to mitigate potential data loss.\n\n##### Strategy 1: Post-Update Import\nAfter reconfiguring the extension, run the import script on your collection to ensure all data is captured. Refer to the \"Import Existing Documents\" section above for detailed steps.\n\n##### Strategy 2: Parallel Instance Method\n1. Install a second instance of the extension that streams to a new BigQuery table\n2. Reconfigure the original extension\n3. Once the original extension is properly configured and streaming events\n4. Uninstall the second instance\n5. Run a BigQuery merge job to combine the data from both tables\n\n##### Considerations\n- Strategy 1 is simpler but may result in duplicate records that need to be deduplicated\n- Strategy 2 requires more setup but provides better data continuity\n- Choose the strategy that best aligns with your data consistency requirements and operational constraints\n\n#### Billing\nTo install an extension, your project must be on the [Blaze (pay as you go) plan](https://firebase.google.com/pricing)\n\n- This extension uses other Firebase and Google Cloud Platform services, which have associated charges if you exceed the service’s no-cost tier:\n  - BigQuery (this extension writes to BigQuery with [streaming inserts](https://cloud.google.com/bigquery/pricing#streaming_pricing))\n  - Cloud Firestore\n  - Cloud Functions (Node.js 10+ runtime. [See FAQs](https://firebase.google.com/support/faq#extensions-pricing))\n\n\n\n**Configuration Parameters:**\n\n* BigQuery Dataset location: Where do you want to deploy the BigQuery dataset created for this extension? For help selecting a location, refer to the [location selection guide](https://cloud.google.com/bigquery/docs/locations).\n\n* BigQuery Project ID: Override the default project for BigQuery instance. This can allow updates to be directed to to a BigQuery instance on another GCP project.\n\n* Firestore Instance ID: The Firestore database to use. Use \"(default)\" for the default database. You can view your available Firestore databases at https://console.cloud.google.com/firestore/databases.\n\n\n* Firestore Instance Location: Where is the Firestore database located? You can check your current database location at https://console.cloud.google.com/firestore/databases.\n\n\n* Collection path: What is the path of the collection that you would like to export? You may use `{wildcard}` notation to match a subcollection of all documents in a collection (for example: `chatrooms/{chatid}/posts`). Parent Firestore Document IDs from `{wildcards}` can be returned in `path_params` as a JSON formatted string.\n\n* Enable Wildcard Column field with Parent Firestore Document IDs: If enabled, creates a column containing a JSON object of all wildcard ids from a documents path.\n\n* Dataset ID: What ID would you like to use for your BigQuery dataset? This extension will create the dataset, if it doesn't already exist.\n\n* Table ID: What identifying prefix would you like to use for your table and view inside your BigQuery dataset? This extension will create the table and view, if they don't already exist.\n\n* BigQuery SQL table Time Partitioning option type: This parameter will allow you to partition the BigQuery table and BigQuery view created by the extension based on data ingestion time. You may select the granularity of partitioning based upon one of: HOUR, DAY, MONTH, YEAR. This will generate one partition per day, hour, month or year, respectively.\n\n* BigQuery Time Partitioning column name: BigQuery table column/schema field name for TimePartitioning. You can choose schema available as `timestamp` OR a new custom defined column that will be assigned to the selected Firestore Document field below. Defaults to pseudo column _PARTITIONTIME if unspecified. Cannot be changed if Table is already partitioned.\n\n* Firestore Document field name for BigQuery SQL Time Partitioning field option: This parameter will allow you to partition the BigQuery table created by the extension based on selected. The Firestore Document field value must be a top-level TIMESTAMP, DATETIME, DATE field BigQuery string format or Firestore timestamp(will be converted to BigQuery TIMESTAMP). Cannot be changed if Table is already partitioned.\n example: `postDate`(Ensure that the Firestore-BigQuery export extension\ncreates the dataset and table before initiating any backfill scripts.\n This step is crucial for the partitioning to function correctly. It is\nessential for the script to insert data into an already partitioned table.)\n\n* BigQuery SQL Time Partitioning table schema field(column) type: Parameter for BigQuery SQL schema field type for the selected Time Partitioning Firestore Document field option. Cannot be changed if Table is already partitioned.\n\n* BigQuery SQL table clustering: This parameter allows you to set up clustering for the BigQuery table created by the extension. Specify up to 4 comma-separated fields (for example:  `data,document_id,timestamp` - no whitespaces). The order of the specified  columns determines the sort order of the data. \nNote: Cluster columns must be top-level, non-repeated columns of one of the  following types: BIGNUMERIC, BOOL, DATE, DATETIME, GEOGRAPHY, INT64, NUMERIC,  RANGE, STRING, TIMESTAMP. Clustering will not be added if a field with an invalid type is present in this parameter.\nAvailable schema extensions table fields for clustering include: `document_id, document_name, timestamp, event_id,  operation, data`.\n\n* Maximum number of synced documents per second: This parameter will set the maximum number of syncronised documents per second with BQ. Please note, any other external updates to a Big Query table will be included within this quota. Ensure that you have a set a low enough number to compensate. Defaults to 100.\n\n* View Type: Select the type of view to create in BigQuery. A regular view is a virtual table defined by a SQL query.  A materialized view persists the results of a query for faster access, with either incremental or  non-incremental updates. Please note that materialized views in this extension come with several  important caveats and limitations - carefully review the pre-install documentation before selecting  these options to ensure they are appropriate for your use case.\n\n* Maximum Staleness Duration: For materialized views only: Specifies the maximum staleness acceptable for the materialized view.  Should be specified as an INTERVAL value following BigQuery SQL syntax.  This parameter will only take effect if View Type is set to a materialized view option.\n\n* Refresh Interval (Minutes): For materialized views only: Specifies how often the materialized view should be refreshed, in minutes.  This parameter will only take effect if View Type is set to a materialized view option.\n\n* Backup Collection Name: This (optional) parameter will allow you to specify a collection for which failed BigQuery updates will be written to.\n\n* Transform function URL: Specify a function URL to call that will transform the payload that will be written to BigQuery. See the pre-install documentation for more details.\n\n* Use new query syntax for snapshots: If enabled, snapshots will be generated with the new query syntax, which should be more performant, and avoid potential resource limitations.\n\n* Exclude old data payloads: If enabled, table rows will never contain old data (document snapshot before the Firestore onDocumentUpdate event: `change.before.data()`). The reduction in data should be more performant, and avoid potential resource limitations.\n\n* Cloud KMS key name: Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS. If this parameter is set, the extension will specify the KMS key name when creating the BQ table. See the PREINSTALL.md for more details.\n\n* Maximum number of enqueue attempts: This parameter will set the maximum number of attempts to enqueue a document to cloud tasks for export to BigQuery.\n\n* Log level: The log level for the extension. The log level controls the verbosity of the extension's logs. The available log levels are: debug, info, warn, and error. To reduce the volume of logs, use a log level of warn or error.\n\n\n\n**Cloud Functions:**\n\n* **syncBigQuery:** A task-triggered function that gets called on BigQuery sync\n\n* **initBigQuerySync:** Runs configuration for sycning with BigQuery\n\n* **setupBigQuerySync:** Runs configuration for sycning with BigQuery\n\n\n\n**Other Resources**:\n\n* fsexportbigquery (firebaseextensions.v1beta.v2function)\n\n\n\n**APIs Used**:\n\n* bigquery.googleapis.com (Reason: Mirrors data from your Cloud Firestore collection in BigQuery.)\n\n\n\n**Access Required**:\n\n\n\nThis extension will operate with the following project IAM roles:\n\n* bigquery.dataEditor (Reason: Allows the extension to configure and export data into BigQuery.)\n\n* datastore.user (Reason: Allows the extension to write updates to the database.)\n\n* bigquery.user (Reason: Allows the extension to create and manage BigQuery materialized views.)\n","lifecycleEvents":[{"stage":"ON_INSTALL","processingMessage":"Configuring BigQuery Sync.","taskQueueTriggerFunction":"initBigQuerySync"},{"stage":"ON_UPDATE","processingMessage":"Configuring BigQuery Sync","taskQueueTriggerFunction":"setupBigQuerySync"},{"stage":"ON_CONFIGURE","processingMessage":"Configuring BigQuery Sync","taskQueueTriggerFunction":"setupBigQuerySync"}],"displayName":"Stream Firestore to BigQuery","events":[{"type":"firebase.extensions.firestore-counter.v1.onStart","description":"Occurs when a trigger has been called within the Extension, and will include data such as the context of the trigger request."},{"type":"firebase.extensions.firestore-counter.v1.onSuccess","description":"Occurs when a task completes successfully. The event will contain further details about specific results."},{"type":"firebase.extensions.firestore-counter.v1.onError","description":"Occurs when an issue has been experienced in the Extension. This will include any error data that has been included within the Error Exception."},{"type":"firebase.extensions.firestore-counter.v1.onCompletion","description":"Occurs when the function is settled. Provides no customized data other than the context."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onStart","description":"Occurs when a trigger has been called within the Extension, and will include data such as the context of the trigger request."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onSuccess","description":"Occurs when a task completes successfully. The event will contain further details about specific results."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onError","description":"Occurs when an issue has been experienced in the Extension. This will include any error data that has been included within the Error Exception."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onCompletion","description":"Occurs when the function is settled. Provides no customized data other than the context."},{"type":"firebase.extensions.big-query-export.v1.sync.start","description":"Occurs on a firestore document write event."}]},"fetchTime":"2025-04-22T16:19:47.253450Z","lastOperationName":"projects/firebaseextensions/operations/373aa99b-62f5-4774-9ed8-8f5c1eb00e33","state":"ACTIVE"},"params":{"DATABASE":"(default)","TABLE_ID":"bigqueryexampleproject_users","BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","VIEW_TYPE":"view","DATASET_ID":"firestore_export","MAX_ENQUEUE_ATTEMPTS":"3","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","DATASET_LOCATION":"us","TABLE_PARTITIONING":"NONE","LOG_LEVEL":"silent","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","DATABASE_REGION":"nam5","WILDCARD_IDS":"false","COLLECTION_PATH":"users","MAX_DISPATCHES_PER_SECOND":"100","EXCLUDE_OLD_DATA":"no"},"populatedPostinstallContent":"### See it in action\n\nYou can test out this extension right away!\n\n1.  Go to your [Cloud Firestore dashboard](https://console.firebase.google.com/project/bigqueryexampleproject-e4ef9/firestore/data) in the Firebase console.\n\n2.  If it doesn't already exist, create the collection you specified during installation: `users`\n\n3.  Create a document in the collection called `bigquery-mirror-test` that contains any fields with any values that you'd like.\n\n4.  Go to the [BigQuery web UI](https://console.cloud.google.com/bigquery?project=bigqueryexampleproject-e4ef9&p=bigqueryexampleproject-e4ef9&d=firestore_export) in the Google Cloud Platform console.\n\n5.  Query your **raw changelog table**, which should contain a single log of creating the `bigquery-mirror-test` document.\n\n    ```\n    SELECT *\n    FROM `bigqueryexampleproject-e4ef9.firestore_export.bigqueryexampleproject_users_raw_changelog`\n    ```\n\n6.  Query your **latest view**, which should return the latest change event for the only document present -- `bigquery-mirror-test`.\n\n    ```\n    SELECT *\n    FROM `bigqueryexampleproject-e4ef9.firestore_export.bigqueryexampleproject_users_raw_latest`\n    ```\n\n7.  Delete the `bigquery-mirror-test` document from [Cloud Firestore](https://console.firebase.google.com/project/bigqueryexampleproject-e4ef9/firestore/data).\n    The `bigquery-mirror-test` document will disappear from the **latest view** and a `DELETE` event will be added to the **raw changelog table**.\n\n8.  You can check the changelogs of a single document with this query:\n\n    ```\n    SELECT *\n    FROM `bigqueryexampleproject-e4ef9.firestore_export.bigqueryexampleproject_users_raw_changelog`\n    WHERE document_name = \"bigquery-mirror-test\"\n    ORDER BY TIMESTAMP ASC\n    ```\n\n### Using the extension\n\nWhenever a document is created, updated, imported, or deleted in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset which contains the following resources:\n\n- **raw changelog table:** [`bigqueryexampleproject_users_raw_changelog`](https://console.cloud.google.com/bigquery?project=bigqueryexampleproject-e4ef9&p=bigqueryexampleproject-e4ef9&d=firestore_export&t=bigqueryexampleproject_users_raw_changelog&page=table)\n- **latest view:** [`bigqueryexampleproject_users_raw_latest`](https://console.cloud.google.com/bigquery?project=bigqueryexampleproject-e4ef9&p=bigqueryexampleproject-e4ef9&d=firestore_export&t=bigqueryexampleproject_users_raw_latest&page=table)\n\nTo review the schema for these two resources, click the **Schema** tab for each resource in BigQuery.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`).\n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in users. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter as your target BigQuery project ID.\n\n2. Identify the service account on the source project associated with the extension. By default, it will be constructed as `ext-<extension-instance-id>@project-id.iam.gserviceaccount.com`. However, if the extension instance ID is too long, it may be truncated and 4 random characters appended to abide by service account length limits.\n\n3. To find the exact service account, navigate to IAM & Admin -> IAM in the Google Cloud Platform Console. Look for the service account listed with \"Name\" as \"Firebase Extensions <your extension instance ID> service account\". The value in the \"Principal\" column will be the service account that needs permissions granted in the target project.\n\n4. Grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID] [-s SERVICE_ACCOUNT]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID] [-ServiceAccount SERVICE_ACCOUNT]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-s`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-ServiceAccount`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n### _(Optional)_ Import existing documents\n\nYou can backfill your BigQuery dataset with all the documents in your collection using the import script.\n\nIf you don't run the import script, the extension only exports the content of documents that are created or changed after installation.\n\nThe import script can read all existing documents in a Cloud Firestore collection and insert them into the raw changelog table created by this extension. The script adds a special changelog for each document with the operation of `IMPORT` and the timestamp of epoch. This is to ensure that any operation on an imported document supersedes the `IMPORT`.\n\n**Important:** Run the import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nLearn more about using the import script to [backfill your existing collection](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n### _(Optional)_ Generate schema views\n\nAfter your data is in BigQuery, you can use the schema-views script (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\nLearn more about using the schema-views script to [generate schema views](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md).\n\n### Monitoring\n\nAs a best practice, you can [monitor the activity](https://firebase.google.com/docs/extensions/manage-installed-extensions#monitor) of your installed extension, including checks on its health, usage, and logs.\n","extensionRef":"firebase/firestore-bigquery-export","extensionVersion":"0.2.2","systemParams":{"firebaseextensions.v1beta.function/memory":"256","firebaseextensions.v1beta.v2function/memory":"256Mi","firebaseextensions.v1beta.function/timeoutSeconds":"120","firebaseextensions.v1beta.function/vpcConnectorEgressSettings":"VPC_CONNECTOR_EGRESS_SETTINGS_UNSPECIFIED","firebaseextensions.v1beta.function/minInstances":"0","firebaseextensions.v1beta.function/location":"us-central1"}},"lastOperationName":"projects/bigqueryexampleproject-e4ef9/operations/a61b124c-6b2a-4a85-ad68-8beffc0b723f","serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","lastOperationType":"CREATE","etag":"6f5055cc72d80ee6d7f93dc8315ae50a19612411f29d1ef679c70f1fde2da1ed","runtimeData":{"stateUpdateTime":"2025-04-24T19:01:43.791080021Z","processingState":{"state":"PROCESSING_COMPLETE","detailMessage":"Sync setup completed"}}}]}
[info] i  functions: preparing functions directory for uploading... 
[info] i  functions: packaged /home/wellington/Documentos/Git/BigQueryExample/Functions-Firebase/functions (64.31 KB) for uploading 
[debug] [2025-04-30T19:03:23.784Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:23.784Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:23.784Z] >>> [apiv2][query] GET https://cloudfunctions.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/-/functions [none]
[debug] [2025-04-30T19:03:25.192Z] <<< [apiv2][status] GET https://cloudfunctions.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/-/functions 200
[debug] [2025-04-30T19:03:25.192Z] <<< [apiv2][body] GET https://cloudfunctions.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/-/functions {"functions":[{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-initBigQuerySync","description":"Runs configuration for sycning with BigQuery","sourceArchiveUrl":"gs://firebase-mod-sources-prod/a8a3e54cd84beb56c7eb0f2379f70adbd1427513f860a1f544893c92df16a047","httpsTrigger":{"url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-initBigQuerySync","securityLevel":"SECURE_OPTIONAL"},"status":"ACTIVE","entryPoint":"initBigQuerySync","timeout":"540s","availableMemoryMb":256,"serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","updateTime":"2025-04-24T18:58:11.627Z","versionId":"2","labels":{"firebase-extensions-ar":"enabled","goog-dm":"firebase-ext-firestore-bigquery-export","goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions","goog-firebase-ext-iid":"firestore-bigquery-export"},"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false"},"runtime":"nodejs20","ingressSettings":"ALLOW_ALL","buildId":"539681f6-8d64-426e-b876-e7462e9cc571","buildEnvironmentVariables":{"GOOGLE_NODE_RUN_SCRIPTS":""},"buildName":"projects/893582587169/locations/us-central1/builds/539681f6-8d64-426e-b876-e7462e9cc571","dockerRegistry":"ARTIFACT_REGISTRY","automaticUpdatePolicy":{},"buildServiceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"},{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-syncBigQuery","description":"A task-triggered function that gets called on BigQuery sync","sourceArchiveUrl":"gs://firebase-mod-sources-prod/a8a3e54cd84beb56c7eb0f2379f70adbd1427513f860a1f544893c92df16a047","httpsTrigger":{"url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-syncBigQuery","securityLevel":"SECURE_OPTIONAL"},"status":"ACTIVE","entryPoint":"syncBigQuery","timeout":"540s","availableMemoryMb":256,"serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","updateTime":"2025-04-24T18:58:16.790Z","versionId":"2","labels":{"firebase-extensions-ar":"enabled","goog-dm":"firebase-ext-firestore-bigquery-export","goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions","goog-firebase-ext-iid":"firestore-bigquery-export"},"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false"},"runtime":"nodejs20","ingressSettings":"ALLOW_ALL","buildId":"aa720a7e-0c24-4c7a-928f-7299737e09a5","buildEnvironmentVariables":{"GOOGLE_NODE_RUN_SCRIPTS":""},"buildName":"projects/893582587169/locations/us-central1/builds/aa720a7e-0c24-4c7a-928f-7299737e09a5","dockerRegistry":"ARTIFACT_REGISTRY","automaticUpdatePolicy":{},"buildServiceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"},{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-setupBigQuerySync","description":"Runs configuration for sycning with BigQuery","sourceArchiveUrl":"gs://firebase-mod-sources-prod/a8a3e54cd84beb56c7eb0f2379f70adbd1427513f860a1f544893c92df16a047","httpsTrigger":{"url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-setupBigQuerySync","securityLevel":"SECURE_OPTIONAL"},"status":"ACTIVE","entryPoint":"setupBigQuerySync","timeout":"540s","availableMemoryMb":256,"serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","updateTime":"2025-04-24T18:57:30.170Z","versionId":"2","labels":{"firebase-extensions-ar":"enabled","goog-dm":"firebase-ext-firestore-bigquery-export","goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions","goog-firebase-ext-iid":"firestore-bigquery-export"},"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false"},"runtime":"nodejs20","ingressSettings":"ALLOW_ALL","buildId":"087f1c66-719a-427e-9758-ba59ba92c9fc","buildEnvironmentVariables":{"GOOGLE_NODE_RUN_SCRIPTS":""},"buildName":"projects/893582587169/locations/us-central1/builds/087f1c66-719a-427e-9758-ba59ba92c9fc","dockerRegistry":"ARTIFACT_REGISTRY","automaticUpdatePolicy":{},"buildServiceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"}]}
[debug] [2025-04-30T19:03:25.194Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:25.194Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:25.195Z] >>> [apiv2][query] GET https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/-/functions filter=environment%3D%22GEN_2%22
[debug] [2025-04-30T19:03:26.464Z] <<< [apiv2][status] GET https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/-/functions 200
[debug] [2025-04-30T19:03:26.464Z] <<< [apiv2][body] GET https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/-/functions {"functions":[{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-fsexportbigquery","description":"Listens for document changes in your specified Cloud Firestore collection, then exports the changes into BigQuery.","buildConfig":{"build":"projects/893582587169/locations/us-central1/builds/59c9e8df-cfe2-4ef4-b917-55b66938b533","runtime":"nodejs22","entryPoint":"fsexportbigquery","source":{"storageSource":{"bucket":"gcf-v2-sources-893582587169-us-central1","object":"ext-firestore-bigquery-export-fsexportbigquery/function-source.zip","generation":"1745520941600164"}},"dockerRepository":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts","sourceProvenance":{"resolvedStorageSource":{"bucket":"gcf-v2-sources-893582587169-us-central1","object":"ext-firestore-bigquery-export-fsexportbigquery/function-source.zip","generation":"1745520941600164"}},"dockerRegistry":"ARTIFACT_REGISTRY","serviceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"},"serviceConfig":{"service":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/services/ext-firestore-bigquery-export-fsexportbigquery","timeoutSeconds":120,"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","FUNCTION_SIGNATURE_TYPE":"cloudevent","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false","LOG_EXECUTION_ID":"true"},"maxInstanceCount":6,"ingressSettings":"ALLOW_INTERNAL_ONLY","uri":"https://ext-firestore-bigquery-export-fsexportbigquery-hijxxwf3ia-uc.a.run.app","serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","availableMemory":"256Mi","allTrafficOnLatestRevision":true,"revision":"ext-firestore-bigquery-export-fsexportbigquery-00002-cuw","maxInstanceRequestConcurrency":1,"availableCpu":"0.1666"},"eventTrigger":{"trigger":"projects/bigqueryexampleproject-e4ef9/locations/nam5/triggers/ext-firestore-bigquery-export-fsexportbigquery-548607","triggerRegion":"nam5","eventType":"google.cloud.firestore.document.v1.written","eventFilters":[{"attribute":"database","value":"(default)"},{"attribute":"document","value":"users/{documentId}","operator":"match-path-pattern"}],"pubsubTopic":"projects/bigqueryexampleproject-e4ef9/topics/eventarc-nam5-ext-firestore-bigquery-export-fsexportbigquery-548607-885","serviceAccountEmail":"893582587169-compute@developer.gserviceaccount.com","retryPolicy":"RETRY_POLICY_DO_NOT_RETRY"},"state":"ACTIVE","updateTime":"2025-04-24T18:57:21.075796035Z","labels":{"goog-firebase-ext-iid":"firestore-bigquery-export","goog-dm":"firebase-ext-firestore-bigquery-export","goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions"},"environment":"GEN_2","url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-fsexportbigquery","createTime":"2025-04-24T18:53:41.744810406Z","satisfiesPzi":true}]}
[info] i  functions: ensuring required API run.googleapis.com is enabled... 
[debug] [2025-04-30T19:03:26.471Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:26.471Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API eventarc.googleapis.com is enabled... 
[debug] [2025-04-30T19:03:26.471Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:26.472Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API pubsub.googleapis.com is enabled... 
[debug] [2025-04-30T19:03:26.472Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:26.472Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API storage.googleapis.com is enabled... 
[debug] [2025-04-30T19:03:26.472Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:26.473Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:26.473Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com [none]
[debug] [2025-04-30T19:03:26.473Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:03:26.477Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com [none]
[debug] [2025-04-30T19:03:26.477Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:03:26.481Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com [none]
[debug] [2025-04-30T19:03:26.481Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:03:26.484Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com [none]
[debug] [2025-04-30T19:03:26.485Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:03:27.798Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com 200
[debug] [2025-04-30T19:03:27.799Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com [omitted]
[info] ✔  functions: required API storage.googleapis.com is enabled 
[debug] [2025-04-30T19:03:27.801Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com 200
[debug] [2025-04-30T19:03:27.801Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com [omitted]
[info] ✔  functions: required API run.googleapis.com is enabled 
[debug] [2025-04-30T19:03:27.802Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com 200
[debug] [2025-04-30T19:03:27.803Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com [omitted]
[info] ✔  functions: required API pubsub.googleapis.com is enabled 
[debug] [2025-04-30T19:03:27.999Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com 200
[debug] [2025-04-30T19:03:27.999Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com [omitted]
[info] ✔  functions: required API eventarc.googleapis.com is enabled 
[info] i  functions: generating the service identity for pubsub.googleapis.com... 
[debug] [2025-04-30T19:03:28.001Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:28.001Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: generating the service identity for eventarc.googleapis.com... 
[debug] [2025-04-30T19:03:28.001Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:28.001Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:28.001Z] >>> [apiv2][query] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/pubsub.googleapis.com:generateServiceIdentity [none]
[debug] [2025-04-30T19:03:28.003Z] >>> [apiv2][query] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/eventarc.googleapis.com:generateServiceIdentity [none]
[debug] [2025-04-30T19:03:28.537Z] <<< [apiv2][status] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/pubsub.googleapis.com:generateServiceIdentity 200
[debug] [2025-04-30T19:03:28.537Z] <<< [apiv2][body] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/pubsub.googleapis.com:generateServiceIdentity {"name":"operations/finished.DONE_OPERATION","done":true,"response":{"@type":"type.googleapis.com/google.api.serviceusage.v1beta1.ServiceIdentity","email":"service-893582587169@gcp-sa-pubsub.iam.gserviceaccount.com","uniqueId":"114949547525185127171"}}
[debug] [2025-04-30T19:03:29.062Z] <<< [apiv2][status] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/eventarc.googleapis.com:generateServiceIdentity 200
[debug] [2025-04-30T19:03:29.062Z] <<< [apiv2][body] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/eventarc.googleapis.com:generateServiceIdentity {"name":"operations/finished.DONE_OPERATION","done":true,"response":{"@type":"type.googleapis.com/google.api.serviceusage.v1beta1.ServiceIdentity","email":"service-893582587169@gcp-sa-eventarc.iam.gserviceaccount.com","uniqueId":"102092505972539947691"}}
[debug] [2025-04-30T19:03:29.066Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:29.066Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:29.066Z] >>> [apiv2][query] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 [none]
[debug] [2025-04-30T19:03:30.093Z] <<< [apiv2][status] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 200
[debug] [2025-04-30T19:03:30.093Z] <<< [apiv2][body] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 {"projectNumber":"893582587169","projectId":"bigqueryexampleproject-e4ef9","lifecycleState":"ACTIVE","name":"BigQueryExampleProject","labels":{"firebase":"enabled","firebase-core":"disabled"},"createTime":"2025-04-24T16:34:06.379745Z"}
[debug] [2025-04-30T19:03:30.095Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:30.095Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:30.096Z] >>> [apiv2][query] GET https://cloudbilling.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/billingInfo [none]
[debug] [2025-04-30T19:03:31.667Z] <<< [apiv2][status] GET https://cloudbilling.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/billingInfo 200
[debug] [2025-04-30T19:03:31.667Z] <<< [apiv2][body] GET https://cloudbilling.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/billingInfo {"name":"projects/bigqueryexampleproject-e4ef9/billingInfo","projectId":"bigqueryexampleproject-e4ef9","billingAccountName":"billingAccounts/019DD6-64C0E7-5ED023","billingEnabled":true}
[debug] [2025-04-30T19:03:31.669Z] [functions] found 6 new HTTP functions, testing setIamPolicy permission...
[debug] [2025-04-30T19:03:31.670Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:31.670Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:31.670Z] >>> [apiv2][query] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions [none]
[debug] [2025-04-30T19:03:31.670Z] >>> [apiv2][(partial)header] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:03:31.670Z] >>> [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.setIamPolicy"]}
[debug] [2025-04-30T19:03:31.991Z] <<< [apiv2][status] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions 200
[debug] [2025-04-30T19:03:31.992Z] <<< [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.setIamPolicy"]}
[debug] [2025-04-30T19:03:31.992Z] [functions] found setIamPolicy permission, proceeding with deploy
[debug] [2025-04-30T19:03:31.993Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:31.993Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:31.993Z] >>> [apiv2][query] POST https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions:generateUploadUrl [none]
[debug] [2025-04-30T19:03:33.223Z] <<< [apiv2][status] POST https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions:generateUploadUrl 200
[debug] [2025-04-30T19:03:33.224Z] <<< [apiv2][body] POST https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions:generateUploadUrl {"uploadUrl":"https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/b80c2597-d74a-4638-8bac-7360c6431cb4.zip?GoogleAccessId=service-893582587169@gcf-admin-robot.iam.gserviceaccount.com&Expires=1746041613&Signature=c2vmbkqwGHYcpa%2B%2FOhTpKr%2BidszRI%2BKilXRvjRHrYvZzo4kL1Hv9l24N5L73RFFNc%2FkzouiyfpQW5coTgtlO32dm1GU7qJBn8NLPyBFFZdnxXVzSTl4BZL7doP2JID5P%2BM5K5EuKVjTms6vrwp%2BGFPg72T8o8DpqGeFGWQvg6vR1XgmokPDu0TB3%2FbBlBMvxKYulNo1Ismhp4sBunGir6kiH7wQ6YoJHbChP7AUxsqwlzW%2B58vVC%2BtfcU3jCYBaMx9a0B%2F7LZ7ioQfxmoZDBnF28oESrnezCKaTGYWNWJ0nep1yFNnUcmo5rEiuLYBYuhbnC3t%2BSfx63gbEjlQyNJQ%3D%3D","storageSource":{"bucket":"gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com","object":"b80c2597-d74a-4638-8bac-7360c6431cb4.zip"}}
[debug] [2025-04-30T19:03:33.226Z] >>> [apiv2][query] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/b80c2597-d74a-4638-8bac-7360c6431cb4.zip GoogleAccessId=service-893582587169%40gcf-admin-robot.iam.gserviceaccount.com&Expires=1746041613&Signature=c2vmbkqwGHYcpa%2B%2FOhTpKr%2BidszRI%2BKilXRvjRHrYvZzo4kL1Hv9l24N5L73RFFNc%2FkzouiyfpQW5coTgtlO32dm1GU7qJBn8NLPyBFFZdnxXVzSTl4BZL7doP2JID5P%2BM5K5EuKVjTms6vrwp%2BGFPg72T8o8DpqGeFGWQvg6vR1XgmokPDu0TB3%2FbBlBMvxKYulNo1Ismhp4sBunGir6kiH7wQ6YoJHbChP7AUxsqwlzW%2B58vVC%2BtfcU3jCYBaMx9a0B%2F7LZ7ioQfxmoZDBnF28oESrnezCKaTGYWNWJ0nep1yFNnUcmo5rEiuLYBYuhbnC3t%2BSfx63gbEjlQyNJQ%3D%3D
[debug] [2025-04-30T19:03:33.227Z] >>> [apiv2][body] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/b80c2597-d74a-4638-8bac-7360c6431cb4.zip [stream]
[debug] [2025-04-30T19:03:34.370Z] <<< [apiv2][status] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/b80c2597-d74a-4638-8bac-7360c6431cb4.zip 200
[debug] [2025-04-30T19:03:34.370Z] <<< [apiv2][body] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/b80c2597-d74a-4638-8bac-7360c6431cb4.zip [omitted]
[info] ✔  functions: functions folder uploaded successfully 
[info] i  functions: creating Node.js 22 (2nd Gen) function createuser(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function deleteuser(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function listusers(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function processdata(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function showuser(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function updateuser(us-central1)... 
[debug] [2025-04-30T19:03:34.406Z] Total Function Deployment time: 7
[debug] [2025-04-30T19:03:34.406Z] 6 Functions Deployed
[debug] [2025-04-30T19:03:34.406Z] 6 Functions Errored
[debug] [2025-04-30T19:03:34.406Z] 0 Function Deployments Aborted
[debug] [2025-04-30T19:03:34.406Z] Average Function Deployment time: 2.1666666666666665
[info] 
[info] Functions deploy had errors with the following functions:
	createuser(us-central1)
	deleteuser(us-central1)
	listusers(us-central1)
	processdata(us-central1)
	showuser(us-central1)
	updateuser(us-central1)
[debug] [2025-04-30T19:03:34.867Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T19:03:34.867Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T19:03:34.867Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T19:03:34.867Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T19:03:34.867Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T19:03:34.867Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T19:03:34.869Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:34.869Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:03:34.869Z] >>> [apiv2][query] GET https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts [none]
[debug] [2025-04-30T19:03:36.414Z] <<< [apiv2][status] GET https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts 200
[debug] [2025-04-30T19:03:36.414Z] <<< [apiv2][body] GET https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts {"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts","format":"DOCKER","description":"This repository is created and used by Cloud Functions for storing function docker images.","labels":{"goog-managed-by":"cloudfunctions"},"createTime":"2025-04-24T18:45:25.809021Z","updateTime":"2025-04-30T18:44:12.443453Z","mode":"STANDARD_REPOSITORY","cleanupPolicies":{"firebase-functions-cleanup":{"id":"firebase-functions-cleanup","action":"DELETE","condition":{"tagState":"ANY","olderThan":"86400s"}}},"sizeBytes":"681973421","vulnerabilityScanningConfig":{"lastEnableTime":"2025-04-24T18:45:16.816360009Z","enablementState":"SCANNING_DISABLED","enablementStateReason":"API containerscanning.googleapis.com is not enabled."},"satisfiesPzi":true,"registryUri":"us-central1-docker.pkg.dev/bigqueryexampleproject-e4ef9/gcf-artifacts"}
[debug] [2025-04-30T19:03:36.415Z] Functions deploy failed.
[debug] [2025-04-30T19:03:36.415Z] {}
[debug] [2025-04-30T19:03:36.415Z] {}
[debug] [2025-04-30T19:03:36.416Z] {}
[debug] [2025-04-30T19:03:36.416Z] {}
[debug] [2025-04-30T19:03:36.416Z] {}
[debug] [2025-04-30T19:03:36.416Z] {}
[error] Error: There was an error deploying functions:
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[debug] [2025-04-30T19:08:25.227Z] ----------------------------------------------------------------------
[debug] [2025-04-30T19:08:25.230Z] Command:       /usr/local/bin/firebase /home/wellington/.cache/firebase/tools/lib/node_modules/firebase-tools/lib/bin/firebase deploy
[debug] [2025-04-30T19:08:25.230Z] CLI Version:   14.0.0
[debug] [2025-04-30T19:08:25.230Z] Platform:      linux
[debug] [2025-04-30T19:08:25.230Z] Node Version:  v20.18.2
[debug] [2025-04-30T19:08:25.230Z] Time:          Wed Apr 30 2025 16:08:25 GMT-0300 (GMT-03:00)
[debug] [2025-04-30T19:08:25.230Z] ----------------------------------------------------------------------
[debug] 
[debug] [2025-04-30T19:08:25.467Z] > command requires scopes: ["email","openid","https://www.googleapis.com/auth/cloudplatformprojects.readonly","https://www.googleapis.com/auth/firebase","https://www.googleapis.com/auth/cloud-platform"]
[debug] [2025-04-30T19:08:25.467Z] > authorizing via signed-in user (wellingtonsilva112000@gmail.com)
[debug] [2025-04-30T19:08:25.468Z] [iam] checking project bigqueryexampleproject-e4ef9 for permissions ["cloudfunctions.functions.create","cloudfunctions.functions.delete","cloudfunctions.functions.get","cloudfunctions.functions.list","cloudfunctions.functions.update","cloudfunctions.operations.get","firebase.projects.get"]
[debug] [2025-04-30T19:08:25.469Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:25.469Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:25.470Z] >>> [apiv2][query] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions [none]
[debug] [2025-04-30T19:08:25.470Z] >>> [apiv2][(partial)header] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:08:25.470Z] >>> [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.create","cloudfunctions.functions.delete","cloudfunctions.functions.get","cloudfunctions.functions.list","cloudfunctions.functions.update","cloudfunctions.operations.get","firebase.projects.get"]}
[debug] [2025-04-30T19:08:26.823Z] <<< [apiv2][status] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions 200
[debug] [2025-04-30T19:08:26.824Z] <<< [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.create","cloudfunctions.functions.delete","cloudfunctions.functions.get","cloudfunctions.functions.list","cloudfunctions.functions.update","cloudfunctions.operations.get","firebase.projects.get"]}
[debug] [2025-04-30T19:08:26.825Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:26.825Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:26.825Z] >>> [apiv2][query] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions [none]
[debug] [2025-04-30T19:08:26.825Z] >>> [apiv2][body] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions {"permissions":["iam.serviceAccounts.actAs"]}
[debug] [2025-04-30T19:08:28.140Z] <<< [apiv2][status] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions 404
[debug] [2025-04-30T19:08:28.141Z] <<< [apiv2][body] POST https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions {"error":{"code":404,"message":"Unknown service account","status":"NOT_FOUND"}}
[debug] [2025-04-30T19:08:28.142Z] [functions] service account IAM check errored, deploy may fail: Request to https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions had HTTP Error: 404, Unknown service account {"name":"FirebaseError","children":[],"context":{"body":{"error":{"code":404,"message":"Unknown service account","status":"NOT_FOUND"}},"response":{"statusCode":404}},"exit":1,"message":"Request to https://iam.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/serviceAccounts/bigqueryexampleproject-e4ef9@appspot.gserviceaccount.com:testIamPermissions had HTTP Error: 404, Unknown service account","status":404}
[info] 
[info] === Deploying to 'bigqueryexampleproject-e4ef9'...
[info] 
[info] i  deploying functions 
[debug] [2025-04-30T19:08:28.149Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:28.149Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:28.149Z] >>> [apiv2][query] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 [none]
[debug] [2025-04-30T19:08:28.551Z] <<< [apiv2][status] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 200
[debug] [2025-04-30T19:08:28.551Z] <<< [apiv2][body] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 {"projectNumber":"893582587169","projectId":"bigqueryexampleproject-e4ef9","lifecycleState":"ACTIVE","name":"BigQueryExampleProject","labels":{"firebase":"enabled","firebase-core":"disabled"},"createTime":"2025-04-24T16:34:06.379745Z"}
[info] i  functions: preparing codebase default for deployment 
[info] i  functions: ensuring required API cloudfunctions.googleapis.com is enabled... 
[debug] [2025-04-30T19:08:28.554Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:28.554Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:28.554Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:28.554Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API cloudbuild.googleapis.com is enabled... 
[debug] [2025-04-30T19:08:28.555Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:28.555Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  artifactregistry: ensuring required API artifactregistry.googleapis.com is enabled... 
[debug] [2025-04-30T19:08:28.556Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:28.556Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:28.556Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com [none]
[debug] [2025-04-30T19:08:28.557Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:08:28.559Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com [none]
[debug] [2025-04-30T19:08:28.560Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:08:28.564Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com [none]
[debug] [2025-04-30T19:08:28.564Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:08:28.567Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com [none]
[debug] [2025-04-30T19:08:28.567Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:08:30.041Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com 200
[debug] [2025-04-30T19:08:30.042Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/runtimeconfig.googleapis.com [omitted]
[debug] [2025-04-30T19:08:30.053Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com 200
[debug] [2025-04-30T19:08:30.054Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudbuild.googleapis.com [omitted]
[info] ✔  functions: required API cloudbuild.googleapis.com is enabled 
[debug] [2025-04-30T19:08:30.057Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com 200
[debug] [2025-04-30T19:08:30.057Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/artifactregistry.googleapis.com [omitted]
[info] ✔  artifactregistry: required API artifactregistry.googleapis.com is enabled 
[debug] [2025-04-30T19:08:30.061Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com 200
[debug] [2025-04-30T19:08:30.061Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/cloudfunctions.googleapis.com [omitted]
[info] ✔  functions: required API cloudfunctions.googleapis.com is enabled 
[debug] [2025-04-30T19:08:30.062Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:30.062Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:30.063Z] >>> [apiv2][query] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig [none]
[debug] [2025-04-30T19:08:30.807Z] <<< [apiv2][status] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig 200
[debug] [2025-04-30T19:08:30.807Z] <<< [apiv2][body] GET https://firebase.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/adminSdkConfig {"projectId":"bigqueryexampleproject-e4ef9","storageBucket":"bigqueryexampleproject-e4ef9.firebasestorage.app"}
[debug] [2025-04-30T19:08:30.808Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:30.808Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:30.809Z] >>> [apiv2][query] GET https://runtimeconfig.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/configs [none]
[debug] [2025-04-30T19:08:31.721Z] <<< [apiv2][status] GET https://runtimeconfig.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/configs 200
[debug] [2025-04-30T19:08:31.721Z] <<< [apiv2][body] GET https://runtimeconfig.googleapis.com/v1beta1/projects/bigqueryexampleproject-e4ef9/configs {}
[debug] [2025-04-30T19:08:31.722Z] Validating nodejs source
[debug] [2025-04-30T19:08:32.600Z] > [functions] package.json contents: {
  "name": "functions",
  "description": "Cloud Functions for Firebase",
  "scripts": {
    "serve": "firebase emulators:start --only functions",
    "shell": "firebase functions:shell",
    "start": "npm run shell",
    "deploy": "firebase deploy --only functions",
    "logs": "firebase functions:log"
  },
  "engines": {
    "node": "22"
  },
  "main": "index.mjs",
  "dependencies": {
    "firebase-admin": "^12.6.0",
    "firebase-functions": "^6.0.1",
    "functions": "file",
    "uuid": "^11.1.0"
  },
  "devDependencies": {
    "firebase-functions-test": "^3.1.0"
  },
  "private": true
}
[debug] [2025-04-30T19:08:32.600Z] Building nodejs source
[info] i  functions: Loading and analyzing source code for codebase default to determine what to deploy 
[debug] [2025-04-30T19:08:32.601Z] Could not find functions.yaml. Must use http discovery
[debug] [2025-04-30T19:08:32.607Z] Found firebase-functions binary at '/home/wellington/Documentos/Git/BigQueryExample/Functions-Firebase/functions/node_modules/.bin/firebase-functions'
[info] Serving at port 8011

[debug] [2025-04-30T19:08:33.377Z] Got response from /__/functions.yaml {"endpoints":{"createuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"createuser"},"deleteuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"deleteuser"},"listusers":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"listusers"},"processdata":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"processdata"},"showuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"showuser"},"updateuser":{"availableMemoryMb":null,"timeoutSeconds":null,"minInstances":null,"maxInstances":null,"ingressSettings":null,"concurrency":null,"serviceAccountEmail":null,"vpc":null,"platform":"gcfv2","labels":{},"httpsTrigger":{},"entryPoint":"updateuser"}},"specVersion":"v1alpha1","requiredAPIs":[],"extensions":{}}
[info] i  extensions: ensuring required API firebaseextensions.googleapis.com is enabled... 
[debug] [2025-04-30T19:08:37.452Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:37.453Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:37.453Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com [none]
[debug] [2025-04-30T19:08:37.453Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:08:38.692Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com 200
[debug] [2025-04-30T19:08:38.692Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/firebaseextensions.googleapis.com [omitted]
[info] ✔  extensions: required API firebaseextensions.googleapis.com is enabled 
[debug] [2025-04-30T19:08:38.692Z] > command requires scopes: ["email","openid","https://www.googleapis.com/auth/cloudplatformprojects.readonly","https://www.googleapis.com/auth/firebase","https://www.googleapis.com/auth/cloud-platform"]
[debug] [2025-04-30T19:08:38.693Z] > authorizing via signed-in user (wellingtonsilva112000@gmail.com)
[debug] [2025-04-30T19:08:38.693Z] [iam] checking project bigqueryexampleproject-e4ef9 for permissions ["firebase.projects.get","firebaseextensions.instances.list"]
[debug] [2025-04-30T19:08:38.693Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:38.693Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:38.694Z] >>> [apiv2][query] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions [none]
[debug] [2025-04-30T19:08:38.694Z] >>> [apiv2][(partial)header] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:08:38.694Z] >>> [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["firebase.projects.get","firebaseextensions.instances.list"]}
[debug] [2025-04-30T19:08:39.720Z] <<< [apiv2][status] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions 200
[debug] [2025-04-30T19:08:39.721Z] <<< [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["firebase.projects.get","firebaseextensions.instances.list"]}
[debug] [2025-04-30T19:08:39.722Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:39.722Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:39.723Z] >>> [apiv2][query] GET https://firebaseextensions.googleapis.com/v1beta/projects/bigqueryexampleproject-e4ef9/instances pageSize=100&pageToken=
[debug] [2025-04-30T19:08:41.452Z] <<< [apiv2][status] GET https://firebaseextensions.googleapis.com/v1beta/projects/bigqueryexampleproject-e4ef9/instances 200
[debug] [2025-04-30T19:08:41.454Z] <<< [apiv2][body] GET https://firebaseextensions.googleapis.com/v1beta/projects/bigqueryexampleproject-e4ef9/instances {"instances":[{"name":"projects/bigqueryexampleproject-e4ef9/instances/firestore-bigquery-export","createTime":"2025-04-24T18:43:47.251262Z","updateTime":"2025-04-24T18:58:40.150355Z","state":"ACTIVE","config":{"name":"projects/bigqueryexampleproject-e4ef9/instances/firestore-bigquery-export/configurations/2661e7fb-2095-49c8-9edb-f7aeeb3fe7d6","createTime":"2025-04-24T18:43:47.251262Z","source":{"name":"projects/firebaseextensions/sources/1bcd5ca9-fd21-4b21-9984-8d6ee3a0a7c8","packageUri":"https://storage.googleapis.com/firebase-extensions-packages-prod/firebase-firestore-bigquery-export-0.2.2-713b5ff6-19ab-459d-8d21-b6eec0e8d5f9.zip","hash":"5df1592f3afe939bcf4a63279f7de800aade1153433d37d17a65109cf854dfa5","extensionRoot":"/","spec":{"specVersion":"v1beta","name":"firestore-bigquery-export","version":"0.2.2","description":"Sends realtime, incremental updates from a specified Cloud Firestore collection to BigQuery.","apis":[{"apiName":"bigquery.googleapis.com","reason":"Mirrors data from your Cloud Firestore collection in BigQuery."}],"roles":[{"role":"bigquery.dataEditor","reason":"Allows the extension to configure and export data into BigQuery."},{"role":"datastore.user","reason":"Allows the extension to write updates to the database."},{"role":"bigquery.user","reason":"Allows the extension to create and manage BigQuery materialized views."}],"resources":[{"name":"fsexportbigquery","type":"firebaseextensions.v1beta.v2function","propertiesYaml":"buildConfig:\n  runtime: nodejs22\neventTrigger:\n  eventFilters:\n  - attribute: database\n    value: ${DATABASE}\n  - attribute: document\n    operator: match-path-pattern\n    value: ${COLLECTION_PATH}/{documentId}\n  eventType: google.cloud.firestore.document.v1.written\n  triggerRegion: ${DATABASE_REGION}\nsourceDirectory: functions\n","description":"Listens for document changes in your specified Cloud Firestore collection, then exports the changes into BigQuery.","deletionPolicy":"DELETE"},{"name":"syncBigQuery","type":"firebaseextensions.v1beta.function","propertiesYaml":"runtime: nodejs20\ntaskQueueTrigger:\n  rateLimits:\n    maxConcurrentDispatches: 500\n    maxDispatchesPerSecond: ${param:MAX_DISPATCHES_PER_SECOND}\n  retryConfig:\n    maxAttempts: 5\n    minBackoffSeconds: 60\n","description":"A task-triggered function that gets called on BigQuery sync","deletionPolicy":"DELETE"},{"name":"initBigQuerySync","type":"firebaseextensions.v1beta.function","propertiesYaml":"runtime: nodejs20\ntaskQueueTrigger:\n  retryConfig:\n    maxAttempts: 15\n    minBackoffSeconds: 60\n","description":"Runs configuration for sycning with BigQuery","deletionPolicy":"DELETE"},{"name":"setupBigQuerySync","type":"firebaseextensions.v1beta.function","propertiesYaml":"runtime: nodejs20\ntaskQueueTrigger:\n  retryConfig:\n    maxAttempts: 15\n    minBackoffSeconds: 60\n","description":"Runs configuration for sycning with BigQuery","deletionPolicy":"DELETE"}],"billingRequired":true,"author":{"authorName":"Firebase","url":"https://firebase.google.com"},"contributors":[{"authorName":"Jan Wyszynski","email":"wyszynski@google.com","url":"https://github.com/IanWyszynski"}],"license":"Apache-2.0","releaseNotesUrl":"https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/CHANGELOG.md","sourceUrl":"https://github.com/firebase/extensions/tree/master/firestore-bigquery-export","params":[{"param":"DATASET_LOCATION","label":"BigQuery Dataset location","type":"SELECT","description":"Where do you want to deploy the BigQuery dataset created for this extension? For help selecting a location, refer to the [location selection guide](https://cloud.google.com/bigquery/docs/locations).","required":true,"options":[{"value":"us-central1","label":"Iowa (us-central1)"},{"value":"us-west4","label":"Las Vegas (us-west4)"},{"value":"europe-central2","label":"Warsaw (europe-central2)"},{"value":"us-west2","label":"Los Angeles (us-west2)"},{"value":"northamerica-northeast1","label":"Montreal (northamerica-northeast1)"},{"value":"us-east4","label":"Northern Virginia (us-east4)"},{"value":"us-west1","label":"Oregon (us-west1)"},{"value":"us-west3","label":"Salt Lake City (us-west3)"},{"value":"southamerica-east1","label":"Sao Paulo (southamerica-east1)"},{"value":"us-east1","label":"South Carolina (us-east1)"},{"value":"europe-west1","label":"Belgium (europe-west1)"},{"value":"europe-north1","label":"Finland (europe-north1)"},{"value":"europe-west3","label":"Frankfurt (europe-west3)"},{"value":"europe-west2","label":"London (europe-west2)"},{"value":"europe-west4","label":"Netherlands (europe-west4)"},{"value":"europe-west6","label":"Zurich (europe-west6)"},{"value":"asia-east1","label":"Taiwan (asia-east1)"},{"value":"asia-east2","label":"Hong Kong (asia-east2)"},{"value":"asia-southeast2","label":"Jakarta (asia-southeast2)"},{"value":"asia-south1","label":"Mumbai (asia-south1)"},{"value":"asia-southeast1","label":"Singapore (asia-southeast1)"},{"value":"asia-northeast2","label":"Osaka (asia-northeast2)"},{"value":"asia-northeast3","label":"Seoul (asia-northeast3)"},{"value":"asia-southeast1","label":"Singapore (asia-southeast1)"},{"value":"australia-southeast1","label":"Sydney (australia-southeast1)"},{"value":"asia-east1","label":"Taiwan (asia-east1)"},{"value":"asia-northeast1","label":"Tokyo (asia-northeast1)"},{"value":"us","label":"United States (multi-regional)"},{"value":"eu","label":"Europe (multi-regional)"}],"default":"us","immutable":true},{"param":"BIGQUERY_PROJECT_ID","label":"BigQuery Project ID","type":"STRING","description":"Override the default project for BigQuery instance. This can allow updates to be directed to to a BigQuery instance on another GCP project.","required":true,"default":"${PROJECT_ID}"},{"param":"DATABASE","label":"Firestore Instance ID","type":"STRING","description":"The Firestore database to use. Use \"(default)\" for the default database. You can view your available Firestore databases at https://console.cloud.google.com/firestore/databases.\n","required":true,"default":"(default)","example":"(default)"},{"param":"DATABASE_REGION","label":"Firestore Instance Location","type":"SELECT","description":"Where is the Firestore database located? You can check your current database location at https://console.cloud.google.com/firestore/databases.\n","required":true,"options":[{"value":"eur3","label":"Multi-region (Europe - Belgium and Netherlands)"},{"value":"nam5","label":"Multi-region (United States)"},{"value":"nam7","label":"Multi-region (Iowa, North Virginia, and Oklahoma)"},{"value":"us-central1","label":"Iowa (us-central1)"},{"value":"us-west1","label":"Oregon (us-west1)"},{"value":"us-west2","label":"Los Angeles (us-west2)"},{"value":"us-west3","label":"Salt Lake City (us-west3)"},{"value":"us-west4","label":"Las Vegas (us-west4)"},{"value":"us-east1","label":"South Carolina (us-east1)"},{"value":"us-east4","label":"Northern Virginia (us-east4)"},{"value":"us-east5","label":"Columbus (us-east5)"},{"value":"us-south1","label":"Dallas (us-south1)"},{"value":"northamerica-northeast1","label":"Montreal (northamerica-northeast1)"},{"value":"northamerica-northeast2","label":"Toronto (northamerica-northeast2)"},{"value":"northamerica-south1","label":"Queretaro (northamerica-south1)"},{"value":"southamerica-east1","label":"Sao Paulo (southamerica-east1)"},{"value":"southamerica-west1","label":"Santiago (southamerica-west1)"},{"value":"europe-west1","label":"Belgium (europe-west1)"},{"value":"europe-west2","label":"London (europe-west2)"},{"value":"europe-west3","label":"Frankfurt (europe-west3)"},{"value":"europe-west4","label":"Netherlands (europe-west4)"},{"value":"europe-west6","label":"Zurich (europe-west6)"},{"value":"europe-west8","label":"Milan (europe-west8)"},{"value":"europe-west9","label":"Paris (europe-west9)"},{"value":"europe-west10","label":"Berlin (europe-west10)"},{"value":"europe-west12","label":"Turin (europe-west12)"},{"value":"europe-southwest1","label":"Madrid (europe-southwest1)"},{"value":"europe-north1","label":"Finland (europe-north1)"},{"value":"europe-north2","label":"Stockholm (europe-north2)"},{"value":"europe-central2","label":"Warsaw (europe-central2)"},{"value":"me-central1","label":"Doha (me-central1)"},{"value":"me-central2","label":"Dammam (me-central2)"},{"value":"me-west1","label":"Tel Aviv (me-west1)"},{"value":"asia-south1","label":"Mumbai (asia-south1)"},{"value":"asia-south2","label":"Delhi (asia-south2)"},{"value":"asia-southeast1","label":"Singapore (asia-southeast1)"},{"value":"asia-southeast2","label":"Jakarta (asia-southeast2)"},{"value":"asia-east1","label":"Taiwan (asia-east1)"},{"value":"asia-east2","label":"Hong Kong (asia-east2)"},{"value":"asia-northeast1","label":"Tokyo (asia-northeast1)"},{"value":"asia-northeast2","label":"Osaka (asia-northeast2)"},{"value":"asia-northeast3","label":"Seoul (asia-northeast3)"},{"value":"australia-southeast1","label":"Sydney (australia-southeast1)"},{"value":"australia-southeast2","label":"Melbourne (australia-southeast2)"},{"value":"africa-south1","label":"Johannesburg (africa-south1)"}]},{"param":"COLLECTION_PATH","label":"Collection path","type":"STRING","description":"What is the path of the collection that you would like to export? You may use `{wildcard}` notation to match a subcollection of all documents in a collection (for example: `chatrooms/{chatid}/posts`). Parent Firestore Document IDs from `{wildcards}` can be returned in `path_params` as a JSON formatted string.","required":true,"default":"posts","example":"posts","validationRegex":"^[^/]+(/[^/]+/[^/]+)*$","validationErrorMessage":"Firestore collection paths must be an odd number of segments separated by slashes, e.g. \"path/to/collection\"."},{"param":"WILDCARD_IDS","label":"Enable Wildcard Column field with Parent Firestore Document IDs","type":"SELECT","description":"If enabled, creates a column containing a JSON object of all wildcard ids from a documents path.","options":[{"value":"false","label":"No"},{"value":"true","label":"Yes"}],"default":"false"},{"param":"DATASET_ID","label":"Dataset ID","type":"STRING","description":"What ID would you like to use for your BigQuery dataset? This extension will create the dataset, if it doesn't already exist.","required":true,"default":"firestore_export","example":"firestore_export","validationRegex":"^[a-zA-Z0-9_]+$","validationErrorMessage":"BigQuery dataset IDs must be alphanumeric (plus underscores) and must be no more than 1024 characters.\n"},{"param":"TABLE_ID","label":"Table ID","type":"STRING","description":"What identifying prefix would you like to use for your table and view inside your BigQuery dataset? This extension will create the table and view, if they don't already exist.","required":true,"default":"posts","example":"posts","validationRegex":"^[a-zA-Z0-9_]+$","validationErrorMessage":"BigQuery table IDs must be alphanumeric (plus underscores) and must be no more than 1024 characters.\n"},{"param":"TABLE_PARTITIONING","label":"BigQuery SQL table Time Partitioning option type","type":"SELECT","description":"This parameter will allow you to partition the BigQuery table and BigQuery view created by the extension based on data ingestion time. You may select the granularity of partitioning based upon one of: HOUR, DAY, MONTH, YEAR. This will generate one partition per day, hour, month or year, respectively.","options":[{"value":"HOUR","label":"hour"},{"value":"DAY","label":"day"},{"value":"MONTH","label":"month"},{"value":"YEAR","label":"year"},{"value":"NONE","label":"none"}],"default":"NONE"},{"param":"TIME_PARTITIONING_FIELD","label":"BigQuery Time Partitioning column name","type":"STRING","description":"BigQuery table column/schema field name for TimePartitioning. You can choose schema available as `timestamp` OR a new custom defined column that will be assigned to the selected Firestore Document field below. Defaults to pseudo column _PARTITIONTIME if unspecified. Cannot be changed if Table is already partitioned."},{"param":"TIME_PARTITIONING_FIRESTORE_FIELD","label":"Firestore Document field name for BigQuery SQL Time Partitioning field option","type":"STRING","description":"This parameter will allow you to partition the BigQuery table created by the extension based on selected. The Firestore Document field value must be a top-level TIMESTAMP, DATETIME, DATE field BigQuery string format or Firestore timestamp(will be converted to BigQuery TIMESTAMP). Cannot be changed if Table is already partitioned.\n example: `postDate`(Ensure that the Firestore-BigQuery export extension\ncreates the dataset and table before initiating any backfill scripts.\n This step is crucial for the partitioning to function correctly. It is\nessential for the script to insert data into an already partitioned table.)"},{"param":"TIME_PARTITIONING_FIELD_TYPE","label":"BigQuery SQL Time Partitioning table schema field(column) type","type":"SELECT","description":"Parameter for BigQuery SQL schema field type for the selected Time Partitioning Firestore Document field option. Cannot be changed if Table is already partitioned.","options":[{"value":"TIMESTAMP","label":"TIMESTAMP"},{"value":"DATETIME","label":"DATETIME"},{"value":"DATE","label":"DATE"},{"value":"omit","label":"omit"}],"default":"omit"},{"param":"CLUSTERING","label":"BigQuery SQL table clustering","type":"STRING","description":"This parameter allows you to set up clustering for the BigQuery table created by the extension. Specify up to 4 comma-separated fields (for example:  `data,document_id,timestamp` - no whitespaces). The order of the specified  columns determines the sort order of the data. \nnote: Cluster columns must be top-level, non-repeated columns of one of the  following types: BIGNUMERIC, BOOL, DATE, DATETIME, GEOGRAPHY, INT64, NUMERIC,  RANGE, STRING, TIMESTAMP. Clustering will not be added if a field with an invalid type is present in this parameter.\nAvailable schema extensions table fields for clustering include: `document_id, document_name, timestamp, event_id,  operation, data`.","example":"data,document_id,timestamp","validationRegex":"^[^,\\s]+(?:,[^,\\s]+){0,3}$","validationErrorMessage":"No whitespaces. Max 4 fields. e.g. `data,timestamp,event_id,operation`"},{"param":"MAX_DISPATCHES_PER_SECOND","label":"Maximum number of synced documents per second","type":"STRING","description":"This parameter will set the maximum number of syncronised documents per second with BQ. Please note, any other external updates to a Big Query table will be included within this quota. Ensure that you have a set a low enough number to compensate. Defaults to 100.","default":"100","validationRegex":"^([1-9]|[1-9][0-9]|[1-4][0-9]{2}|500)$","validationErrorMessage":"Please select a number between 1 and 500"},{"param":"VIEW_TYPE","label":"View Type","type":"SELECT","description":"Select the type of view to create in BigQuery. A regular view is a virtual table defined by a SQL query.  A materialized view persists the results of a query for faster access, with either incremental or  non-incremental updates. Please note that materialized views in this extension come with several  important caveats and limitations - carefully review the pre-install documentation before selecting  these options to ensure they are appropriate for your use case.","required":true,"options":[{"value":"view","label":"View"},{"value":"materialized_incremental","label":"Materialized View (Incremental)"},{"value":"materialized_non_incremental","label":"Materialized View (Non-incremental)"}],"default":"view"},{"param":"MAX_STALENESS","label":"Maximum Staleness Duration","type":"STRING","description":"For materialized views only: Specifies the maximum staleness acceptable for the materialized view.  Should be specified as an INTERVAL value following BigQuery SQL syntax.  This parameter will only take effect if View Type is set to a materialized view option.","example":"INTERVAL \"8:0:0\" HOUR TO SECOND"},{"param":"REFRESH_INTERVAL_MINUTES","label":"Refresh Interval (Minutes)","type":"STRING","description":"For materialized views only: Specifies how often the materialized view should be refreshed, in minutes.  This parameter will only take effect if View Type is set to a materialized view option.","example":"60","validationRegex":"^[1-9][0-9]*$","validationErrorMessage":"Must be a positive integer"},{"param":"BACKUP_COLLECTION","label":"Backup Collection Name","type":"STRING","description":"This (optional) parameter will allow you to specify a collection for which failed BigQuery updates will be written to."},{"param":"TRANSFORM_FUNCTION","label":"Transform function URL","type":"STRING","description":"Specify a function URL to call that will transform the payload that will be written to BigQuery. See the pre-install documentation for more details.","example":"https://us-west1-my-project-id.cloudfunctions.net/myTransformFunction"},{"param":"USE_NEW_SNAPSHOT_QUERY_SYNTAX","label":"Use new query syntax for snapshots","type":"SELECT","description":"If enabled, snapshots will be generated with the new query syntax, which should be more performant, and avoid potential resource limitations.","required":true,"options":[{"value":"yes","label":"Yes"},{"value":"no","label":"No"}],"default":"no"},{"param":"EXCLUDE_OLD_DATA","label":"Exclude old data payloads","type":"SELECT","description":"If enabled, table rows will never contain old data (document snapshot before the Firestore onDocumentUpdate event: `change.before.data()`). The reduction in data should be more performant, and avoid potential resource limitations.","options":[{"value":"yes","label":"Yes"},{"value":"no","label":"No"}],"default":"no"},{"param":"KMS_KEY_NAME","label":"Cloud KMS key name","type":"STRING","description":"Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS. If this parameter is set, the extension will specify the KMS key name when creating the BQ table. See the PREINSTALL.md for more details.","validationRegex":"projects/([^/]+)/locations/([^/]+)/keyRings/([^/]+)/cryptoKeys/([^/]+)","validationErrorMessage":"The key name must be of the format 'projects/PROJECT_NAME/locations/KEY_RING_LOCATION/keyRings/KEY_RING_ID/cryptoKeys/KEY_ID'."},{"param":"MAX_ENQUEUE_ATTEMPTS","label":"Maximum number of enqueue attempts","type":"STRING","description":"This parameter will set the maximum number of attempts to enqueue a document to cloud tasks for export to BigQuery.","default":"3","validationRegex":"^(10|[1-9])$","validationErrorMessage":"Please select an integer between 1 and 10"},{"param":"LOG_LEVEL","label":"Log level","type":"SELECT","description":"The log level for the extension. The log level controls the verbosity of the extension's logs. The available log levels are: debug, info, warn, and error. To reduce the volume of logs, use a log level of warn or error.","required":true,"options":[{"value":"debug","label":"Debug"},{"value":"info","label":"Info"},{"value":"warn","label":"Warn"},{"value":"error","label":"Error"},{"value":"silent","label":"Silent"}],"default":"info"}],"preinstallContent":"Use this extension to export the documents in a Cloud Firestore collection to BigQuery. Exports are realtime and incremental, so the data in BigQuery is a mirror of your content in Cloud Firestore.\n\nThe extension creates and updates a [dataset](https://cloud.google.com/bigquery/docs/datasets-intro) containing the following two BigQuery resources:\n\n- A [table](https://cloud.google.com/bigquery/docs/tables-intro) of raw data that stores a full change history of the documents within your collection. This table includes a number of metadata fields so that BigQuery can display the current state of your data. The principle metadata fields are `timestamp`, `document_name`, and the `operation` for the document change.\n- A [view](https://cloud.google.com/bigquery/docs/views-intro) which represents the current state of the data within your collection. It also shows a log of the latest `operation` for each document (`CREATE`, `UPDATE`, or `IMPORT`).\n\n*Warning*: A BigQuery table corresponding to your configuration will be automatically generated upon installing or updating this extension. Manual table creation may result in discrepancies with your configured settings.\n\nIf you create, update, or delete a document in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`). \n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in ${param:COLLECTION_PATH}. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\nNote: To enable partitioning for a Big Query database, the following fields are required:\n\n - Time Partitioning option type\n - Time partitioning column name\n - Time partiitioning table schema\n - Firestore document field name\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n\n\n#### Additional setup\n\nBefore installing this extension, you'll need to:\n\n- [Set up Cloud Firestore in your Firebase project.](https://firebase.google.com/docs/firestore/quickstart)\n- [Link your Firebase project to BigQuery.](https://support.google.com/firebase/answer/6318765)\n\n\n#### Import existing documents\n\nTo import existing documents you can run the external [import script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n**Important:** Run the external import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nWithout use of this import script, the extension only exports the content of documents that are created or changed after installation.\n\n#### Transform function\n\nPrior to sending the document change to BigQuery, you have an opportunity to transform the data with an HTTP function. The payload will contain the following:\n\n```\n{ \n  data: [{\n    insertId: int;\n    json: {\n      timestamp: int;\n      event_id: int;\n      document_name: string;\n      document_id: int;\n      operation: ChangeType;\n      data: string;\n    },\n  }]\n}\n```\n\nThe response should be indentical in structure.\n\n#### Materialized Views\n\nThis extension supports both regular views and materialized views in BigQuery. While regular views compute their results each time they're queried, materialized views store their query results, providing faster access at the cost of additional storage.\n\nThere are two types of materialized views available:\n\n1. **Non-incremental Materialized Views**: These views support more complex queries including filtering on aggregated fields, but require complete recomputation during refresh.\n\n2. **Incremental Materialized Views**: These views update more efficiently by processing only new or changed records, but come with query restrictions. Most notably, they don't allow filtering or partitioning on aggregated fields in their defining SQL, among other limitations.\n\n**Important Considerations:**\n- Neither type of materialized view in this extension currently supports partitioning or clustering\n- Both types allow you to configure refresh intervals and maximum staleness settings during extension installation or configuration\n- Once created, a materialized view's SQL definition cannot be modified. If you reconfigure the extension to change either the view type (incremental vs non-incremental) or the SQL query, the extension will drop the existing materialized view and recreate it\n- Carefully consider your use case before choosing materialized views:\n  - They incur additional storage costs as they cache query results\n  - Non-incremental views may have higher processing costs during refresh\n  - Incremental views have more query restrictions but are more efficient to update\n\nExample of a non-incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    allow_non_incremental_definition = true,\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\n  AS (\n    WITH latests AS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY document_name\n    )\n    SELECT *\n    FROM latests\n    WHERE operation != \"DELETE\"\n  )\n```\n\nExample of an incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\nAS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM\n        `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY\n        document_name\n    )\n```\n\nPlease review [BigQuery's documentation on materialized views](https://cloud.google.com/bigquery/docs/materialized-views-intro) to fully understand the implications for your use case.\n\n#### Using Customer Managed Encryption Keys\n\nBy default, BigQuery encrypts your content stored at rest. BigQuery handles and manages this default encryption for you without any additional actions on your part.\n\nIf you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery. Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS.\n\nFor more general information on this, see [the docs](https://cloud.google.com/bigquery/docs/customer-managed-encryption).\n\nTo use CMEK and the Key Management Service (KMS) with this extension\n1. [Enable the KMS API in your Google Cloud Project](https://console.cloud.google.com/apis/enableflow?apiid=cloudkms.googleapis.com).\n2. Create a keyring and keychain in the KMS. Note that the region of the keyring and key *must* match the region of your bigquery dataset\n3. Grant the BigQuery service account permission to encrypt and decrypt using that key. The Cloud KMS CryptoKey Encrypter/Decrypter role grants this permission. First find your project number. You can find this for example on the cloud console dashboard `https://console.cloud.google.com/home/dashboard?project={PROJECT_ID}`. The service account which needs the Encrypter/Decrypter role is then `bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com`. You can grant this role through the credentials service in the console, or through the CLI:\n```\ngcloud kms keys add-iam-policy-binding \\\n--project=KMS_PROJECT_ID \\\n--member serviceAccount:bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com \\\n--role roles/cloudkms.cryptoKeyEncrypterDecrypter \\\n--location=KMS_KEY_LOCATION \\\n--keyring=KMS_KEY_RING \\\nKMS_KEY\n```\n4. When installing this extension, enter the resource name of your key. It will look something like the following:\n```\nprojects/<YOUR PROJECT ID>/locations/<YOUR REGION>/keyRings/<YOUR KEY RING NAME>/cryptoKeys/<YOUR KEY NAME>\n```\nIf you follow these steps, your changelog table should be created using your customer-managed encryption.\n\n#### Generate schema views\n\nAfter your data is in BigQuery, you can run the [schema-views script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md) (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter to your target BigQuery project ID.\n\n2. After installation, you'll need to grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n#### Mitigating Data Loss During Extension Updates\n\nWhen updating or reconfiguring this extension, there may be a brief period where data streaming from Firestore to BigQuery is interrupted. While this limitation exists within the Extensions platform, we provide two strategies to mitigate potential data loss.\n\n##### Strategy 1: Post-Update Import\nAfter reconfiguring the extension, run the import script on your collection to ensure all data is captured. Refer to the \"Import Existing Documents\" section above for detailed steps.\n\n##### Strategy 2: Parallel Instance Method\n1. Install a second instance of the extension that streams to a new BigQuery table\n2. Reconfigure the original extension\n3. Once the original extension is properly configured and streaming events\n4. Uninstall the second instance\n5. Run a BigQuery merge job to combine the data from both tables\n\n##### Considerations\n- Strategy 1 is simpler but may result in duplicate records that need to be deduplicated\n- Strategy 2 requires more setup but provides better data continuity\n- Choose the strategy that best aligns with your data consistency requirements and operational constraints\n\n#### Billing\nTo install an extension, your project must be on the [Blaze (pay as you go) plan](https://firebase.google.com/pricing)\n\n- This extension uses other Firebase and Google Cloud Platform services, which have associated charges if you exceed the service’s no-cost tier:\n  - BigQuery (this extension writes to BigQuery with [streaming inserts](https://cloud.google.com/bigquery/pricing#streaming_pricing))\n  - Cloud Firestore\n  - Cloud Functions (Node.js 10+ runtime. [See FAQs](https://firebase.google.com/support/faq#extensions-pricing))","postinstallContent":"### See it in action\n\nYou can test out this extension right away!\n\n1.  Go to your [Cloud Firestore dashboard](https://console.firebase.google.com/project/${param:BIGQUERY_PROJECT_ID}/firestore/data) in the Firebase console.\n\n2.  If it doesn't already exist, create the collection you specified during installation: `${param:COLLECTION_PATH}`\n\n3.  Create a document in the collection called `bigquery-mirror-test` that contains any fields with any values that you'd like.\n\n4.  Go to the [BigQuery web UI](https://console.cloud.google.com/bigquery?project=${param:BIGQUERY_PROJECT_ID}&p=${param:BIGQUERY_PROJECT_ID}&d=${param:DATASET_ID}) in the Google Cloud Platform console.\n\n5.  Query your **raw changelog table**, which should contain a single log of creating the `bigquery-mirror-test` document.\n\n    ```\n    SELECT *\n    FROM `${param:BIGQUERY_PROJECT_ID}.${param:DATASET_ID}.${param:TABLE_ID}_raw_changelog`\n    ```\n\n6.  Query your **latest view**, which should return the latest change event for the only document present -- `bigquery-mirror-test`.\n\n    ```\n    SELECT *\n    FROM `${param:BIGQUERY_PROJECT_ID}.${param:DATASET_ID}.${param:TABLE_ID}_raw_latest`\n    ```\n\n7.  Delete the `bigquery-mirror-test` document from [Cloud Firestore](https://console.firebase.google.com/project/${param:BIGQUERY_PROJECT_ID}/firestore/data).\n    The `bigquery-mirror-test` document will disappear from the **latest view** and a `DELETE` event will be added to the **raw changelog table**.\n\n8.  You can check the changelogs of a single document with this query:\n\n    ```\n    SELECT *\n    FROM `${param:BIGQUERY_PROJECT_ID}.${param:DATASET_ID}.${param:TABLE_ID}_raw_changelog`\n    WHERE document_name = \"bigquery-mirror-test\"\n    ORDER BY TIMESTAMP ASC\n    ```\n\n### Using the extension\n\nWhenever a document is created, updated, imported, or deleted in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset which contains the following resources:\n\n- **raw changelog table:** [`${param:TABLE_ID}_raw_changelog`](https://console.cloud.google.com/bigquery?project=${param:BIGQUERY_PROJECT_ID}&p=${param:BIGQUERY_PROJECT_ID}&d=${param:DATASET_ID}&t=${param:TABLE_ID}_raw_changelog&page=table)\n- **latest view:** [`${param:TABLE_ID}_raw_latest`](https://console.cloud.google.com/bigquery?project=${param:BIGQUERY_PROJECT_ID}&p=${param:BIGQUERY_PROJECT_ID}&d=${param:DATASET_ID}&t=${param:TABLE_ID}_raw_latest&page=table)\n\nTo review the schema for these two resources, click the **Schema** tab for each resource in BigQuery.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`).\n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in ${param:COLLECTION_PATH}. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter as your target BigQuery project ID.\n\n2. Identify the service account on the source project associated with the extension. By default, it will be constructed as `ext-<extension-instance-id>@project-id.iam.gserviceaccount.com`. However, if the extension instance ID is too long, it may be truncated and 4 random characters appended to abide by service account length limits.\n\n3. To find the exact service account, navigate to IAM & Admin -> IAM in the Google Cloud Platform Console. Look for the service account listed with \"Name\" as \"Firebase Extensions <your extension instance ID> service account\". The value in the \"Principal\" column will be the service account that needs permissions granted in the target project.\n\n4. Grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID] [-s SERVICE_ACCOUNT]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID] [-ServiceAccount SERVICE_ACCOUNT]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-s`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-ServiceAccount`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n### _(Optional)_ Import existing documents\n\nYou can backfill your BigQuery dataset with all the documents in your collection using the import script.\n\nIf you don't run the import script, the extension only exports the content of documents that are created or changed after installation.\n\nThe import script can read all existing documents in a Cloud Firestore collection and insert them into the raw changelog table created by this extension. The script adds a special changelog for each document with the operation of `IMPORT` and the timestamp of epoch. This is to ensure that any operation on an imported document supersedes the `IMPORT`.\n\n**Important:** Run the import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nLearn more about using the import script to [backfill your existing collection](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n### _(Optional)_ Generate schema views\n\nAfter your data is in BigQuery, you can use the schema-views script (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\nLearn more about using the schema-views script to [generate schema views](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md).\n\n### Monitoring\n\nAs a best practice, you can [monitor the activity](https://firebase.google.com/docs/extensions/manage-installed-extensions#monitor) of your installed extension, including checks on its health, usage, and logs.\n","readmeContent":"# Stream Firestore to BigQuery\n\n**Author**: Firebase (**[https://firebase.google.com](https://firebase.google.com)**)\n\n**Description**: Sends realtime, incremental updates from a specified Cloud Firestore collection to BigQuery.\n\n\n\n**Details**: Use this extension to export the documents in a Cloud Firestore collection to BigQuery. Exports are realtime and incremental, so the data in BigQuery is a mirror of your content in Cloud Firestore.\n\nThe extension creates and updates a [dataset](https://cloud.google.com/bigquery/docs/datasets-intro) containing the following two BigQuery resources:\n\n- A [table](https://cloud.google.com/bigquery/docs/tables-intro) of raw data that stores a full change history of the documents within your collection. This table includes a number of metadata fields so that BigQuery can display the current state of your data. The principle metadata fields are `timestamp`, `document_name`, and the `operation` for the document change.\n- A [view](https://cloud.google.com/bigquery/docs/views-intro) which represents the current state of the data within your collection. It also shows a log of the latest `operation` for each document (`CREATE`, `UPDATE`, or `IMPORT`).\n\n*Warning*: A BigQuery table corresponding to your configuration will be automatically generated upon installing or updating this extension. Manual table creation may result in discrepancies with your configured settings.\n\nIf you create, update, or delete a document in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`). \n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in ${param:COLLECTION_PATH}. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\nNote: To enable partitioning for a Big Query database, the following fields are required:\n\n - Time Partitioning option type\n - Time partitioning column name\n - Time partiitioning table schema\n - Firestore document field name\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n\n\n#### Additional setup\n\nBefore installing this extension, you'll need to:\n\n- [Set up Cloud Firestore in your Firebase project.](https://firebase.google.com/docs/firestore/quickstart)\n- [Link your Firebase project to BigQuery.](https://support.google.com/firebase/answer/6318765)\n\n\n#### Import existing documents\n\nTo import existing documents you can run the external [import script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n**Important:** Run the external import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nWithout use of this import script, the extension only exports the content of documents that are created or changed after installation.\n\n#### Transform function\n\nPrior to sending the document change to BigQuery, you have an opportunity to transform the data with an HTTP function. The payload will contain the following:\n\n```\n{ \n  data: [{\n    insertId: int;\n    json: {\n      timestamp: int;\n      event_id: int;\n      document_name: string;\n      document_id: int;\n      operation: ChangeType;\n      data: string;\n    },\n  }]\n}\n```\n\nThe response should be indentical in structure.\n\n#### Materialized Views\n\nThis extension supports both regular views and materialized views in BigQuery. While regular views compute their results each time they're queried, materialized views store their query results, providing faster access at the cost of additional storage.\n\nThere are two types of materialized views available:\n\n1. **Non-incremental Materialized Views**: These views support more complex queries including filtering on aggregated fields, but require complete recomputation during refresh.\n\n2. **Incremental Materialized Views**: These views update more efficiently by processing only new or changed records, but come with query restrictions. Most notably, they don't allow filtering or partitioning on aggregated fields in their defining SQL, among other limitations.\n\n**Important Considerations:**\n- Neither type of materialized view in this extension currently supports partitioning or clustering\n- Both types allow you to configure refresh intervals and maximum staleness settings during extension installation or configuration\n- Once created, a materialized view's SQL definition cannot be modified. If you reconfigure the extension to change either the view type (incremental vs non-incremental) or the SQL query, the extension will drop the existing materialized view and recreate it\n- Carefully consider your use case before choosing materialized views:\n  - They incur additional storage costs as they cache query results\n  - Non-incremental views may have higher processing costs during refresh\n  - Incremental views have more query restrictions but are more efficient to update\n\nExample of a non-incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    allow_non_incremental_definition = true,\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\n  AS (\n    WITH latests AS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY document_name\n    )\n    SELECT *\n    FROM latests\n    WHERE operation != \"DELETE\"\n  )\n```\n\nExample of an incremental materialized view SQL definition generated by the extension:\n```sql\nCREATE MATERIALIZED VIEW `my_project.my_dataset.my_table_raw_changelog`\n  OPTIONS (\n    enable_refresh = true,\n    refresh_interval_minutes = 60,\n    max_staleness = INTERVAL \"4:0:0\" HOUR TO SECOND\n  )\nAS (\n      SELECT\n        document_name,\n        MAX_BY(document_id, timestamp) AS document_id,\n        MAX(timestamp) AS timestamp,\n        MAX_BY(event_id, timestamp) AS event_id,\n        MAX_BY(operation, timestamp) AS operation,\n        MAX_BY(data, timestamp) AS data,\n        MAX_BY(old_data, timestamp) AS old_data,\n        MAX_BY(extra_field, timestamp) AS extra_field\n      FROM\n        `my_project.my_dataset.my_table_raw_changelog`\n      GROUP BY\n        document_name\n    )\n```\n\nPlease review [BigQuery's documentation on materialized views](https://cloud.google.com/bigquery/docs/materialized-views-intro) to fully understand the implications for your use case.\n\n#### Using Customer Managed Encryption Keys\n\nBy default, BigQuery encrypts your content stored at rest. BigQuery handles and manages this default encryption for you without any additional actions on your part.\n\nIf you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery. Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS.\n\nFor more general information on this, see [the docs](https://cloud.google.com/bigquery/docs/customer-managed-encryption).\n\nTo use CMEK and the Key Management Service (KMS) with this extension\n1. [Enable the KMS API in your Google Cloud Project](https://console.cloud.google.com/apis/enableflow?apiid=cloudkms.googleapis.com).\n2. Create a keyring and keychain in the KMS. Note that the region of the keyring and key *must* match the region of your bigquery dataset\n3. Grant the BigQuery service account permission to encrypt and decrypt using that key. The Cloud KMS CryptoKey Encrypter/Decrypter role grants this permission. First find your project number. You can find this for example on the cloud console dashboard `https://console.cloud.google.com/home/dashboard?project={PROJECT_ID}`. The service account which needs the Encrypter/Decrypter role is then `bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com`. You can grant this role through the credentials service in the console, or through the CLI:\n```\ngcloud kms keys add-iam-policy-binding \\\n--project=KMS_PROJECT_ID \\\n--member serviceAccount:bq-PROJECT_NUMBER@bigquery-encryption.iam.gserviceaccount.com \\\n--role roles/cloudkms.cryptoKeyEncrypterDecrypter \\\n--location=KMS_KEY_LOCATION \\\n--keyring=KMS_KEY_RING \\\nKMS_KEY\n```\n4. When installing this extension, enter the resource name of your key. It will look something like the following:\n```\nprojects/<YOUR PROJECT ID>/locations/<YOUR REGION>/keyRings/<YOUR KEY RING NAME>/cryptoKeys/<YOUR KEY NAME>\n```\nIf you follow these steps, your changelog table should be created using your customer-managed encryption.\n\n#### Generate schema views\n\nAfter your data is in BigQuery, you can run the [schema-views script](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md) (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter to your target BigQuery project ID.\n\n2. After installation, you'll need to grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n#### Mitigating Data Loss During Extension Updates\n\nWhen updating or reconfiguring this extension, there may be a brief period where data streaming from Firestore to BigQuery is interrupted. While this limitation exists within the Extensions platform, we provide two strategies to mitigate potential data loss.\n\n##### Strategy 1: Post-Update Import\nAfter reconfiguring the extension, run the import script on your collection to ensure all data is captured. Refer to the \"Import Existing Documents\" section above for detailed steps.\n\n##### Strategy 2: Parallel Instance Method\n1. Install a second instance of the extension that streams to a new BigQuery table\n2. Reconfigure the original extension\n3. Once the original extension is properly configured and streaming events\n4. Uninstall the second instance\n5. Run a BigQuery merge job to combine the data from both tables\n\n##### Considerations\n- Strategy 1 is simpler but may result in duplicate records that need to be deduplicated\n- Strategy 2 requires more setup but provides better data continuity\n- Choose the strategy that best aligns with your data consistency requirements and operational constraints\n\n#### Billing\nTo install an extension, your project must be on the [Blaze (pay as you go) plan](https://firebase.google.com/pricing)\n\n- This extension uses other Firebase and Google Cloud Platform services, which have associated charges if you exceed the service’s no-cost tier:\n  - BigQuery (this extension writes to BigQuery with [streaming inserts](https://cloud.google.com/bigquery/pricing#streaming_pricing))\n  - Cloud Firestore\n  - Cloud Functions (Node.js 10+ runtime. [See FAQs](https://firebase.google.com/support/faq#extensions-pricing))\n\n\n\n**Configuration Parameters:**\n\n* BigQuery Dataset location: Where do you want to deploy the BigQuery dataset created for this extension? For help selecting a location, refer to the [location selection guide](https://cloud.google.com/bigquery/docs/locations).\n\n* BigQuery Project ID: Override the default project for BigQuery instance. This can allow updates to be directed to to a BigQuery instance on another GCP project.\n\n* Firestore Instance ID: The Firestore database to use. Use \"(default)\" for the default database. You can view your available Firestore databases at https://console.cloud.google.com/firestore/databases.\n\n\n* Firestore Instance Location: Where is the Firestore database located? You can check your current database location at https://console.cloud.google.com/firestore/databases.\n\n\n* Collection path: What is the path of the collection that you would like to export? You may use `{wildcard}` notation to match a subcollection of all documents in a collection (for example: `chatrooms/{chatid}/posts`). Parent Firestore Document IDs from `{wildcards}` can be returned in `path_params` as a JSON formatted string.\n\n* Enable Wildcard Column field with Parent Firestore Document IDs: If enabled, creates a column containing a JSON object of all wildcard ids from a documents path.\n\n* Dataset ID: What ID would you like to use for your BigQuery dataset? This extension will create the dataset, if it doesn't already exist.\n\n* Table ID: What identifying prefix would you like to use for your table and view inside your BigQuery dataset? This extension will create the table and view, if they don't already exist.\n\n* BigQuery SQL table Time Partitioning option type: This parameter will allow you to partition the BigQuery table and BigQuery view created by the extension based on data ingestion time. You may select the granularity of partitioning based upon one of: HOUR, DAY, MONTH, YEAR. This will generate one partition per day, hour, month or year, respectively.\n\n* BigQuery Time Partitioning column name: BigQuery table column/schema field name for TimePartitioning. You can choose schema available as `timestamp` OR a new custom defined column that will be assigned to the selected Firestore Document field below. Defaults to pseudo column _PARTITIONTIME if unspecified. Cannot be changed if Table is already partitioned.\n\n* Firestore Document field name for BigQuery SQL Time Partitioning field option: This parameter will allow you to partition the BigQuery table created by the extension based on selected. The Firestore Document field value must be a top-level TIMESTAMP, DATETIME, DATE field BigQuery string format or Firestore timestamp(will be converted to BigQuery TIMESTAMP). Cannot be changed if Table is already partitioned.\n example: `postDate`(Ensure that the Firestore-BigQuery export extension\ncreates the dataset and table before initiating any backfill scripts.\n This step is crucial for the partitioning to function correctly. It is\nessential for the script to insert data into an already partitioned table.)\n\n* BigQuery SQL Time Partitioning table schema field(column) type: Parameter for BigQuery SQL schema field type for the selected Time Partitioning Firestore Document field option. Cannot be changed if Table is already partitioned.\n\n* BigQuery SQL table clustering: This parameter allows you to set up clustering for the BigQuery table created by the extension. Specify up to 4 comma-separated fields (for example:  `data,document_id,timestamp` - no whitespaces). The order of the specified  columns determines the sort order of the data. \nNote: Cluster columns must be top-level, non-repeated columns of one of the  following types: BIGNUMERIC, BOOL, DATE, DATETIME, GEOGRAPHY, INT64, NUMERIC,  RANGE, STRING, TIMESTAMP. Clustering will not be added if a field with an invalid type is present in this parameter.\nAvailable schema extensions table fields for clustering include: `document_id, document_name, timestamp, event_id,  operation, data`.\n\n* Maximum number of synced documents per second: This parameter will set the maximum number of syncronised documents per second with BQ. Please note, any other external updates to a Big Query table will be included within this quota. Ensure that you have a set a low enough number to compensate. Defaults to 100.\n\n* View Type: Select the type of view to create in BigQuery. A regular view is a virtual table defined by a SQL query.  A materialized view persists the results of a query for faster access, with either incremental or  non-incremental updates. Please note that materialized views in this extension come with several  important caveats and limitations - carefully review the pre-install documentation before selecting  these options to ensure they are appropriate for your use case.\n\n* Maximum Staleness Duration: For materialized views only: Specifies the maximum staleness acceptable for the materialized view.  Should be specified as an INTERVAL value following BigQuery SQL syntax.  This parameter will only take effect if View Type is set to a materialized view option.\n\n* Refresh Interval (Minutes): For materialized views only: Specifies how often the materialized view should be refreshed, in minutes.  This parameter will only take effect if View Type is set to a materialized view option.\n\n* Backup Collection Name: This (optional) parameter will allow you to specify a collection for which failed BigQuery updates will be written to.\n\n* Transform function URL: Specify a function URL to call that will transform the payload that will be written to BigQuery. See the pre-install documentation for more details.\n\n* Use new query syntax for snapshots: If enabled, snapshots will be generated with the new query syntax, which should be more performant, and avoid potential resource limitations.\n\n* Exclude old data payloads: If enabled, table rows will never contain old data (document snapshot before the Firestore onDocumentUpdate event: `change.before.data()`). The reduction in data should be more performant, and avoid potential resource limitations.\n\n* Cloud KMS key name: Instead of Google managing the key encryption keys that protect your data, you control and manage key encryption keys in Cloud KMS. If this parameter is set, the extension will specify the KMS key name when creating the BQ table. See the PREINSTALL.md for more details.\n\n* Maximum number of enqueue attempts: This parameter will set the maximum number of attempts to enqueue a document to cloud tasks for export to BigQuery.\n\n* Log level: The log level for the extension. The log level controls the verbosity of the extension's logs. The available log levels are: debug, info, warn, and error. To reduce the volume of logs, use a log level of warn or error.\n\n\n\n**Cloud Functions:**\n\n* **syncBigQuery:** A task-triggered function that gets called on BigQuery sync\n\n* **initBigQuerySync:** Runs configuration for sycning with BigQuery\n\n* **setupBigQuerySync:** Runs configuration for sycning with BigQuery\n\n\n\n**Other Resources**:\n\n* fsexportbigquery (firebaseextensions.v1beta.v2function)\n\n\n\n**APIs Used**:\n\n* bigquery.googleapis.com (Reason: Mirrors data from your Cloud Firestore collection in BigQuery.)\n\n\n\n**Access Required**:\n\n\n\nThis extension will operate with the following project IAM roles:\n\n* bigquery.dataEditor (Reason: Allows the extension to configure and export data into BigQuery.)\n\n* datastore.user (Reason: Allows the extension to write updates to the database.)\n\n* bigquery.user (Reason: Allows the extension to create and manage BigQuery materialized views.)\n","lifecycleEvents":[{"stage":"ON_INSTALL","processingMessage":"Configuring BigQuery Sync.","taskQueueTriggerFunction":"initBigQuerySync"},{"stage":"ON_UPDATE","processingMessage":"Configuring BigQuery Sync","taskQueueTriggerFunction":"setupBigQuerySync"},{"stage":"ON_CONFIGURE","processingMessage":"Configuring BigQuery Sync","taskQueueTriggerFunction":"setupBigQuerySync"}],"displayName":"Stream Firestore to BigQuery","events":[{"type":"firebase.extensions.firestore-counter.v1.onStart","description":"Occurs when a trigger has been called within the Extension, and will include data such as the context of the trigger request."},{"type":"firebase.extensions.firestore-counter.v1.onSuccess","description":"Occurs when a task completes successfully. The event will contain further details about specific results."},{"type":"firebase.extensions.firestore-counter.v1.onError","description":"Occurs when an issue has been experienced in the Extension. This will include any error data that has been included within the Error Exception."},{"type":"firebase.extensions.firestore-counter.v1.onCompletion","description":"Occurs when the function is settled. Provides no customized data other than the context."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onStart","description":"Occurs when a trigger has been called within the Extension, and will include data such as the context of the trigger request."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onSuccess","description":"Occurs when a task completes successfully. The event will contain further details about specific results."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onError","description":"Occurs when an issue has been experienced in the Extension. This will include any error data that has been included within the Error Exception."},{"type":"firebase.extensions.firestore-bigquery-export.v1.onCompletion","description":"Occurs when the function is settled. Provides no customized data other than the context."},{"type":"firebase.extensions.big-query-export.v1.sync.start","description":"Occurs on a firestore document write event."}]},"fetchTime":"2025-04-22T16:19:47.253450Z","lastOperationName":"projects/firebaseextensions/operations/373aa99b-62f5-4774-9ed8-8f5c1eb00e33","state":"ACTIVE"},"params":{"DATASET_ID":"firestore_export","MAX_DISPATCHES_PER_SECOND":"100","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","TABLE_ID":"bigqueryexampleproject_users","DATABASE_REGION":"nam5","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","LOG_LEVEL":"silent","WILDCARD_IDS":"false","MAX_ENQUEUE_ATTEMPTS":"3","BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","VIEW_TYPE":"view","EXCLUDE_OLD_DATA":"no","DATABASE":"(default)","COLLECTION_PATH":"users","DATASET_LOCATION":"us","TABLE_PARTITIONING":"NONE"},"populatedPostinstallContent":"### See it in action\n\nYou can test out this extension right away!\n\n1.  Go to your [Cloud Firestore dashboard](https://console.firebase.google.com/project/bigqueryexampleproject-e4ef9/firestore/data) in the Firebase console.\n\n2.  If it doesn't already exist, create the collection you specified during installation: `users`\n\n3.  Create a document in the collection called `bigquery-mirror-test` that contains any fields with any values that you'd like.\n\n4.  Go to the [BigQuery web UI](https://console.cloud.google.com/bigquery?project=bigqueryexampleproject-e4ef9&p=bigqueryexampleproject-e4ef9&d=firestore_export) in the Google Cloud Platform console.\n\n5.  Query your **raw changelog table**, which should contain a single log of creating the `bigquery-mirror-test` document.\n\n    ```\n    SELECT *\n    FROM `bigqueryexampleproject-e4ef9.firestore_export.bigqueryexampleproject_users_raw_changelog`\n    ```\n\n6.  Query your **latest view**, which should return the latest change event for the only document present -- `bigquery-mirror-test`.\n\n    ```\n    SELECT *\n    FROM `bigqueryexampleproject-e4ef9.firestore_export.bigqueryexampleproject_users_raw_latest`\n    ```\n\n7.  Delete the `bigquery-mirror-test` document from [Cloud Firestore](https://console.firebase.google.com/project/bigqueryexampleproject-e4ef9/firestore/data).\n    The `bigquery-mirror-test` document will disappear from the **latest view** and a `DELETE` event will be added to the **raw changelog table**.\n\n8.  You can check the changelogs of a single document with this query:\n\n    ```\n    SELECT *\n    FROM `bigqueryexampleproject-e4ef9.firestore_export.bigqueryexampleproject_users_raw_changelog`\n    WHERE document_name = \"bigquery-mirror-test\"\n    ORDER BY TIMESTAMP ASC\n    ```\n\n### Using the extension\n\nWhenever a document is created, updated, imported, or deleted in the specified collection, this extension sends that update to BigQuery. You can then run queries on this mirrored dataset which contains the following resources:\n\n- **raw changelog table:** [`bigqueryexampleproject_users_raw_changelog`](https://console.cloud.google.com/bigquery?project=bigqueryexampleproject-e4ef9&p=bigqueryexampleproject-e4ef9&d=firestore_export&t=bigqueryexampleproject_users_raw_changelog&page=table)\n- **latest view:** [`bigqueryexampleproject_users_raw_latest`](https://console.cloud.google.com/bigquery?project=bigqueryexampleproject-e4ef9&p=bigqueryexampleproject-e4ef9&d=firestore_export&t=bigqueryexampleproject_users_raw_latest&page=table)\n\nTo review the schema for these two resources, click the **Schema** tab for each resource in BigQuery.\n\nNote that this extension only listens for _document_ changes in the collection, but not changes in any _subcollection_. You can, though, install additional instances of this extension to specifically listen to a subcollection or other collections in your database. Or if you have the same subcollection across documents in a given collection, you can use `{wildcard}` notation to listen to all those subcollections (for example: `chats/{chatid}/posts`).\n\nEnabling wildcard references will provide an additional STRING based column. The resulting JSON field value references any wildcards that are included in users. You can extract them using [JSON_EXTRACT_SCALAR](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions#json_extract_scalar).\n\n\n`Partition` settings cannot be updated on a pre-existing table, if these options are required then a new table must be created.\n\n`Clustering` will not need to create or modify a table when adding clustering options, this will be updated automatically.\n\n#### Cross-project Streaming\n\nBy default, the extension exports data to BigQuery in the same project as your Firebase project. However, you can configure it to export to a BigQuery instance in a different Google Cloud project. To do this:\n\n1. During installation, set the `BIGQUERY_PROJECT_ID` parameter as your target BigQuery project ID.\n\n2. Identify the service account on the source project associated with the extension. By default, it will be constructed as `ext-<extension-instance-id>@project-id.iam.gserviceaccount.com`. However, if the extension instance ID is too long, it may be truncated and 4 random characters appended to abide by service account length limits.\n\n3. To find the exact service account, navigate to IAM & Admin -> IAM in the Google Cloud Platform Console. Look for the service account listed with \"Name\" as \"Firebase Extensions <your extension instance ID> service account\". The value in the \"Principal\" column will be the service account that needs permissions granted in the target project.\n\n4. Grant the extension's service account the necessary BigQuery permissions on the target project. You can use our provided scripts:\n\n**For Linux/Mac (Bash):**\n```bash\ncurl -O https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.sh\nchmod +x grant-crossproject-access.sh\n./grant-crossproject-access.sh -f SOURCE_FIREBASE_PROJECT -b TARGET_BIGQUERY_PROJECT [-i EXTENSION_INSTANCE_ID] [-s SERVICE_ACCOUNT]\n```\n\n**For Windows (PowerShell):**\n```powershell\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/firebase/extensions/master/firestore-bigquery-export/scripts/grant-crossproject-access.ps1\" -OutFile \"grant-crossproject-access.ps1\"\n.\\grant-crossproject-access.ps1 -FirebaseProject SOURCE_FIREBASE_PROJECT -BigQueryProject TARGET_BIGQUERY_PROJECT [-ExtensionInstanceId EXTENSION_INSTANCE_ID] [-ServiceAccount SERVICE_ACCOUNT]\n```\n\n**Parameters:**\nFor Bash script:\n- `-f`: Your Firebase (source) project ID\n- `-b`: Your target BigQuery project ID\n- `-i`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-s`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\nFor PowerShell script:\n- `-FirebaseProject`: Your Firebase (source) project ID\n- `-BigQueryProject`: Your target BigQuery project ID\n- `-ExtensionInstanceId`: (Optional) Extension instance ID if different from default \"firestore-bigquery-export\"\n- `-ServiceAccount`: (Optional) Service account email. If not provided, it will be constructed using the extension instance ID\n\n**Prerequisites:**\n- You must have the [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed and configured\n- You must have permission to grant IAM roles on the target BigQuery project\n- The extension must be installed before running the script\n\n**Note:** If extension installation is failing to create a dataset on the target project initially due to missing permissions, don't worry. The extension will automatically retry once you've granted the necessary permissions using these scripts.\n\n### _(Optional)_ Import existing documents\n\nYou can backfill your BigQuery dataset with all the documents in your collection using the import script.\n\nIf you don't run the import script, the extension only exports the content of documents that are created or changed after installation.\n\nThe import script can read all existing documents in a Cloud Firestore collection and insert them into the raw changelog table created by this extension. The script adds a special changelog for each document with the operation of `IMPORT` and the timestamp of epoch. This is to ensure that any operation on an imported document supersedes the `IMPORT`.\n\n**Important:** Run the import script over the entire collection _after_ installing this extension, otherwise all writes to your database during the import might be lost.\n\nLearn more about using the import script to [backfill your existing collection](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/IMPORT_EXISTING_DOCUMENTS.md).\n\n### _(Optional)_ Generate schema views\n\nAfter your data is in BigQuery, you can use the schema-views script (provided by this extension) to create views that make it easier to query relevant data. You only need to provide a JSON schema file that describes your data structure, and the schema-views script will create the views.\n\nLearn more about using the schema-views script to [generate schema views](https://github.com/firebase/extensions/blob/master/firestore-bigquery-export/guides/GENERATE_SCHEMA_VIEWS.md).\n\n### Monitoring\n\nAs a best practice, you can [monitor the activity](https://firebase.google.com/docs/extensions/manage-installed-extensions#monitor) of your installed extension, including checks on its health, usage, and logs.\n","extensionRef":"firebase/firestore-bigquery-export","extensionVersion":"0.2.2","systemParams":{"firebaseextensions.v1beta.function/memory":"256","firebaseextensions.v1beta.v2function/memory":"256Mi","firebaseextensions.v1beta.function/timeoutSeconds":"120","firebaseextensions.v1beta.function/vpcConnectorEgressSettings":"VPC_CONNECTOR_EGRESS_SETTINGS_UNSPECIFIED","firebaseextensions.v1beta.function/minInstances":"0","firebaseextensions.v1beta.function/location":"us-central1"}},"lastOperationName":"projects/bigqueryexampleproject-e4ef9/operations/a61b124c-6b2a-4a85-ad68-8beffc0b723f","serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","lastOperationType":"CREATE","etag":"6f5055cc72d80ee6d7f93dc8315ae50a19612411f29d1ef679c70f1fde2da1ed","runtimeData":{"stateUpdateTime":"2025-04-24T19:01:43.791080021Z","processingState":{"state":"PROCESSING_COMPLETE","detailMessage":"Sync setup completed"}}}]}
[info] i  functions: preparing functions directory for uploading... 
[info] i  functions: packaged /home/wellington/Documentos/Git/BigQueryExample/Functions-Firebase/functions (64.32 KB) for uploading 
[debug] [2025-04-30T19:08:41.533Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:41.533Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:41.533Z] >>> [apiv2][query] GET https://cloudfunctions.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/-/functions [none]
[debug] [2025-04-30T19:08:43.046Z] <<< [apiv2][status] GET https://cloudfunctions.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/-/functions 200
[debug] [2025-04-30T19:08:43.046Z] <<< [apiv2][body] GET https://cloudfunctions.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/-/functions {"functions":[{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-initBigQuerySync","description":"Runs configuration for sycning with BigQuery","sourceArchiveUrl":"gs://firebase-mod-sources-prod/a8a3e54cd84beb56c7eb0f2379f70adbd1427513f860a1f544893c92df16a047","httpsTrigger":{"url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-initBigQuerySync","securityLevel":"SECURE_OPTIONAL"},"status":"ACTIVE","entryPoint":"initBigQuerySync","timeout":"540s","availableMemoryMb":256,"serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","updateTime":"2025-04-24T18:58:11.627Z","versionId":"2","labels":{"firebase-extensions-ar":"enabled","goog-dm":"firebase-ext-firestore-bigquery-export","goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions","goog-firebase-ext-iid":"firestore-bigquery-export"},"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false"},"runtime":"nodejs20","ingressSettings":"ALLOW_ALL","buildId":"539681f6-8d64-426e-b876-e7462e9cc571","buildEnvironmentVariables":{"GOOGLE_NODE_RUN_SCRIPTS":""},"buildName":"projects/893582587169/locations/us-central1/builds/539681f6-8d64-426e-b876-e7462e9cc571","dockerRegistry":"ARTIFACT_REGISTRY","automaticUpdatePolicy":{},"buildServiceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"},{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-syncBigQuery","description":"A task-triggered function that gets called on BigQuery sync","sourceArchiveUrl":"gs://firebase-mod-sources-prod/a8a3e54cd84beb56c7eb0f2379f70adbd1427513f860a1f544893c92df16a047","httpsTrigger":{"url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-syncBigQuery","securityLevel":"SECURE_OPTIONAL"},"status":"ACTIVE","entryPoint":"syncBigQuery","timeout":"540s","availableMemoryMb":256,"serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","updateTime":"2025-04-24T18:58:16.790Z","versionId":"2","labels":{"goog-firebase-ext-iid":"firestore-bigquery-export","firebase-extensions-ar":"enabled","goog-dm":"firebase-ext-firestore-bigquery-export","goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions"},"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false"},"runtime":"nodejs20","ingressSettings":"ALLOW_ALL","buildId":"aa720a7e-0c24-4c7a-928f-7299737e09a5","buildEnvironmentVariables":{"GOOGLE_NODE_RUN_SCRIPTS":""},"buildName":"projects/893582587169/locations/us-central1/builds/aa720a7e-0c24-4c7a-928f-7299737e09a5","dockerRegistry":"ARTIFACT_REGISTRY","automaticUpdatePolicy":{},"buildServiceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"},{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-setupBigQuerySync","description":"Runs configuration for sycning with BigQuery","sourceArchiveUrl":"gs://firebase-mod-sources-prod/a8a3e54cd84beb56c7eb0f2379f70adbd1427513f860a1f544893c92df16a047","httpsTrigger":{"url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-setupBigQuerySync","securityLevel":"SECURE_OPTIONAL"},"status":"ACTIVE","entryPoint":"setupBigQuerySync","timeout":"540s","availableMemoryMb":256,"serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","updateTime":"2025-04-24T18:57:30.170Z","versionId":"2","labels":{"firebase-extensions-ar":"enabled","goog-dm":"firebase-ext-firestore-bigquery-export","goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions","goog-firebase-ext-iid":"firestore-bigquery-export"},"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false"},"runtime":"nodejs20","ingressSettings":"ALLOW_ALL","buildId":"087f1c66-719a-427e-9758-ba59ba92c9fc","buildEnvironmentVariables":{"GOOGLE_NODE_RUN_SCRIPTS":""},"buildName":"projects/893582587169/locations/us-central1/builds/087f1c66-719a-427e-9758-ba59ba92c9fc","dockerRegistry":"ARTIFACT_REGISTRY","automaticUpdatePolicy":{},"buildServiceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"}]}
[debug] [2025-04-30T19:08:43.048Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:43.049Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:43.049Z] >>> [apiv2][query] GET https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/-/functions filter=environment%3D%22GEN_2%22
[debug] [2025-04-30T19:08:44.372Z] <<< [apiv2][status] GET https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/-/functions 200
[debug] [2025-04-30T19:08:44.372Z] <<< [apiv2][body] GET https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/-/functions {"functions":[{"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions/ext-firestore-bigquery-export-fsexportbigquery","description":"Listens for document changes in your specified Cloud Firestore collection, then exports the changes into BigQuery.","buildConfig":{"build":"projects/893582587169/locations/us-central1/builds/59c9e8df-cfe2-4ef4-b917-55b66938b533","runtime":"nodejs22","entryPoint":"fsexportbigquery","source":{"storageSource":{"bucket":"gcf-v2-sources-893582587169-us-central1","object":"ext-firestore-bigquery-export-fsexportbigquery/function-source.zip","generation":"1745520941600164"}},"dockerRepository":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts","sourceProvenance":{"resolvedStorageSource":{"bucket":"gcf-v2-sources-893582587169-us-central1","object":"ext-firestore-bigquery-export-fsexportbigquery/function-source.zip","generation":"1745520941600164"}},"dockerRegistry":"ARTIFACT_REGISTRY","serviceAccount":"projects/bigqueryexampleproject-e4ef9/serviceAccounts/893582587169-compute@developer.gserviceaccount.com"},"serviceConfig":{"service":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/services/ext-firestore-bigquery-export-fsexportbigquery","timeoutSeconds":120,"environmentVariables":{"BIGQUERY_PROJECT_ID":"bigqueryexampleproject-e4ef9","COLLECTION_PATH":"users","DATABASE":"(default)","DATABASE_INSTANCE":"","DATABASE_REGION":"nam5","DATABASE_URL":"","DATASET_ID":"firestore_export","DATASET_LOCATION":"us","EXCLUDE_OLD_DATA":"no","EXT_INSTANCE_ID":"firestore-bigquery-export","FIREBASE_CONFIG":"{\"projectId\":\"bigqueryexampleproject-e4ef9\",\"databaseURL\":\"\",\"storageBucket\":\"bigqueryexampleproject-e4ef9.firebasestorage.app\"}","FUNCTION_SIGNATURE_TYPE":"cloudevent","GCLOUD_PROJECT":"bigqueryexampleproject-e4ef9","LOCATION":"us-central1","LOG_LEVEL":"silent","MAX_DISPATCHES_PER_SECOND":"100","MAX_ENQUEUE_ATTEMPTS":"3","PROJECT_ID":"bigqueryexampleproject-e4ef9","STORAGE_BUCKET":"bigqueryexampleproject-e4ef9.firebasestorage.app","TABLE_ID":"bigqueryexampleproject_users","TABLE_PARTITIONING":"NONE","TIME_PARTITIONING_FIELD_TYPE":"TIMESTAMP","USE_NEW_SNAPSHOT_QUERY_SYNTAX":"yes","VIEW_TYPE":"view","WILDCARD_IDS":"false","LOG_EXECUTION_ID":"true"},"maxInstanceCount":6,"ingressSettings":"ALLOW_INTERNAL_ONLY","uri":"https://ext-firestore-bigquery-export-fsexportbigquery-hijxxwf3ia-uc.a.run.app","serviceAccountEmail":"ext-firestore-bigquery-export@bigqueryexampleproject-e4ef9.iam.gserviceaccount.com","availableMemory":"256Mi","allTrafficOnLatestRevision":true,"revision":"ext-firestore-bigquery-export-fsexportbigquery-00002-cuw","maxInstanceRequestConcurrency":1,"availableCpu":"0.1666"},"eventTrigger":{"trigger":"projects/bigqueryexampleproject-e4ef9/locations/nam5/triggers/ext-firestore-bigquery-export-fsexportbigquery-548607","triggerRegion":"nam5","eventType":"google.cloud.firestore.document.v1.written","eventFilters":[{"attribute":"database","value":"(default)"},{"attribute":"document","value":"users/{documentId}","operator":"match-path-pattern"}],"pubsubTopic":"projects/bigqueryexampleproject-e4ef9/topics/eventarc-nam5-ext-firestore-bigquery-export-fsexportbigquery-548607-885","serviceAccountEmail":"893582587169-compute@developer.gserviceaccount.com","retryPolicy":"RETRY_POLICY_DO_NOT_RETRY"},"state":"ACTIVE","updateTime":"2025-04-24T18:57:21.075796035Z","labels":{"goog-firebase-ext":"firestore-bigquery-export","deployment-tool":"firebase-extensions","goog-firebase-ext-iid":"firestore-bigquery-export","goog-dm":"firebase-ext-firestore-bigquery-export"},"environment":"GEN_2","url":"https://us-central1-bigqueryexampleproject-e4ef9.cloudfunctions.net/ext-firestore-bigquery-export-fsexportbigquery","createTime":"2025-04-24T18:53:41.744810406Z","satisfiesPzi":true}]}
[info] i  functions: ensuring required API run.googleapis.com is enabled... 
[debug] [2025-04-30T19:08:44.379Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:44.379Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API eventarc.googleapis.com is enabled... 
[debug] [2025-04-30T19:08:44.379Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:44.379Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API pubsub.googleapis.com is enabled... 
[debug] [2025-04-30T19:08:44.380Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:44.380Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: ensuring required API storage.googleapis.com is enabled... 
[debug] [2025-04-30T19:08:44.380Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:44.380Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:44.381Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com [none]
[debug] [2025-04-30T19:08:44.381Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:08:44.385Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com [none]
[debug] [2025-04-30T19:08:44.385Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:08:44.390Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com [none]
[debug] [2025-04-30T19:08:44.390Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:08:44.395Z] >>> [apiv2][query] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com [none]
[debug] [2025-04-30T19:08:44.395Z] >>> [apiv2][(partial)header] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:08:45.575Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com 200
[debug] [2025-04-30T19:08:45.575Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/eventarc.googleapis.com [omitted]
[info] ✔  functions: required API eventarc.googleapis.com is enabled 
[debug] [2025-04-30T19:08:45.579Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com 200
[debug] [2025-04-30T19:08:45.580Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/pubsub.googleapis.com [omitted]
[info] ✔  functions: required API pubsub.googleapis.com is enabled 
[debug] [2025-04-30T19:08:45.651Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com 200
[debug] [2025-04-30T19:08:45.651Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/storage.googleapis.com [omitted]
[info] ✔  functions: required API storage.googleapis.com is enabled 
[debug] [2025-04-30T19:08:45.655Z] <<< [apiv2][status] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com 200
[debug] [2025-04-30T19:08:45.655Z] <<< [apiv2][body] GET https://serviceusage.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/services/run.googleapis.com [omitted]
[info] ✔  functions: required API run.googleapis.com is enabled 
[info] i  functions: generating the service identity for pubsub.googleapis.com... 
[debug] [2025-04-30T19:08:45.656Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:45.656Z] Checked if tokens are valid: true, expires at: 1746042184875
[info] i  functions: generating the service identity for eventarc.googleapis.com... 
[debug] [2025-04-30T19:08:45.656Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:45.656Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:45.657Z] >>> [apiv2][query] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/pubsub.googleapis.com:generateServiceIdentity [none]
[debug] [2025-04-30T19:08:45.658Z] >>> [apiv2][query] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/eventarc.googleapis.com:generateServiceIdentity [none]
[debug] [2025-04-30T19:08:46.143Z] <<< [apiv2][status] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/pubsub.googleapis.com:generateServiceIdentity 200
[debug] [2025-04-30T19:08:46.143Z] <<< [apiv2][body] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/pubsub.googleapis.com:generateServiceIdentity {"name":"operations/finished.DONE_OPERATION","done":true,"response":{"@type":"type.googleapis.com/google.api.serviceusage.v1beta1.ServiceIdentity","email":"service-893582587169@gcp-sa-pubsub.iam.gserviceaccount.com","uniqueId":"114949547525185127171"}}
[debug] [2025-04-30T19:08:46.710Z] <<< [apiv2][status] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/eventarc.googleapis.com:generateServiceIdentity 200
[debug] [2025-04-30T19:08:46.710Z] <<< [apiv2][body] POST https://serviceusage.googleapis.com/v1beta1/projects/893582587169/services/eventarc.googleapis.com:generateServiceIdentity {"name":"operations/finished.DONE_OPERATION","done":true,"response":{"@type":"type.googleapis.com/google.api.serviceusage.v1beta1.ServiceIdentity","email":"service-893582587169@gcp-sa-eventarc.iam.gserviceaccount.com","uniqueId":"102092505972539947691"}}
[debug] [2025-04-30T19:08:46.715Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:46.715Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:46.715Z] >>> [apiv2][query] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 [none]
[debug] [2025-04-30T19:08:47.832Z] <<< [apiv2][status] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 200
[debug] [2025-04-30T19:08:47.832Z] <<< [apiv2][body] GET https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9 {"projectNumber":"893582587169","projectId":"bigqueryexampleproject-e4ef9","lifecycleState":"ACTIVE","name":"BigQueryExampleProject","labels":{"firebase":"enabled","firebase-core":"disabled"},"createTime":"2025-04-24T16:34:06.379745Z"}
[debug] [2025-04-30T19:08:47.835Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:47.835Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:47.835Z] >>> [apiv2][query] GET https://cloudbilling.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/billingInfo [none]
[debug] [2025-04-30T19:08:49.336Z] <<< [apiv2][status] GET https://cloudbilling.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/billingInfo 200
[debug] [2025-04-30T19:08:49.336Z] <<< [apiv2][body] GET https://cloudbilling.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/billingInfo {"name":"projects/bigqueryexampleproject-e4ef9/billingInfo","projectId":"bigqueryexampleproject-e4ef9","billingAccountName":"billingAccounts/019DD6-64C0E7-5ED023","billingEnabled":true}
[debug] [2025-04-30T19:08:49.338Z] [functions] found 6 new HTTP functions, testing setIamPolicy permission...
[debug] [2025-04-30T19:08:49.338Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:49.339Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:49.339Z] >>> [apiv2][query] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions [none]
[debug] [2025-04-30T19:08:49.339Z] >>> [apiv2][(partial)header] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions x-goog-quota-user=projects/bigqueryexampleproject-e4ef9
[debug] [2025-04-30T19:08:49.339Z] >>> [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.setIamPolicy"]}
[debug] [2025-04-30T19:08:49.645Z] <<< [apiv2][status] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions 200
[debug] [2025-04-30T19:08:49.645Z] <<< [apiv2][body] POST https://cloudresourcemanager.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9:testIamPermissions {"permissions":["cloudfunctions.functions.setIamPolicy"]}
[debug] [2025-04-30T19:08:49.645Z] [functions] found setIamPolicy permission, proceeding with deploy
[debug] [2025-04-30T19:08:49.646Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:49.646Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:49.646Z] >>> [apiv2][query] POST https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions:generateUploadUrl [none]
[debug] [2025-04-30T19:08:50.875Z] <<< [apiv2][status] POST https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions:generateUploadUrl 200
[debug] [2025-04-30T19:08:50.875Z] <<< [apiv2][body] POST https://cloudfunctions.googleapis.com/v2/projects/bigqueryexampleproject-e4ef9/locations/us-central1/functions:generateUploadUrl {"uploadUrl":"https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/348f5bce-0183-4ae3-b201-f5954ef0553d.zip?GoogleAccessId=service-893582587169@gcf-admin-robot.iam.gserviceaccount.com&Expires=1746041930&Signature=iTFA8fkpK3FWRSPadBtPgO8jOUla%2FnD%2FJ0g8WuemVKSOjWZ8hMTqEU8IQYFQMIY85VGRO8gQAAnkMEDX77e%2BfwtTyf77dFih8%2BQRCJpuNjnhk%2Fekmgy%2FtzdWYO%2BBq%2B9ccpAd8ppepc14M6kthTBrg3yyica28QHw2RRkFNySPmE4NOnOVkStDZ63lt4uwoW%2B1fv8fXaGS7cVvS05YXjfkVZNA19aS8RWYxhm2ZxSDBjYyDhQNtjo7ub48SZtTatAs99gpSwewcy69RXrSOlSwPLp5nDLkli8evNZwGjk%2FohYl5p2zZqyZdE47ae%2FskXyg9y1c8Vnpay1FmAcbxmDVw%3D%3D","storageSource":{"bucket":"gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com","object":"348f5bce-0183-4ae3-b201-f5954ef0553d.zip"}}
[debug] [2025-04-30T19:08:50.878Z] >>> [apiv2][query] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/348f5bce-0183-4ae3-b201-f5954ef0553d.zip GoogleAccessId=service-893582587169%40gcf-admin-robot.iam.gserviceaccount.com&Expires=1746041930&Signature=iTFA8fkpK3FWRSPadBtPgO8jOUla%2FnD%2FJ0g8WuemVKSOjWZ8hMTqEU8IQYFQMIY85VGRO8gQAAnkMEDX77e%2BfwtTyf77dFih8%2BQRCJpuNjnhk%2Fekmgy%2FtzdWYO%2BBq%2B9ccpAd8ppepc14M6kthTBrg3yyica28QHw2RRkFNySPmE4NOnOVkStDZ63lt4uwoW%2B1fv8fXaGS7cVvS05YXjfkVZNA19aS8RWYxhm2ZxSDBjYyDhQNtjo7ub48SZtTatAs99gpSwewcy69RXrSOlSwPLp5nDLkli8evNZwGjk%2FohYl5p2zZqyZdE47ae%2FskXyg9y1c8Vnpay1FmAcbxmDVw%3D%3D
[debug] [2025-04-30T19:08:50.878Z] >>> [apiv2][body] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/348f5bce-0183-4ae3-b201-f5954ef0553d.zip [stream]
[debug] [2025-04-30T19:08:52.035Z] <<< [apiv2][status] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/348f5bce-0183-4ae3-b201-f5954ef0553d.zip 200
[debug] [2025-04-30T19:08:52.035Z] <<< [apiv2][body] PUT https://storage.googleapis.com/gcf-v2-uploads-893582587169.us-central1.cloudfunctions.appspot.com/348f5bce-0183-4ae3-b201-f5954ef0553d.zip [omitted]
[info] ✔  functions: functions folder uploaded successfully 
[info] i  functions: creating Node.js 22 (2nd Gen) function createuser(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function deleteuser(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function listusers(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function processdata(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function showuser(us-central1)... 
[info] i  functions: creating Node.js 22 (2nd Gen) function updateuser(us-central1)... 
[debug] [2025-04-30T19:08:52.062Z] Total Function Deployment time: 8
[debug] [2025-04-30T19:08:52.062Z] 6 Functions Deployed
[debug] [2025-04-30T19:08:52.062Z] 6 Functions Errored
[debug] [2025-04-30T19:08:52.062Z] 0 Function Deployments Aborted
[debug] [2025-04-30T19:08:52.062Z] Average Function Deployment time: 2.3333333333333335
[info] 
[info] Functions deploy had errors with the following functions:
	createuser(us-central1)
	deleteuser(us-central1)
	listusers(us-central1)
	processdata(us-central1)
	showuser(us-central1)
	updateuser(us-central1)
[debug] [2025-04-30T19:08:52.700Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T19:08:52.700Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T19:08:52.700Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T19:08:52.700Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T19:08:52.700Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T19:08:52.701Z] Not printing URL for HTTPS function. Typically this means it didn't match a filter or we failed deployment
[debug] [2025-04-30T19:08:52.702Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:52.702Z] Checked if tokens are valid: true, expires at: 1746042184875
[debug] [2025-04-30T19:08:52.702Z] >>> [apiv2][query] GET https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts [none]
[debug] [2025-04-30T19:08:54.145Z] <<< [apiv2][status] GET https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts 200
[debug] [2025-04-30T19:08:54.145Z] <<< [apiv2][body] GET https://artifactregistry.googleapis.com/v1/projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts {"name":"projects/bigqueryexampleproject-e4ef9/locations/us-central1/repositories/gcf-artifacts","format":"DOCKER","description":"This repository is created and used by Cloud Functions for storing function docker images.","labels":{"goog-managed-by":"cloudfunctions"},"createTime":"2025-04-24T18:45:25.809021Z","updateTime":"2025-04-30T18:44:12.443453Z","mode":"STANDARD_REPOSITORY","cleanupPolicies":{"firebase-functions-cleanup":{"id":"firebase-functions-cleanup","action":"DELETE","condition":{"tagState":"ANY","olderThan":"86400s"}}},"sizeBytes":"681973421","vulnerabilityScanningConfig":{"lastEnableTime":"2025-04-24T18:45:16.816360009Z","enablementState":"SCANNING_DISABLED","enablementStateReason":"API containerscanning.googleapis.com is not enabled."},"satisfiesPzi":true,"registryUri":"us-central1-docker.pkg.dev/bigqueryexampleproject-e4ef9/gcf-artifacts"}
[debug] [2025-04-30T19:08:54.146Z] Functions deploy failed.
[debug] [2025-04-30T19:08:54.146Z] {}
[debug] [2025-04-30T19:08:54.146Z] {}
[debug] [2025-04-30T19:08:54.146Z] {}
[debug] [2025-04-30T19:08:54.147Z] {}
[debug] [2025-04-30T19:08:54.147Z] {}
[debug] [2025-04-30T19:08:54.147Z] {}
[error] Error: There was an error deploying functions:
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
[error] - TypeError Cannot read properties of null (reading 'length')
